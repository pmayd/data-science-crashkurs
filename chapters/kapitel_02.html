
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Der Prozess von Data Science Projekten &#8212; Einführung in Data Science</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Erkunden der Daten" href="kapitel_03.html" />
    <link rel="prev" title="1. Big Data und Data Science" href="kapitel_01.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Einführung in Data Science</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../vorwort.html">
   Vorwort
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Kapitel
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_01.html">
   1. Big Data und Data Science
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Der Prozess von Data Science Projekten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_03.html">
   3. Erkunden der Daten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_04.html">
   4. Allgemeines zur Datenanalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_05.html">
   5. Assoziationsregeln
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Übungen
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercises/uebung_01.html">
   Übung 1
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/chapters/kapitel_02.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/sherbold/einfuehrung-in-data-science"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sherbold/einfuehrung-in-data-science/main?urlpath=tree/content/chapters/kapitel_02.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prozesse">
   2.1. Prozesse
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#der-generische-data-science-prozess">
   2.2. Der generische Data Science Prozess
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discovery">
     2.2.1. Discovery
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#daten-vorbereitung">
     2.2.2. Daten Vorbereitung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modellplanung">
     2.2.3. Modellplanung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modellerstellung">
     2.2.4. Modellerstellung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kommunikation-der-ergebnisse">
     2.2.5. Kommunikation der Ergebnisse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#operationalisierung">
     2.2.6. Operationalisierung
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rollen-in-data-science-projekten">
   2.3. Rollen in Data Science Projekten
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anwender">
     2.3.1. Anwender
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#projektsponsor">
     2.3.2. Projektsponsor
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#projektmanager">
     2.3.3. Projektmanager
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dateningenieur-engineer">
     2.3.4. Dateningenieur Engineer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#datenbankadministrator">
     2.3.5. Datenbankadministrator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-scientist">
     2.3.6. Data Scientist
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="der-prozess-von-data-science-projekten">
<h1><span class="section-number">2. </span>Der Prozess von Data Science Projekten<a class="headerlink" href="#der-prozess-von-data-science-projekten" title="Permalink to this headline">¶</a></h1>
<div class="section" id="prozesse">
<h2><span class="section-number">2.1. </span>Prozesse<a class="headerlink" href="#prozesse" title="Permalink to this headline">¶</a></h2>
<p>Prozesse sind der Kern jeder Aktivität, auch wenn man sich dessen oft gar nicht bewusst ist. Menschen führen Aktivitöten durch das Anwenden von Techniken durch. Die Prozesse steuern und organisieren diese Aktivitäten und beschreiben die Techniken die eingesetzt werden. <a class="reference internal" href="kapitel_01.html#fig-webtraffic"><span class="std std-numref">Fig. 1.1</span></a> zeigt Beziehung von Menschen, Techniken und Prozessen.</p>
<div class="figure align-default" id="fig-processes">
<a class="reference internal image-reference" href="../_images/processes.png"><img alt="../_images/processes.png" src="../_images/processes.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.1 </span><span class="caption-text">Wachstum des Datenvolumens des Internetverkehrs</span><a class="headerlink" href="#fig-processes" title="Permalink to this image">¶</a></p>
</div>
<p>Das Ziel eines guten Prozesses ist es die Menschen zu unterstützen, zum Beispiel in dem Sichergestellt wird das wichtige Aktivitäten nicht vergessen werden oder durch die Verwendung von geeigneten Werkzeugen zum Lösen von Problemen. Prozesse erreichen dies, in dem Sie geeignete Best Practices beschreiben. Diese Best Practices sollten basierend auf der erfolgreichen Anwendung in der Vergangenheit bestimmt werden. Hierdurch wird soll das Wissen und der Erfolg aus Vergangenen Projekten konserviert und genutzt werden, um die Fähigkeiten der Menschen zu unterstützen und das Risiko das ein Projekt fehlschlägt zu reduzieren. Dies funktioniert jedoch nur, wenn die Prozesse von Menschen auch unterstützt werden.</p>
<p>Wenn die Menschen den Projekt nicht akzeptieren oder an seinen Nutzen glauben, erreicht man das Gegenteil und erhöht stattdessen das Risiko der Projekte. Daher sollten die Menschen die notwendigen Schulungen erhalten, um die Techniken einzusetzen und ihren Nutzen zu kennen. Außerdem muss man sicherstellen, dass die Techniken auch zum Prozess passen.</p>
<p>Man sollte sich auch immer bewusst sein, das es nicht <em>den einen Prozess</em> gibt, der Perfekt zu jedem Projekt passt. Man sollte den Prozess daher immer an die Situation anpassen, man spricht hier auch vom <em>tailoring</em>. Hierbei sollte man Aspekte wie die zur Verfügung stehenden Techniken und den Projektkontext berücksichtigen, zum Beispiel die Größe und Priorität des Projekts, ob es sicherheitskritische Aspekte gibt, oder ob die Mitarbeiter vorwissen aus ähnlichen Projekten mitbringen.</p>
</div>
<div class="section" id="der-generische-data-science-prozess">
<h2><span class="section-number">2.2. </span>Der generische Data Science Prozess<a class="headerlink" href="#der-generische-data-science-prozess" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="#fig-dsprocess"><span class="std std-numref">Fig. 2.2</span></a> zeigt den einen generischen Prozess für Data Science Projekte, der aus sechs Phasen besteht.</p>
<div class="figure align-default" id="fig-dsprocess">
<a class="reference internal image-reference" href="../_images/data_science_process.png"><img alt="../_images/data_science_process.png" src="../_images/data_science_process.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.2 </span><span class="caption-text">Wachstum des Datenvolumens des Internetverkehrs</span><a class="headerlink" href="#fig-dsprocess" title="Permalink to this image">¶</a></p>
</div>
<p>Der Prozess ist iterativ, das heißt das mehrere Wiederholungen aller Phasen innerhalb eines Projekts möglich sind. Innerhalb einer Iteration, kann man nur zu den vorherigen Phasen zurück, solange man die Ergebnisse der Iteration noch nicht kommuniziert hat. Der Grund hierfür ist offensichtlich: Sobald man die Projektergebnisse übermittelt hat, zum Beispiel an die das Management, die Kunden, oder andere Forscher in Form einer Publikation, kann man diese nicht ohne weiteres ändern. Im Folgenden betrachten wir die Projektphasen im Detail.</p>
<div class="section" id="discovery">
<h3><span class="section-number">2.2.1. </span>Discovery<a class="headerlink" href="#discovery" title="Permalink to this headline">¶</a></h3>
<p>Jedes Data Science Projekt beginnt mit der Discovery Phase. Das Ziel der Discovery ist es die Projektdomäne, Ziele und Daten zu verstehen. Anhand der gesammelten Informationen wird beurteilt, ob die für das Projekt notwendigen Resourcen zu verfügung stehen. Anschließend werden die weiteren Projektschritte geplant.</p>
<p>Der Data Scientist muss das notwendige Wissen erlangen, um die Domäne des Projekts zu verstehen, insbesondere den Anwendungsfall der adressiert werden soll. Häufig wird der Data Scientist hierbei von Domainenexperten unterstützt, die die notwendigen Informationen liefern. Dies kann zum Beispiel bedeuten, dass Sie zur Verfügung stehen um Fragen zu beantworten oder auch Anforderungen in Form von Interviews und Workshops gesammelt werden. Hierdurch lernt der Data Scientist auch bereits die Daten und Informationsquellen kennen. Dieses Wissen ist notwendig, um das Projekt zu verstehen und die Ergebnisse zu Interpretieren. Es kann sein, dass der Data Scientists bereits selbst Experte in der Anwendungsdomäne ist, zum Beispiel weil dies nicht das erste derartige Projekt ist. Dennoch ist es häufig Sinnvoll, den Anwendungsfall zusammen mit weiteren Domänenexperten zu betrachten um sicherzustellen, das keine wesentlichen Aspekte übersehen werden.</p>
<p>Als Teil des Lernprozesses der Domäne sollten auch die Vergangenheit nicht außer acht gelassen werden. Bei Forschungsprojekten ist es in der akademischen Welt üblich, zuerst die verwandten Forschungsarbeiten zu identifizieren und den Stand der Forschung aufzuarbeiten. Ähnlich sollte man auch in der Wirtschaft vorgehen: Gab es vielleicht bereits frühere Projekte mit einem ähnlichen Ziel? Falls ja, warum sind diese gescheitert, beziehungsweise warum wird dieses Projekt jetzt neu durchgeführt? Die alten Projektergebnisse - sowohl positive, als auch negative - sind ideale Quellen um Projektrisiken zu identifizieren. Hierbei sollte man sich auch nicht nur auf das eigene Unternehmen beschränken. Im Rahmen der Möglichkeiten des Urheber- und Patentrechts ist eine Analyse der Konkurenzprodukte of Hilfreich, um mögliche Lösungen zu verstehen.</p>
<p>Sobald der Data Scientist die Domäne verstanden hat, kann er Anfangen aus den genannten Zielen eine Problembeschreibung zu erarbeiten, die durch die Datenanalyse gelöst werden soll. Diese Problembeschreibung ist nicht identisch zu den wirtschaftlichen Zielen oder Forschungsinteressen: Während die Ziele nicht direkt mit den Daten, sondern viel mehr der Anwendung zusammen hängen, formuliert die Problembeschreibung die Ziele als Datenanalyseproblem um. Dies ist dem Data Scientist nur möglich, da er vorher die Domäne und den Anwendungsfall mit Hilfe der Domänenexperten kennengelernt hat. Zur Problembeschreibung gehört auch eine Analyse der Stakeholder, zum Beispiel müssen die Stakeholder, deren Daten benötigt werden, identifiziert werden. Weitere Wichtige Stakeholder sind diejenigen, die von der Analyse direkt betroffen sind, zum Beispiel weil ihre Arbeit dadurch unterstützt werden soll. Neben Stakeholdern müssen auch die aktuellen Probleme aufgearbeitet werden, sofern dies nicht schon geschehen ist. Hierdurch wird die Motivation vom Projekt geklärt und das Verständnis der Ziele verbessert. Anschließend werden die Projektziele, Erfolgskriterien, und Risiken des Projekts klar definiert um eine verbindliche Grundlage für das weitere vorgehen zu schaffen.</p>
<p>Durch die oben beschrieben Aktivitäten bekommt der Data Scientists ein Grundverständis der Daten, die im Projekt genutzt werden. Grundsätzlich muss man hierbei unterscheiden zwischen Daten die bereits vorhanden sind (zum Beispiel in einem Data Warehouse), und Daten die noch gesammelt werden müssen. In beiden Fällen obliegt es dem Data Scientists den Umfang, die Struktur, sowie ein abstraktes Verständnis der verfügbaren Informationen zu bekommen. Andernfalls wäre die abschließende Analyse der verfügbaren und benötigten Resourcen nicht möglich.</p>
<p>Während er Discovery befasst man sich auch mit dem <em>wissenschaftlichen</em> Teil von Data Science. Datenanalysen sollten nicht rein Explorativ sein. Stattdessen sollten klare Erwartungen in Form von Hypothesen die getestet werden können formuliert werden. Anderfalls erhöht man das Risiko, das die Projektergebnisse sich nicht über die im Projekt verfügbaren Daten hinaus generalisieren lassen. Die Hypothesen steuern auch das Vorgehen in den weiteren Phasen des Projekts. Es sollten zum Beispiel erwartungen Definiert werden, welche Daten nützlich sind, wie man Sie verwenden sollte, und welches Wissen man aus den Daten gewinnen möchte. Auch wenn der Data Scientists Hauptverantworlich für die Hypothesen ist, sollten diese immer mit Domänenexperten besprochen werden um ihre Plausibilität zu prüfen.</p>
<p>Der letzte Schritt der Discovery ist die Entscheidung ob das Projekt machbar ist und weiter gehen sollte oder nicht. Diese Auswertung sollte die identifizierten Risiken und die verfügbaren Resourcen berücksichtigen. In jedem Fall sollten die folgenden Resourcen berücksichtigt werden:</p>
<ul class="simple">
<li><p>Die <em>technologische Resourcen</em>, zum Beispiel Datenspeicher, verfügbare Rechenkraft, sowie die Verfügbarkeit und die Kosten eventuell benötigter Softwarelizenzen.</p></li>
<li><p>Die <em>benötigtend Daten</em>, das heißt ob bereits ausreichend Daten vorhanden sind oder ob es mit vertretbarem Aufwand im Rahmen des Projektes die benötigten Daten zu sammeln. Die Betrachtung besteht aus zwei Aspekten: 1) Ist die Anzahl der Datenpunkte ausreichend? 2) Hat man für jeden Datenpunkt die benötigten Informationen. Sollten noch Daten gesammelt werden müssen, sollte man dies bei den Projektrisiken immer berücksichtigen.</p></li>
<li><p>Die <em>Arbeitszeit</em>, sowohl in Kalenderzeit, als auch in Personenmonaten. Die Kalenderzeit ist die Dauer des Projekts. Für Projekte deren Kalendarzeit weniger als ein Jahr ist, sollte man auch noch berücksichten, in welchen Monaten des Jahres das Projekt durchgeführt werden soll, da die üblichen Urlaubszeiten bedeuten können, das Mitarbeiter nicht wie erwartet zu verfügung stehen. Wenn Projekte in einem internationalen Umfeld durchgeführt werden, sollte man hierbei auch die lokalen Gepflogenheiten der jeweiligen Projektpartner nicht vergessen. Personenmonate sind ein verbreitetes Mittel um den Entwicklungsaufwand der für ein Projekt investiert wird abzuschätzen. Zwei Mitarbeiter, die je einen Monat am Projekt arbeiten, entspricht hierbei zwei Personenmonaten. Man sollte bei der Betrachtung der Arbeitszeit aber nie vergessen, das zwei Mitarbeiter in der Regel nicht doppelt so effizient sind wie ein Mitarbeiter. Diese Phänomen ist in der Softwareentwicklung auch das der <em>Mythical Man-Month</em> <a class="footnote-reference brackets" href="#manmonth" id="id1">1</a> bekannt.</p></li>
<li><p>Die <em>Mitarbeiter</em> (gerne auch Human Resources genannt), das heißt die Personen, die für das Projekt zur Verfügung stehen. Hierbei sollte insbesondere betrachtet werden, ob das Fähigkeitsprofil der Mitarbeit zu den Anforderungen des Projekts passt.</p></li>
</ul>
<p>Sofern ausreichend Resourcen verfügbar sind und die Risiken kontrolierbar, kann das Projekt gestartet werden.</p>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Ein Kunde ist Besitzer eines Webshops für Kleidung. Der Kunde möchte gerne die Anzahl seiner Verkäufe durch <em>Cross-Sell</em> erhöhen, das heißt Kunden sollen dazu gebracht werden weitere Produkte in den Warenkorb zu legen. Ihre Aufgabe ist es eine Anwendung hierfür zu entwickeln, welche auf den vergangen Verkaufsdaten basiert. Die Discovery könnte in etwa so aussehen:</p>
<ul class="simple">
<li><p>Sie führen ein Interview mit dem Kunden durch um Herauszufinden, ob der Kunde bereits eine Idee hat wie mehr Cross-Sell ermöglicht werden könnte. Sie finden raus, das gezielt Werbung während des Einkaufs schalten möchte, sobald ein Produkt in den Warenkorb gelegt wird. Diese Information liefert ihnen eine Kernanforderung des Projekts, andernfalls wäre zum Beispiel auch Emailmarketing denkbar gewesen.</p></li>
<li><p>Sie gucken sich Webshops an, die bereits ähnliche Lösungen benutzen.</p></li>
<li><p>Sie definieren die Problembeschreibung als das Ziel geeignete Werbung vorherzusagen, basierend auf dem vergangenen Verhalten des Kunden, der vergangenen Verhalten allter Kunden, sowie dem aktuellen Inhalt des Warenkorbs.</p></li>
<li><p>Sie identifizieren zwei wichtige Stakeholder: 1) Den Besitzer des Webshops als Auftraggeber, Ansprechpartner auf Kundenseite, und Domänenexperten. 2) Die Kunden des Webshops, die für sich relevante Produkte kaufen wollen und eine gute User Experience (UX) bei der Benutzung der Software. Unpassende Werbung könnte die Benutzererfahrung verschlechtern, sehr gute Werbung sogar verbessern.</p></li>
<li><p>Sie identifizieren keine Probleme mit dem Status Quo, die gelöst werden sollen. Es geht also um eine reine Optimierung der Einnahmen.</p></li>
<li><p>Aus diesen Erkenntnissen definieren sie die zwei konkrete Projektziele:</p>
<ol class="simple">
<li><p>Erhöhung der Anzahl der verkauften Produkte und dadurch des Umsatzes.</p></li>
<li><p>Gleichbleibende oder verbesserte UX.</p></li>
</ol>
</li>
<li><p>Die Erfüllung der Projektziele soll durch die Beobachtung des Umsatzes, sowie durch eine Umfrage unter den Benutzern bezüglich ihrer Zufriedenheit, bestimmt werden. Das Projekt gilt als Erfolgreich, wenn der Umsatz sich um mindestens 5% erhöht und die Benutzerzufriedenheit sich nicht verschlechtert. Sie identifizieren eine mögliche Verschlechterung der Benutzerzufriedenheit die zu einem Abfall des Umsatzes führt wird als Hauptrisiko des Projekts.</p></li>
<li><p>Als Daten stehen Ihnen die hauptsächlich die Transaktionen von vergangenen Einkäufen zur Verfügung. Diese Daten beinhalten welche Kunden welche Produkte innerhalb einer Bestellung gekauft haben. Die Daten liegen in einer relationalen Datenbank vor. Weitere Daten stehen Ihnen nicht zur Verfügung.</p></li>
<li><p>Sie formulieren drei Hypothesen:</p>
<ol class="simple">
<li><p>Produkte die in der Vergangenheit häufig zusammen gekauft wurden, werden auch in der Zukunft häufig zusammen gekauft.</p></li>
<li><p>Es gibt saisonale Muster in den Verkaufsdaten (zum Beispiel für Winter- und Sommerkleidung), welche relevant sind die die Empfehlungen.</p></li>
<li><p>The Kategorien, durch die sich die Produkte beschreiben lassen, sind relevant für die Werbung, insbesondere die Marken und die Art der Kleidung.</p></li>
</ol>
</li>
<li><p>Sie sind der Meinung das die Resourcen ausreichen, um eine Pilotstudie durchzuführen, ind er die Machbarkeit von nützlichen Vorhersagen für Cross-Sell Werbung geprüft wird. Für eine detailierte Evaluation der Benutzererfahrung, sowie eine Operationationalisierung der Ergebnisse in den produktiven Betrieb innerhalb des Projektes mit den zur Verfügung stehenden Resourcen nicht realistisch. Im Falle einer erfolgreichen Machbarkeit der Vorhersagen, wird dies in einem Folgeprojekt umgesetzt.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="daten-vorbereitung">
<h3><span class="section-number">2.2.2. </span>Daten Vorbereitung<a class="headerlink" href="#daten-vorbereitung" title="Permalink to this headline">¶</a></h3>
<p>Mit dem Abschluss der Discovery beginnt die technische Arbeit des Projekts, in dem die Daten für die Analyse vorbereitet werden. Hierbei gibt es zwei wesentliche Ziele:</p>
<ol class="simple">
<li><p>Die Infrastruktur für die Datenanalyse muss erstellt werden und alle relevanten Daten müssen in diese Infrastruktur geladen werden.</p></li>
<li><p>Der Data Scientists sollte ein tiefgehendes und detailiertes Verständnis der Daten erlangen.</p></li>
</ol>
<p>Der Aufwand für die Vorbereitung der Infrastruktur hängt stark vom Projekt an. Es kann nahezu trivial sein und mit wenigen Zeilen Quelltext erledigt sein, oder mehrere Personenjahre an Resourcen verschlingen. Ist das Datenvolumen relativ klein und die Daten können durch eine einzelne SQL-Abfrage geladen werden, ist diese Aufgabe in kürzester Zeit erledigt. Handelt es sich wiederum um ein Big Data Projekt, in dem die Daten erst noch gesammelt werden müssen, oder wo der Zugriff auf die Daten schwierig ist (zum Beispiel aus Sicherheits- oder Datenschutzbedenken), kann dies extrem Schwierig und aufwendig sein.</p>
<p>Der grundsätzliche Prozess des Ladens von Daten in die Analyseinfrastruktur wird <em>ETL</em> genannt: <em>Extract</em>, <em>Transform</em>, <em>Load</em>. Zuerst werden die Daten von ihrem aktuellen Speicherort extrahiert. Dies bedeutet das der Code zum Laden der Daten aus Dateien, Datenbanken, oder zum sammeln von Daten aus anderen Quellen (zum Beispiel dem Internet durch “Scraping”) geschrieben wird. Sobald die Daten extrahiert sind, werden sie in das benötigte Format konvertiert. Diese Transformation beinhaltet üblicherweise auch die Qualitätskontrolle der Daten: Zum Beispiel kann man hier unvollständige oder implausible Datenpunkte entfernen. Weiterhin müssen die Daten häufig restrukturiert und in andere Formate konvertiert werden. Das kann zum Beispiel bedeuten, dass informationen aus verschiedenen Quellen integriert werden. Es kann aber auch heißen das Informationen neu aufgeteilt werden. Zum Beispiel könnte man den Inhalt von Blogposts in verschiedene Felder aufteilen: Titel, Inhalt, und Kommentare. Bei textuellen Daten kann es auch herausfordernd sein, diese in ein einheitliches Textformat zu konvertieren, da es viele verschiedene Kodierungen für Textdaten gibt, zum Beispiel ASCII, ISO-8859, UTF-8, und UTF-16, um nur einige gängige zu nennen. Ähnliche Probleme kann es bei Datumformaten geben. Ob 04/05/21 sich auf den vierten Mai oder den fünften April bezieht hängt davon ab ob es sich um die amerikanische oder britische Konvention handelt. Ob es sich um das Jahr 2021 oder 1921 hängt vom Zeitpunkt ab, an dem dieses Datum geschrieben wurde. Sobald alle Daten transformiert sind, können Sie in die Analyseumgebung geladen werden.</p>
<p>Häufig kann man ETL auch Abwandeln, in dem man die Transformation und das Laden der Daten vertauscht, also ELT. In diesem Fall werden die Rohdaten direkt in die Analyseumgebung geladen und benötigte Transformationen werden innerhalb der Analyseumgebung durchgeführt. Ob ETL oder ELT die bessere Wahl ist, hängt vom Anwendungsfall ab. Ein guter Grund, warum man ELT statt ETL nutzen sollte ist, das die Transformationen so komplex sind, dass man sie ohne die Rechenkraft der Analyseumgebung nicht durchführen kann. Ein weiterer Vorteil von ELT ist, dass man verschieden Transformationen ausprobieren und flexibel miteinander vergleiche kann. Zuletzt gibt es auch Anwendungen, die vom Zugriff auf die Rohdaten profitieren können, da diese zum Beispiel dann auch von Algorithmen als Merkmal verwendet werden können. Auf der andern Seite ist ETL zu favorisieren, wenn die Transformationen sehr Zeitaufwendig sind und nicht mehrfach bei jedem Laden der Daten durchgeführt werden sollen, oder wenn man die Transformationen direkt bei einem Datenbankzugriff durchführen kann.</p>
<p>Ein zweitere wesentlicher Aspekt der Daten Vorbereitung ist das detailierte Verständnis der Daten. Dies bedeutet das Studium der Dokumentation der Daten. Sofern diese nicht vorhanden oder nicht ausreichend ist, müssen die Daten mit Hilfe von Domänenwissen interpretiert werden. Im Idealfall kennt der Data Scientists zum Abschluss dieser Phase alle Details der Daten, zum Beispiel die Bedeutung jeder Spalte in einer relationalen Datenbank oder welche Arten von Dokumenten es gibt und wie diese Strukturiert sind. Diese Arbeit kann man auch als Lernen der <em>Metadaten</em> bezeichnen, also der Daten über die Daten. Zusätzlich zu den Metadaten, sollten auch die daten selbst <em>erkundet</em> werden - eine Aktivität die häufig eng mit den Transformation von ETL zusammenhängt. Hierzu betrachtet man zum Beispiel Statistiken und Visualusierungen (TODO Referenz Hinzufügen). Hierdurch kann man zum Beispiel Erkenntnisse über die Wahrscheinlichkeitsverteilungen der Daten gewinnen, ungültige Daten identifizieren, oder Skaleneffekte entdecken und entfernen um die Daten besser zu vereinheitlichen.</p>
<p>Diese detailierte Betrachtung der Daten erlaubt es dem Data Scientist auch zu erkennen, welche Daten wirkliche Wertvoll für das Projekt sind, und welche Daten man eventuell doch nicht benötigt werden. Hierbei muss man ein gesundes Mittelmaß finden: Auf der einen Seite geht man ein Risiko ein, wenn man Daten frühzeitig entfernt, da man etwas übersehen haben könnte und die Daten eventuell doch nützlich wären. Auf der andern Seite reduziert man die komplexität des Projekts, wenn weniger Daten vorhanden sind. Insbesondere wenn große Datenmengen entfernt werden können, kann sich der Analyseaufwand stark reduzieren.</p>
<p>Am Ende der Daten Vorbereitung sind alle Daten für die Analyse verfügbar und alle benötigte Transformationen sind definiert und durchgeführt.</p>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Die Verkaufsdaten sind in einer relationalen Datenbank gespeichert, welche 352.152 Transaktionen beinhaltet. Im Mittel wurden 2,3 Gegenstände in einer Transaktion gekauft. Für jede Transaktion ist ein Zeitstempel im ISO 8601 Format so eine pseudonymizierter Identifier des Benutzers, der die Gegenstände gekauft hat, verfürbar. In einer zweiten Tabelle werden die Informationen über die Gegenstände selbst gespeichert, zum Beispiel: Der Preis, die Kategorien denen ein Gegenstand zugeordnet ist (Männerkleidung, Frauenkleidung, Hose, Pullover, Socken, Herstellermarke). Es gibt noch weitere Daten, wie zum Beispiel die Bezahlart. Diese weiteren Daten werden jedoch als für dieses Projekt nicht relavnt eingestuft und nicht für die Analyse verwendet.</p>
<p>Das Datenvolumen ist ca. ein Gigabyte. Daher entscheiden Sie sich für eine ELT Prozess, da das Laden der Daten aus der Datenbank nur etwa eine Minute dauert und man die Daten flexibel in der Analyseumgebung erkunden kann und während dessen die benötigten Transformationen definiert.</p>
<p>Während dem erkunden der Daten entdecken sie 2.132 Transaktionen ohne Gegenstände. Diese entfernen Sie, da es sich um ungültige Datenpunkte handelt. Außerdem stelle Sie fest, das die Kleidung einer Marken nur selten gekauft werden. Daher fassen Sie diese Marken in einer neuen Kategorie “Andere Marken” zusammen.</p>
<p>Sie bestimmen außerdem vier Repräsentation der Transaktionen, die für die Cross-Sell Analyse nützlich sein könnten:</p>
<ul class="simple">
<li><p>Identisch zur Datenbank, das heißt direkt durch die verkauften Gegenstände.</p></li>
<li><p>Die Gegenstände werden durch die Art der Kleidung ersetzt.</p></li>
<li><p>Die Gegenstände werden durch die Marke ersetzt.</p></li>
<li><p>Die Gegenstände werden durch die Kombination von der Art der Kleidung und der Marke ersetzt.</p></li>
</ul>
<p>Die verschiedenen Repräsentation ermöglichen es Ihnen, den Cross-Sell auf bestimmte Kleidungstypen oder Marken zu fokusieren.</p>
</div></blockquote>
</div>
<div class="section" id="modellplanung">
<h3><span class="section-number">2.2.3. </span>Modellplanung<a class="headerlink" href="#modellplanung" title="Permalink to this headline">¶</a></h3>
<p>Das Ziel der Modellplanung ist das Design des Analysemodells. Hierzu muss man aus den verschiedenen Möglichkeiten, wie die Datenanalyse gestaltet werden kann ein geeignetes Modell auswählen, welches sowohl zu den Daten, als auch zum Analyseziel passt. Es gibt verschiedene Aspekte, die bei der Modellauswahl berücksichtigt werden müssen. Das Projektziel gibt üblicherwise die grundsätzliche Art des Modells vor:</p>
<ul class="simple">
<li><p><em>Assoziationsregeln</em> können benutzt werden um Regeln zu finden, welche relevante Beziehungen innerhalb von Transaktionen beschreiben (TODO ref).</p></li>
<li><p><em>Clusteranalyse</em> ist dazu geeignet Gruppierungen innerhalb von Daten zu finden (TODO ref).</p></li>
<li><p><em>Klassifikation</em> wird benutzt um Vorherzusagen, zu welcher Kategorie Daten gehören (TODO ref).</p></li>
<li><p><em>Regressionsmodelle</em> beschreiben den Zusammenhang zwischen Merkmalen der Daten und können benutzt werden um kontinuierliche Werte vorherzusagen (TODO ref).</p></li>
<li><p><em>Zeitreihenanalyse</em> berücksichtigt zeitliche Abhängigkeiten zwischen Datenpunkten um zukünftige Entwicklungen abzuschätzen (TODO ref).</p></li>
</ul>
<p>Bei der Modellplanung muss man aus den vielen Möglichkeiten, wie man zum Beispiel ein Klassifikationsmodell erstellen kann, eine für das Projekt passende auswählen. Es gibt viele Aspekte die diese Auswahl beeinflussen. Hier sind einige wichtige Fragen, deren Beantwortung bei der Modellauswahl hilft.</p>
<ul class="simple">
<li><p>Ist es wichtig, dass man die Modelldetails als Mensch nachvollziehen kann oder reicht es, wenn das Modell entsprechend definierter Metriken gut ist? Ein <em>White-Box</em> Modell kann vom Data Scientist und eventuell sogar von Domänenexperten benutzt werden, um die Logik, die das Modell verwendet, im Detail nachzuvollziehen. Dies kann zum Beispiel wichtig sein, damit die Nutzer des Modells in der Lage sind, verantwortung für die Entscheidungen, die mit Hilfe der Modelle getroffen werden, zu übernehmen. Wenn dies für den Anwendungsfall als weniger wichtig betrachtet wird, kann man auch <em>Black-Box</em> Modelle verwenden. Hier versteht man dann zwar in der Regel nicht, wie die Entscheidungen zu stande gekommen sind, dafür sind solche Modelle häufig Präziser. Der Grund für die höhere Präsizion ist, das die interne komplixät nicht von der Interpretierbarkeit beschränkt ist. Ein Beispiel für ein White-Box Modell sind Entscheidungsbäume, in denen anhand einfacher Regeln (größer, kleiner, gleich) Merkmale in einer festgelegten Reihenfolge bewertet werden, um zu einer Entscheidung zu gelangen. Ein Beispiel für Black-Box Modelle sind tiefe neuronale Netze (engl. Deep Neural Networks, DNN), in es Millionen von Parameter gibt, die eine für Menschen nicht nachvollziehbare mathematische Funktion beschreiben.</p></li>
<li><p>Wie hoch ist das Datenvolumen? Obwohl in der Regel alle Modelle besser werden, wenn mehr Daten zur Verfügung stehen, gibt es starke Unterschiede bezüglich zwischen den Modellen. Auf der einen Seite ist die Modellperformance, die sich mit der Datengröße für unterschiedliche Modelle mit anderen Trends entwickelt (<a class="reference internal" href="#fig-ceiling"><span class="std std-numref">Fig. 2.3</span></a>). Auf der anderen Seite ist die Laufzeit zum Training und für die Vorhersagen: Wenn die Berechnungen zu komplex sind, kann es schwierig sein ein Modell basierend mit einem großen Datenvolumen zu lernen.</p></li>
<li><p>Welche Modelle wurden in der Vergangenheit in diesem Kontext erfolgreich verwendet? Hier sollte man auf das gesammelte Wissen aus der Discovery zurückgreifen.</p></li>
<li><p>Mit welchen Modellen hat das Projektteam die meiste Erfahrung? Gerade wenn es mehrere geeignete Kandidaten gibt, ist es Sinnvoll, ein Modell zu wählen was von den Mitarbeitern bereits gut verstanden wird.</p></li>
</ul>
<p>Damit Sie in Ihrer Rolle als Data Scientists ein geeignetes Modell auswählen können, ist es deshalb wichtig, dass Sie nicht nur detailwissen über wenige Modelle habe. Stattdessen sollten Data Scientists eine Vielzahl von Modellen kennen, um für jeden Kontext geeignete Modelle identifzieren zu können.</p>
<div class="figure align-default" id="fig-ceiling">
<a class="reference internal image-reference" href="../_images/ceiling.png"><img alt="../_images/ceiling.png" src="../_images/ceiling.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.3 </span><span class="caption-text">Vergleich der Performanzentwicklung zweier fiktiver Modelle, bezogen auf die Datengröße. Das Orange Modell ist besser, wenn nur wenige Daten zur verfügbar sind. Das Blaue Modell ist besser, wenn viele Daten verfügbar sind. In diesem fiktiven Beispiel könnte das orange Modell für Random Forests (TODO ref) und das blaue Modell für tiefe neuronale Netze (TODO ref) stehen.</span><a class="headerlink" href="#fig-ceiling" title="Permalink to this image">¶</a></p>
</div>
<p>Auch wenn die Auswahl des Modells ein entscheidener Faktor für den Projekterfolg ist, macht Sie in der Regel nur einen relativ kleinen Teil der Modellplanung aus. Weitere wichtige Aktivitäten sind:</p>
<ul class="simple">
<li><p>Die Auswahl von Merkmalen (engl. feature), welche die Grundlage für die Modelle sind. Hierbei müssen auch die genauen Berechnungsvorschriften für die Merkmale definiert werden und der Umgang mit Daten, die nicht plausibel sind.</p></li>
<li><p>Die Auswertungskriterien für die Modelle müssen durch Metriken definiert werden, mit denen die Modellqualität bewertet werden kann. Die Metriken können zum Beispiel in Bezug auf die allgemeine Genauigkeit oder bestimmte Fehlerklassen.</p></li>
<li><p>Statistische Methoden und Visualisierung zur Interpretation der Ergebnisse müssen definiert werden.</p></li>
<li><p>Die Daten müssen eventuell in verschieden Teilmengen unterteilt werden, zum Beispiel in Trainingsdaten zum lernen von Modellen, Validationsdaten zur Auswahl des besten Modells, und Testdaten zur Bewertung der Güte des besten Modells.</p></li>
<li><p>Im Fall von Big Data muss eine Arbeitsumgebung geschaffen werden, in der man mit den Daten, oder einer kleinen Teilmenge der Daten, experimentieren kann.</p></li>
</ul>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Sie entscheiden sich für den Apriori Algorithmus zum bestimmen von Assoziationsregeln, um Produkte, die häufig gemeinsam gekauft wurde zu identifizieren. Für die Varianten der Daten, in denen Sie die Produkte durch Kategorien ersetzt haben, möchten Sie ein Random Forest Regressionsmodell nutzen, um vorherzusagen, mit welcher Wahrscheinlichkeit bestimmte Produkte gekauft werden, gegeben dem Inhalt des Warenkorbs. Je nach Qualität der Ergebnisse dieser Modelle, planen Sie außerdem noch die Modelle gegebenenfalls in einer zweiten Iteration zu verändern um Verbesserungen zu erreichen. Also Risiko, welches zu einer solchen zweiten Iteration führen kann, sehen Sie das eventuell zu viele günstige Produkte vorhergesagt werden, so dass selbst richtige Vorhersagen nur geringe Auswirkungen auf den Umsatz haben würden. Als Metrik für die Modellqualität bewerten Sie, wie häufig zufällig aus dem Warenkorb entfernte Gegenstände richtig vorhergesagt werden von ihren Modellen. Um die Analyse der Modellqualität zu unterstützen, planen Sie eine Visualisierung zu erstellen, die Ihnen hilft zu verstehen welche Produkte häufig gemeinsam gekauft werden. Außerdem entschließen Sie sich ihre Daten anhand der Zeitstempel in drei Teildatensätze zu unterteilen: Die ältesten 50% der Daten sollen zum Trainieren der Modelle benutzt werden, die nächsten 25% als Validierungsdaten, und die neusten 25% als Testdaten.</p>
</div></blockquote>
</div>
<div class="section" id="modellerstellung">
<h3><span class="section-number">2.2.4. </span>Modellerstellung<a class="headerlink" href="#modellerstellung" title="Permalink to this headline">¶</a></h3>
<p>Zur Modellerstellung müssen Sie den notwendigen Code schreiben um die geplanten Modelle zu trainieren. Häufig gibt es mehrere Iterationen zwischen der Modellplanung und der Modellerstellung. Innerhalb dieser Iterationen wird das Modell stückweise basierend auf den Erkenntnissen von zwischenauswertungen verbessert. So können zum Beispiel probleme, die man spät in den Daten entdeckt noch behoben werden oder potentiale für die Verbesserung, die man noch findet, genutzt werden. Wichtig ist jedoch, das für diese iterative Verbesserungen nur die Trainings- und Validierungsdaten verwendet werden. Die Testdaten darf man nur zu abschließenden Bewertung der Modellqualität benutzen. Wenn die Testdaten mehrfach benutzt werden, degenerieren sie zu Validierungsdaten (TODO ref).</p>
<p>Man könnte die Modellplanung und die Modellerstellung auch als eine Phase betrachten. Der Grund, weshalb wir dies hier als getrennte Phasen betrachten, ist das die Modellerstellung sehr teuer und zeitintensiv sein kann, zum Beispiel wenn riesige neuronale Netze in einem gemieteten Cluster in der Cloud trainiert werden. Da dies nicht oft wiederholt werden kann, hilft es das Training als separaten Teil des Prozesses hervorzuheben.</p>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Sie entschließen sich das Modell mit der Programmiersprache Python basierend auf den Bibliotheken pandas, scikit-learn, und mlextend zu erstellen. Sie können das Modell auf einer normalen Workstation trainieren und evaluieren.</p>
</div></blockquote>
</div>
<div class="section" id="kommunikation-der-ergebnisse">
<h3><span class="section-number">2.2.5. </span>Kommunikation der Ergebnisse<a class="headerlink" href="#kommunikation-der-ergebnisse" title="Permalink to this headline">¶</a></h3>
<p>Irgendwann kommt der Punkt, an dem der Zyklus von iterativen Modellplanungs- und Modellerstellungsschritten beendet werden muss und die finalen Projektergebnisse kommuniziert werden. Üblicherweise ist dies entweder der Fall, wenn die Zeit oder die geplanten Resourcen verbraucht sind, die gewünschte Modellqualität erreicht ist, keine Verbesserungen mehr möglich sind, oder es absehbar ist, das kein adequates Modell gefunden wird. Die Projektergebnisse müssen dann den relevanten Stakeholdern mitgeteilt werden. Welche dies sind, hängt vom Projekt ab. In der Industrie können das zum Beispiel die Kunden oder das höhere Management sein. In der Forschung könnte das der Betreuer einer Abschlussarbeit sein oder andere Forscher. Die Form, in der die Ergebnisse kommuniziert werden hängt ebenfalls vom Kontext ab. Häufig gibt es eine Abschlusspräsentation, eventuell gibt es auch Projektberichte, in der Wissenschaft auch Fachartikel.</p>
<p>Egal in welcher Form die Ergebnisse kommunziert werden, der wichtigste Aspekt ist das klar wird ob die Projektziele erreich wurden oder nicht und, als direkt Folgerung, ob das Projekt erfolgreich war. Zusätzlich sollte man noch die wichtigsten Erkenntnisse des Projekts herausstreichen, zum Beispiel ob das Modell zu Gewinne erhöht oder Kosten reduziert, wie der erwartete Return on Investment (ROI) ist, oder welche Auswirkungen auf das operative Geschäft oder weitere Projekte erwartet werden. Domänenexperten sind häufig auch an Details interesiert, zum Beispiel welche Merkmale wichtig sind und wie diese mit den Ergebnissen zusammen hängen. Gerade wenn etwas unerwartetes passiert, kann dies zu spannenden Erkenntnissen führen. Bei Forschungsprojekten kommt hin, dass man auch klar kommunizieren muss, inwiefern die Erkenntnisse aus dem Projekt den Stand der Forschung voran bringen.</p>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Die Assoziationsregeln für Kategorien von Produkten in Kombination mit der vorhersage von geeigneten Gegenständen durch das Regressionsmodell liefert die besten Ergebnisse. Sie schätzen das sich die Verkäufe um 10% erhöhen lassen und das sich der Umsatz hierdurch um 6% erhöhen wird. Sie finden außerdem heraus, das Ihre Modelle nur dann zuverlässig Funktionieren, wenn sie zwischen Frauen- und Männerkleidung unterscheiden. Andernfalls sagt das Regressionsmodell zu häufig Frauenkleidung vorraus.</p>
</div></blockquote>
</div>
<div class="section" id="operationalisierung">
<h3><span class="section-number">2.2.6. </span>Operationalisierung<a class="headerlink" href="#operationalisierung" title="Permalink to this headline">¶</a></h3>
<p>Wenn die Ergebnisse des Modells gut genug sind, wird unter umständen die Entscheidung getroffen, dass Modell im operationellen Betrieb einzusetzen. Hiermit ist in der Regel ein signifikanter Aufwand verbunden: Oft muss das Modell in der operationellen Umgebung neu Implementiert werden. Hierbei müssen sowohl die Sicherheitsaspekte der Umgebung, als auch die Lauftzeit anforderungen respektiert werden. Wenn das Modell zum Beispiel direkt innerhalb einer Datenbank ausgeführt werden soll, muss man es in der Regel von Grund auf neu programmieren. Aufgrund dieses oft hohen Aufwands, ist die Operationalisierung oft ein separates Projekt.</p>
<p>Wenn Sie sich dafür Entscheiden, ein Modell in den operationellen Betrieb zu übernehmen sollten sie dies im Idealfall pilotieren, also zuerst in einem kleinen Kontext ausprobieren. Diese Pilotstudie ist ihr Sicherheitheitnetz und sollte sicherstellen, dass die Erwartung bezüglich der Modelleigenschaften, die sie auf den Testdaten überprüft haben, auch im operationellen Betrieb gültig sind. Es kann zum Beispiel Unterschiede geben, weil sich die Benutzer nicht so verhalten, wie dies erwarten weil sich das Nutzerverhalten seit dem Sammeln der Daten für das Projekt geändert hat.</p>
<p>Die Alterung der Daten ist ein Problem für die Operationalisierung, welches man nicht unterschätzen sollte: Nicht nur die Daten altern, sondern auch die Modelle. Als Folge verschlechtert sich häufig die GÜte der Modelle mit der Zeit. Damit dies kein Problem wird, sollte als Teil der Operationalisierung definiert werden, wann und wie die Modellqualität regelmäßig überprüft wird und unter welchen Umständen das Modell durch ein auf neuen Daten trainiertes Modell ersetzt wird.</p>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Ihr Kunde ist Zufrieden mit den Projektergebnissen und möchte das Modell in seinem Webshop durch eine Pilotstudie testen. Die Pilotstudie zeigt die Vorhersagen bei zufällig ausgewählten Kunden und protokoliert ob weitere Produkte basierend auf den Vorhersagen gekauft wurden. Die Dauer der Pilotstudie ist ein Monat. Unabhängig davon, ob Nutzer die Vorhersagen sehen oder nicht, werden alle Nutzer eingeladen an einer Umfrage zur Zufriedenheit mit dem Webshop teilzunehmen. Diese Umfrage soll zeigen, wie sich die gezielt platzierte Werbung auf die Kundenzufriedenheit auswirkt. Nach Abschluss der Pilotstudie wird die Entscheidung getroffen, ob das Modell in Zukunft immer eingesetzt wird, ob basierend auf der Umfrage Verbesserungen mit Bezug auf die Kundenzufriedenheit notwendig sind, oder ob sich der Einsatz nicht lohnt, da die platzierte Werbung ignoriert wird.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="rollen-in-data-science-projekten">
<h2><span class="section-number">2.3. </span>Rollen in Data Science Projekten<a class="headerlink" href="#rollen-in-data-science-projekten" title="Permalink to this headline">¶</a></h2>
<p>In jedem Prozess gibt es <em>Rollen</em> die ausgefüllt werden müssen.</p>
<blockquote>
<div><p><strong>Definition von Rollen</strong>:</p>
<p>Eine Funktion oder Tätigkeit, die einen bestimmen Teil einer Operation oder eines Prozesses durchführt.</p>
</div></blockquote>
<p>Mit anderen Worten: Rollen beschreiben die Verantwortlichkeiten und Aufgaben der Personen, die an einem Prozess beteiligt sind. In der Praxis sind Rollen oft mit Jobtiteln verwandt, zum Beispiel “Softwareentwickler”, “Projektmanager”, oder “Data Scientist”. Im Allgemeinen gibt es jedoch keine eins-zu-eins Beziehung von Rollen und Personen. Eine Person kann mehrere Rollen ausfüllen und mehrere Personen können die die gleiche Rolle haben. Es kann zum Beispiel mehrere “Data Scientists” in einem Projekt geben und diese “Data Scientists” können noch weitere Rollen haben. Einer könnte zum Beispiel noch zusätzlich der “Projektmanager” sein, ein anderer der “Datenbankadministrator”.</p>
<p>Wir betrachten sieben Rollen in Data Science Projekten. Bitte beachten Sie, dass es in der Praxis, insbesondere in großen Projekten oder im Rahmen der Operationaliesierung häufig noch weitere Rollen gibt, zum Beispiel Softwareentwickler, Softwarearchitekten, Cloudarchitekten, Community Manager, und Tester. Außerdem betrachten wir das Rollenmodell aus der Perspektive der Industrie. Es gibt jedoch für jede Rolle auch ein Gegenstück in der akademischen Welt, welches wir auch nennen.</p>
<div class="section" id="anwender">
<h3><span class="section-number">2.3.1. </span>Anwender<a class="headerlink" href="#anwender" title="Permalink to this headline">¶</a></h3>
<p>Die Anwender sind der Teil ihrer Zielgruppe, die später die Projektergebnisse direkt benutzen sollen. Die Anwender sind daher wichtige Stakeholder und häufig auch Domänenexperten die man für ein besseres Verständnis konsultieren kann. Die Anwender können helfen die Daten zu verstehen, der Güte der Ergebnisse einzuordnen, und um die Rolle eines Projekts in einem Geschäftsprozess zu verstehen. Auch wenn man die Anwender immer konsoltieren sollte, ist dies spätestens dann zwingend nötig, wenn die Projektergebnisse operationalisiert werden sollen. In akademischen Projekten sind alle, die die Forschungsergebnisse direkt oder indirekt in ihrer täglichen Arbeit nutzen die Anwender. Wenn Forschung sich bereits im Stadium des Industrietransfer befindet, sind die Anwender die gleichen wie bei industriellen Projekten. In der Grundlagenforschung sind die Anwender in der Regel andere Forscher.</p>
<p>Auch wenn die Anwender eine wichtige Rolle für Projekte sind, was sich auch insbesondere in modernen agilen Prozessmodellen wie Scrum widersprigelt <a class="footnote-reference brackets" href="#scrum" id="id2">2</a>, sind sie häufig nicht Teil des Tagesgeschäfts von Projekten, sondern werden eher zu festen Zeitpunkten konsultiert.</p>
</div>
<div class="section" id="projektsponsor">
<h3><span class="section-number">2.3.2. </span>Projektsponsor<a class="headerlink" href="#projektsponsor" title="Permalink to this headline">¶</a></h3>
<p>Ohne den Sponsor, gäbe es kein Projekt. Die Projektsponsoren entscheiden welche Projekte gestartet werden und stellen die benötigten Resourcen bereit. Entsprechend könnten die Projektsponsoren die Mitglieder des Managements einer Firma sein oder aber auch Kunden die einen Auftrag erteilen. Die Projektsponsoren sind auch für die Beurteilung ob ein Projekt erfolgreich ist wesentlich. Sie entscheiden ob Sie mit den Ergebnissen zufrieden sind und ob weitere Resourcen für Folgeprojekte bereit gestellt werden, zum Beispiel um ein Ergebnis zu operationalisieren. In der akademischen Welt sind die Projektsponsoren in der Regel die <em>Principal Investigator</em>, in der Regel also Professoren und Postdoktoranden.</p>
<p>Ähnlich wie die Anwender, sind die Projektsponsoren in der Regel nicht teil des Tagesgeschäfts. Stattdessen sind sie nur an Meilensteinen involviert, wenn wesentliche Projektergebnisse vorgestellt werden und wenn wichtige Entscheidungen getroffen werden müssen.</p>
</div>
<div class="section" id="projektmanager">
<h3><span class="section-number">2.3.3. </span>Projektmanager<a class="headerlink" href="#projektmanager" title="Permalink to this headline">¶</a></h3>
<p>Der Projektmanager organisiert die Projektarbeit und das Tagesgeschäft. Die Aufgaben umfassen unter anderem die Resourcenplanung (Finanziell, Mitarbeiter, Rechenzeit), sowie die Überwachung des Projektfortschritts. Der Projektmanager ist dafür verantwortlich, dass Meilensteine und Projektziele zu den geplanten Zeitpunkten erreicht und die Qualitätsziele dabei erfüllt werden. Damit dies gewährleistet ist, bewertet der Projektmanager regelmäßig die Risiken und leitet sofern notwendig Maßnahmen zur Minimierung des Risikos ein. Im Extremfall kann das bedeuten, dass der Projektmanager auch entscheiden muss, das ein Projekt gescheitert ist und nicht fortgeführt wird. Alternativ könnten aber auch zusätzliche Resourcen beantragt oder die Ziele angepasst werden. Im akademischen Umfeld ist der Projektmanager in der Regel auch der Projektsponsor, oder ein wissenschaftlicher Mitarbeiter des Projektsponsors.</p>
</div>
<div class="section" id="dateningenieur-engineer">
<h3><span class="section-number">2.3.4. </span>Dateningenieur Engineer<a class="headerlink" href="#dateningenieur-engineer" title="Permalink to this headline">¶</a></h3>
<p>Ein Dateningenieur beschäftigt sich mit dem ELT/ETL der DAten und ist verantwortlich dafür die Daten in einer skalierbaren Analyseumgebung zur Verfügung zu stellen. Die Dateningenieure brauchen daher ein gutes Verständnis der Technologien zum sammeln, speichern, und verarbeiten von Daten. Insbesondere bei Big Data Projekten oder wenn größere Mengen an Daten noch gesammelt werden müssen, kann dies sehr herausfordernd sein. In akademischen Projekten gibt es in der Regel keine Trennung zwischen dem Dateningenieur und dem Data Scientists. In einigen Disziplinen ist das bereitstellen der Daten jedoch so aufwending, das es auch hier spezialisten gibt, zum Beispiel in Genetik und der Hochenergiephysik. Das Human Genome Project <a class="footnote-reference brackets" href="#genome" id="id3">3</a> und das CERN <a class="footnote-reference brackets" href="#cern" id="id4">4</a> sind daher nicht nur für die Biologie und Physik hochrelevent, sondern auch Treiber von Technologien zum Umgang mit großen Datenmengen.</p>
</div>
<div class="section" id="datenbankadministrator">
<h3><span class="section-number">2.3.5. </span>Datenbankadministrator<a class="headerlink" href="#datenbankadministrator" title="Permalink to this headline">¶</a></h3>
<p>Der Datenbankadministrator unterstützt die Dateningeure und Data Scientists durch die Bereitstellung und Administration von Datenbanken als Teil der Analyseumgebung für das Projekt. Die Aufgaben umfassen die Installation und Konfiguration von (verteilten) Datenbanken und Rechenclustern, inklusive der benötigten Werkzeuge für die Datenanalyse. Ob diese Rolle mit dem Dateningenieur oder dem Data Scientist zusammenfällt hängt Organisation ab: Wenn es zum Beispiel ein extra Rechenzentrum gibt oder Organisationseinheiten die für die Nutzung von Cloudresourcen verfügbar sind, ist dies in der Regel separat, anderfalls fallen diese Aufgaben häufig direkt mit andern Tätitkeiten im Projekt zusammen.</p>
</div>
<div class="section" id="data-scientist">
<h3><span class="section-number">2.3.6. </span>Data Scientist<a class="headerlink" href="#data-scientist" title="Permalink to this headline">¶</a></h3>
<p>Der Data Scientists ist der Experte für die Datenanalyse und Modellierung und hierfür verantwortlich. Daher muss man als Data Scientist ein detailiertes Verständnis von Modellierungs- und Analysetechniken mitbringen, damit für ein Projekt geeignete Methoden ausgewählt werden können. Als Data Scientists trägt man wesentliche Verantwortung für den Projekterfolg und das erreichen der Projektziele. Wenn klar wird, das dies nicht möglich ist oder Risiken identifiziert werden, muss man als Data Scientist eng mit dem Projektmanagement zusammenzuarbeiten um geeignete Schritte einzuleiten. Die Rolle des Data Scientists ist eng verwand mit der Rolle von Business Intelligence Analysts. Bei Business Intelligence handelt es sich gewissermaßen um den Vorreiten von Data Science in der Industrie, wobei hier der Fokus auf der häufig eher auf der Analyse von dem Status Quo lag, wobei Data Science Projekte häufig auch in die Zukunft blicken um vorhersagen und automatisierte Entscheidungen zu ermöglichen.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="manmonth"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.1145/390016.808439">https://doi.org/10.1145/390016.808439</a></p>
</dd>
<dt class="label" id="scrum"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p><a class="reference external" href="https://www.scrumalliance.org/">https://www.scrumalliance.org/</a></p>
</dd>
<dt class="label" id="genome"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p><a class="reference external" href="https://www.genome.gov/human-genome-project">https://www.genome.gov/human-genome-project</a></p>
</dd>
<dt class="label" id="cern"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p><a class="reference external" href="https://home.cern/">https://home.cern/</a></p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="kapitel_01.html" title="previous page"><span class="section-number">1. </span>Big Data und Data Science</a>
    <a class='right-next' id="next-link" href="kapitel_03.html" title="next page"><span class="section-number">3. </span>Erkunden der Daten</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Steffen Herbold<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>