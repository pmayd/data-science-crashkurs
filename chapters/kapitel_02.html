
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Der Prozess von Data Science Projekten &#8212; Einführung in Data Science</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Erkunden der Daten" href="kapitel_03.html" />
    <link rel="prev" title="1. Big Data und Data Science" href="kapitel_01.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Einführung in Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../vorwort.html">
   Vorwort
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Kapitel
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_01.html">
   1. Big Data und Data Science
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Der Prozess von Data Science Projekten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_03.html">
   3. Erkunden der Daten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_04.html">
   4. Allgemeines zur Datenanalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_05.html">
   5. Assoziationsregeln
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_06.html">
   6. Clusteranalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_07.html">
   7. Klassifikation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_08.html">
   8. Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_09.html">
   9. Zeitreihenanalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_10.html">
   10. Textmining
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_11.html">
   11. Statistik
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_12.html">
   12. Big Data Processing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Übungen
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercises/uebung_01.html">
   Übung 1
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/chapters/kapitel_02.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/sherbold/einfuehrung-in-data-science"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sherbold/einfuehrung-in-data-science/main?urlpath=tree/content/chapters/kapitel_02.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prozesse">
   2.1. Prozesse
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#der-generische-data-science-prozess">
   2.2. Der generische Data Science Prozess
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discovery">
     2.2.1. Discovery
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#datenvorbereitung">
     2.2.2. Datenvorbereitung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modellplanung">
     2.2.3. Modellplanung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modellerstellung">
     2.2.4. Modellerstellung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kommunikation-der-ergebnisse">
     2.2.5. Kommunikation der Ergebnisse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#operationalisierung">
     2.2.6. Operationalisierung
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rollen-in-data-science-projekten">
   2.3. Rollen in Data Science Projekten
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anwenderin">
     2.3.1. Anwenderin
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#projektsponsorin">
     2.3.2. Projektsponsorin
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#projektmanagerinnen">
     2.3.3. Projektmanagerinnen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dateningenieurinnen">
     2.3.4. Dateningenieurinnen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#datenbankadministratorin">
     2.3.5. Datenbankadministratorin
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-scientist">
     2.3.6. Data Scientist
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deliverables">
   2.4. Deliverables
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sponsorenprasentation">
     2.4.1. Sponsorenpräsentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#analystenprasentation">
     2.4.2. Analystenpräsentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quelltext">
     2.4.3. Quelltext
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#technische-spezifikation">
     2.4.4. Technische Spezifikation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#daten">
     2.4.5. Daten
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="der-prozess-von-data-science-projekten">
<h1><span class="section-number">2. </span>Der Prozess von Data Science Projekten<a class="headerlink" href="#der-prozess-von-data-science-projekten" title="Permalink to this headline">¶</a></h1>
<div class="section" id="prozesse">
<h2><span class="section-number">2.1. </span>Prozesse<a class="headerlink" href="#prozesse" title="Permalink to this headline">¶</a></h2>
<p>Prozesse sind der Kern jeder Aktivität, auch wenn man sich dessen oft gar nicht bewusst ist. Menschen führen Aktivitäten durch das Anwenden von Techniken durch. Die Prozesse steuern und organisieren diese Aktivitäten und beschreiben die Techniken die eingesetzt werden. <a class="reference internal" href="kapitel_01.html#fig-webtraffic"><span class="std std-numref">Fig. 1.1</span></a> zeigt die Beziehung von Menschen, Techniken und Prozessen.</p>
<div class="figure align-default" id="fig-processes">
<a class="reference internal image-reference" href="../_images/processes.png"><img alt="../_images/processes.png" src="../_images/processes.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.1 </span><span class="caption-text">Wachstum des Datenvolumens des Internetverkehrs</span><a class="headerlink" href="#fig-processes" title="Permalink to this image">¶</a></p>
</div>
<p>Das Ziel eines guten Prozesses ist es,die Menschen zu unterstützen, zum Beispiel in dem Sichergestellt wird das wichtige Aktivitäten nicht vergessen werden oder durch die Verwendung von geeigneten Werkzeugen zum Lösen von Problemen. Prozesse erreichen dies, in dem Sie geeignete Best Practices beschreiben. Diese Best Practices sollten basierend auf der erfolgreichen Anwendung in der Vergangenheit bestimmt werden. Hierdurch wird soll das Wissen und der Erfolg aus Vergangenen Projekten konserviert und genutzt werden, um die Fähigkeiten der Menschen zu unterstützen und das Risiko das ein Projekt fehlschlägt zu reduzieren. Dies funktioniert jedoch nur, wenn die Prozesse von Menschen auch unterstützt werden.</p>
<p>Wenn die Menschen den Prozess nicht akzeptieren oder nicht an seinen Nutzen glauben, erreicht man das Gegenteil und erhöht stattdessen das Risiko von Projekten. Daher sollten die Menschen die notwendigen Schulungen erhalten, um die Techniken einzusetzen und ihren Nutzen zu kennen. Außerdem muss man sicherstellen, dass die Techniken auch zum Prozess passen.</p>
<p>Man sollte sich auch immer bewusst sein, das es nicht <em>den einen Prozess</em> gibt, der Perfekt zu jedem Projekt passt. Man sollte den Prozess daher immer an die Situation anpassen, man spricht hier auch vom <em>Tailoring</em>. Hierbei sollte man die zur Verfügung stehenden Techniken und den Projektkontext berücksichtigen, zum Beispiel die Größe und Priorität des Projekts, ob es sicherheitskritische Aspekte gibt oder ob die Mitarbeiterinnen Vorwissen aus ähnlichen Projekten mitbringen.</p>
</div>
<div class="section" id="der-generische-data-science-prozess">
<h2><span class="section-number">2.2. </span>Der generische Data Science Prozess<a class="headerlink" href="#der-generische-data-science-prozess" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="#fig-dsprocess"><span class="std std-numref">Fig. 2.2</span></a> zeigt den einen generischen Prozess für Data Science Projekte, der aus sechs Phasen besteht.</p>
<div class="figure align-default" id="fig-dsprocess">
<a class="reference internal image-reference" href="../_images/process_german.png"><img alt="../_images/process_german.png" src="../_images/process_german.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.2 </span><span class="caption-text">Überblick über den generellen Prozess von Data Science Projekten.</span><a class="headerlink" href="#fig-dsprocess" title="Permalink to this image">¶</a></p>
</div>
<p>Der Prozess ist iterativ, das heißt, dass mehrere Wiederholungen aller Phasen innerhalb eines Projekts möglich sind. Innerhalb einer Iteration, kann man nur zu den vorherigen Phasen zurück, solange man die Ergebnisse der Iteration noch nicht kommuniziert hat. Der Grund hierfür ist offensichtlich: Sobald man die Projektergebnisse übermittelt hat, zum Beispiel an die das Management, die Kunden oder andere Forscher in Form einer Publikation, kann man diese nicht ohne weiteres ändern. Im Folgenden betrachten wir die Projektphasen im Detail.</p>
<div class="section" id="discovery">
<h3><span class="section-number">2.2.1. </span>Discovery<a class="headerlink" href="#discovery" title="Permalink to this headline">¶</a></h3>
<p>Jedes Data Science Projekt beginnt mit der Discovery Phase. Das Ziel der Discovery ist es die Projektdomäne, Ziele und Daten zu verstehen. Anhand der gesammelten Informationen wird beurteilt, ob die für das Projekt notwendigen Ressourcen zu verfügung stehen und die weiteren Projektschritte werden geplant.</p>
<p>Der Data Scientist muss das notwendige Wissen erlangen, um die Domäne des Projekts zu verstehen, insbesondere den Anwendungsfall der adressiert werden soll. Häufig wird der Data Scientist hierbei von Domainenexperteninnen unterstützt, die die notwendigen Informationen liefern. Dies kann zum Beispiel bedeuten, dass sie zur Verfügung stehen um Fragen zu beantworten oder auch das Anforderungen in Form von Interviews und Workshops gesammelt werden. Hierdurch lernt der Data Scientist auch bereits die Daten und Informationsquellen kennen. Dieses Wissen ist notwendig, um das Projekt zu verstehen und die Ergebnisse zu Interpretieren. Es kann sein, dass der Data Scientists bereits selbst Expertin in der Anwendungsdomäne ist, zum Beispiel weil dies nicht das erste derartige Projekt ist. Dennoch ist es häufig sinnvoll, den Anwendungsfall zusammen mit weiteren Domänenexperteninnen zu betrachten um sicherzustellen, das keine wesentlichen Aspekte übersehen werden.</p>
<p>Als Teil des Lernprozesses der Domäne sollten auch die Vergangenheit nicht außer acht gelassen werden. Bei Forschungsprojekten ist es in der akademischen Welt üblich, zuerst die verwandten Forschungsarbeiten zu identifizieren und den Stand der Forschung aufzuarbeiten. Ähnlich sollte man auch in der Wirtschaft vorgehen: Gab es vielleicht bereits frühere Projekte mit einem ähnlichen Ziel? Falls ja, warum sind diese gescheitert, beziehungsweise warum wird dieses Projekt jetzt neu durchgeführt? Die alten Projektergebnisse - sowohl positive, als auch negative - sind ideale Quellen um Projektrisiken zu identifizieren. Hierbei sollte man sich auch nicht nur auf das eigene Unternehmen beschränken. Im Rahmen der Möglichkeiten des Urheber- und Patentrechts ist eine Analyse der Konkurenzprodukte oft Hilfreich, um mögliche Lösungen zu verstehen.</p>
<p>Sobald der Data Scientist die Domäne verstanden hat, kann sie Anfangen aus den genannten Zielen eine Problembeschreibung zu erarbeiten, die durch die Datenanalyse gelöst werden soll. Diese Problembeschreibung ist nicht identisch zu den wirtschaftlichen Zielen oder Forschungsinteressen: Während die Ziele nicht direkt mit den Daten, sondern viel mehr der Anwendung zusammen hängen, formuliert die Problembeschreibung die Ziele als Datenanalyseproblem um. Dies ist dem Data Scientist nur möglich, da sie vorher die Domäne und den Anwendungsfall mit Hilfe der Domänenexperten kennengelernt hat. Zur Problembeschreibung gehört auch eine Analyse der Stakeholder, zum Beispiel müssen die Stakeholder, deren Daten benötigt werden, identifiziert werden. Weitere wichtige Stakeholder sind diejenigen, die von der Analyse direkt betroffen sind, zum Beispiel weil ihre Arbeit dadurch unterstützt werden soll. Neben Stakeholdern müssen auch die aktuellen Probleme aufgearbeitet werden, sofern dies nicht schon geschehen ist. Hierdurch wird die Motivation vom Projekt geklärt und das Verständnis der Ziele verbessert. Anschließend werden die Projektziele, Erfolgskriterien und Risiken des Projekts klar definiert um eine verbindliche Grundlage für das weitere vorgehen zu schaffen.</p>
<p>Durch die oben beschrieben Aktivitäten bekommt der Data Scientists ein Grundverständis der Daten, die im Projekt genutzt werden. Grundsätzlich muss man hierbei zwischen Daten die bereits vorhanden sind (zum Beispiel in einem Data Warehouse) und Daten die noch gesammelt werden müssen unterscheiden. In beiden Fällen obliegt es dem Data Scientists den Umfang, die Struktur, sowie ein abstraktes Verständnis der verfügbaren Informationen zu bekommen. Andernfalls wäre die abschließende Analyse der verfügbaren und benötigten Ressourcen nicht möglich.</p>
<p>Während der Discovery befasst man sich auch mit dem <em>wissenschaftlichen</em> Teil von Data Science. Datenanalysen sollten nicht rein explorativ sein. Stattdessen sollten klare Erwartungen in Form von Hypothesen die getestet werden können formuliert werden. Anderfalls erhöht man das Risiko, das die Projektergebnisse sich nicht über die im Projekt verfügbaren Daten hinaus generalisieren lassen. Die Hypothesen steuern auch das Vorgehen in den weiteren Phasen des Projekts. Es sollten zum Beispiel Erwartungen definiert werden, welche Daten nützlich sind, wie man sie verwenden sollte, und welches Wissen man aus den Daten gewinnen möchte. Auch wenn der Data Scientists Hauptverantworlich für die Hypothesen ist, sollten diese immer mit Domänenexperteninnen besprochen werden um ihre Plausibilität zu prüfen.</p>
<p>Der letzte Schritt der Discovery ist die Entscheidung ob das Projekt machbar ist. Diese Auswertung sollte die identifizierten Risiken und die verfügbaren Ressourcen berücksichtigen. In jedem Fall sollten die folgenden Ressourcen berücksichtigt werden:</p>
<ul class="simple">
<li><p>Die <em>technologische Ressourcen</em>, zum Beispiel Datenspeicher, verfügbare Rechenkraft, sowie die Verfügbarkeit und die Kosten eventuell benötigter Softwarelizenzen.</p></li>
<li><p>Die <em>benötigten Daten</em>, das heißt ob bereits ausreichend Daten vorhanden sind oder ob es mit vertretbarem Aufwand im Rahmen des Projektes die benötigten Daten zu sammeln. Die Betrachtung besteht aus zwei Aspekten: 1) Ist die Anzahl der Datenpunkte ausreichend? 2) Hat man für jeden Datenpunkt die benötigten Informationen. Sollten noch Daten gesammelt werden müssen, sollte man dies bei den Projektrisiken immer berücksichtigen.</p></li>
<li><p>Die <em>Arbeitszeit</em>, sowohl in Kalenderzeit, als auch in Personenmonaten. Die Kalenderzeit ist die Dauer des Projekts. Für Projekte deren Kalendarzeit weniger als ein Jahr ist, sollte man auch noch berücksichten, in welchen Monaten des Jahres das Projekt durchgeführt werden soll, da die üblichen Urlaubszeiten bedeuten können, das Mitarbeiterinnen nicht wie erwartet zur Verfügung stehen. Wenn Projekte in einem internationalen Umfeld durchgeführt werden, sollte man hierbei auch die lokalen Gepflogenheiten der jeweiligen Projektpartner nicht vergessen. Personenmonate sind ein verbreitetes Mittel um den Entwicklungsaufwand, der für ein Projekt investiert wird, abzuschätzen. Zwei Mitarbeiterinnen, die je einen Monat am Projekt arbeiten, entspricht zwei Personenmonaten. Man sollte bei der Betrachtung der Arbeitszeit aber nie vergessen, dass zwei Mitarbeiterinnen in der Regel nicht doppelt so effizient sind wie eine Mitarbeiterinnen. Diese Phänomen ist in der Softwareentwicklung auch als der <em>Mythical Man-Month</em> <a class="footnote-reference brackets" href="#manmonth" id="id1">1</a> bekannt.</p></li>
<li><p>Die <em>Mitarbeiterinnen</em> (gerne auch Human Ressources genannt), das heißt die Personen, die für das Projekt zur Verfügung stehen. Hierbei sollte insbesondere betrachtet werden, ob das Fähigkeitsprofil der Mitarbeiterinnen zu den Anforderungen des Projekts passt.</p></li>
</ul>
<p>Sofern ausreichend Ressourcen verfügbar sind und die Risiken kontrolierbar, kann das Projekt gestartet werden.</p>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Ein Kunde ist Besitzer eines Webshops für Kleidung. Der Kunde möchte gerne die Anzahl seiner Verkäufe durch <em>Cross-Sell</em> erhöhen, das heißt Kunden sollen dazu gebracht werden weitere Produkte in den Warenkorb zu legen. Unsere Aufgabe ist es eine Anwendung hierfür zu entwickeln, welche auf den vergangen Verkaufsdaten basiert. Die Discovery könnte in etwa so aussehen:</p>
<ul class="simple">
<li><p>Wir führen ein Interview mit dem Kunden durch um Herauszufinden, ob der Kunde bereits eine Idee hat wie mehr Cross-Sell ermöglicht werden könnte. Wir finden raus, das der Kunde gezielte Werbung während des Einkaufs schalten möchte, sobald ein Produkt in den Warenkorb gelegt wird. Diese Information liefert uns eine Kernanforderung des Projekts, andernfalls wäre zum Beispiel auch Emailmarketing denkbar gewesen.</p></li>
<li><p>Wir gucken uns Webshops an, die bereits ähnliche Lösungen benutzen.</p></li>
<li><p>Wir definieren die Problembeschreibung als das Ziel geeignete Werbung vorherzusagen, basierend auf dem vergangenen Verhalten des Kunden, dem vergangenen Verhalten aller Kunden, sowie dem aktuellen Inhalt des Warenkorbs.</p></li>
<li><p>Wir identifizieren zwei wichtige Stakeholder: 1) Den Besitzer des Webshops als Auftraggeber, Ansprechpartner auf Kundenseite und Domänenexperten. 2) Die Kunden des Webshops, die für sich relevante Produkte kaufen wollen und eine gute User Experience (UX) bei der Benutzung der Software wünschen. Unpassende Werbung könnte die Benutzererfahrung verschlechtern, sehr gute Werbung sogar verbessern.</p></li>
<li><p>Wir identifizieren keine Probleme mit dem Status Quo, die gelöst werden sollen. Es geht also um eine reine Optimierung der Einnahmen.</p></li>
<li><p>Aus diesen Erkenntnissen definieren wir zwei konkrete Projektziele:</p>
<ol class="simple">
<li><p>Erhöhung der Anzahl der verkauften Produkte und dadurch des Umsatzes.</p></li>
<li><p>Gleichbleibende oder verbesserte UX.</p></li>
</ol>
</li>
<li><p>Die Erfüllung der Projektziele soll durch die Beobachtung des Umsatzes, sowie durch eine Umfrage unter den Benutzern bezüglich ihrer Zufriedenheit, bestimmt werden. Das Projekt gilt als Erfolgreich, wenn der Umsatz sich um mindestens 5% erhöht und die Benutzerzufriedenheit sich nicht verschlechtert. Sie identifizieren eine mögliche Verschlechterung der Benutzerzufriedenheit die zu einem Abfall des Umsatzes führt wird als Hauptrisiko des Projekts.</p></li>
<li><p>Als Daten stehen uns hauptsächlich die Transaktionen von vergangenen Einkäufen zur Verfügung. Diese Daten beinhalten welche Kunden welche Produkte innerhalb einer Bestellung gekauft haben. Die Daten liegen in einer relationalen Datenbank vor. Weitere Daten stehen uns nicht zur Verfügung.</p></li>
<li><p>Wir formulieren drei Hypothesen:</p>
<ol class="simple">
<li><p>Produkte die in der Vergangenheit häufig zusammen gekauft wurden, werden auch in der Zukunft häufig zusammen gekauft.</p></li>
<li><p>Es gibt saisonale Muster in den Verkaufsdaten (zum Beispiel für Winter- und Sommerkleidung), welche relevant sind für die Werbung.</p></li>
<li><p>The Kategorien, durch die sich die Produkte beschreiben lassen, sind relevant für die Werbung, insbesondere die Marken und die Art der Kleidung.</p></li>
</ol>
</li>
<li><p>Wir sind der Meinung das die Ressourcen ausreichen, um eine Pilotstudie durchzuführen, in der die Machbarkeit von nützlichen Vorhersagen für Cross-Sell Werbung geprüft wird. Eine detailierte Evaluation der Benutzererfahrung, sowie eine Operationationalisierung der Ergebnisse für produktiven Betrieb ist mit den zur Verfügung stehenden Ressourcen nicht realistisch. Im Falle einer erfolgreichen Machbarkeit der Vorhersagen, wird dies in einem Folgeprojekt umgesetzt.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="datenvorbereitung">
<h3><span class="section-number">2.2.2. </span>Datenvorbereitung<a class="headerlink" href="#datenvorbereitung" title="Permalink to this headline">¶</a></h3>
<p>Mit dem Abschluss der Discovery beginnt die technische Arbeit des Projekts, in dem die Daten für die Analyse vorbereitet werden. Hierbei gibt es zwei wesentliche Ziele:</p>
<ol class="simple">
<li><p>Die Infrastruktur für die Datenanalyse muss erstellt werden und alle relevanten Daten müssen in diese Infrastruktur geladen werden.</p></li>
<li><p>Der Data Scientists sollte ein tiefgehendes und detailiertes Verständnis der Daten erlangen.</p></li>
</ol>
<p>Der Aufwand für die Vorbereitung der Infrastruktur hängt stark vom Projekt ab. Es kann nahezu trivial sein und mit wenigen Zeilen Quelltext erledigt sein, oder mehrere Personenjahre an Ressourcen verschlingen. Ist das Datenvolumen relativ klein und die Daten können durch eine einzelne SQL-Abfrage geladen werden, ist diese Aufgabe in kürzester Zeit erledigt. Handelt es sich wiederum um ein Big Data Projekt, in dem die Daten erst noch gesammelt werden müssen oder wo der Zugriff auf die Daten schwierig ist (zum Beispiel aus Sicherheits- oder Datenschutzbedenken), kann dies extrem aufwendig sein.</p>
<p>Der grundsätzliche Prozess des Ladens von Daten in die Analyseinfrastruktur wird <em>ETL</em> genannt: <em>Extract</em>, <em>Transform</em>, <em>Load</em>. Zuerst werden die Daten von ihrem aktuellen Speicherort extrahiert. Dies bedeutet das der Code zum Laden der Daten aus Dateien, Datenbanken, oder zum Sammeln von Daten aus anderen Quellen (zum Beispiel dem Internet durch “Scraping”) geschrieben wird. Sobald die Daten extrahiert sind, werden sie in das benötigte Format konvertiert. Diese Transformation beinhaltet üblicherweise auch die Qualitätskontrolle der Daten: Zum Beispiel kann man hier unvollständige oder implausible Datenpunkte entfernen. Weiterhin müssen die Daten häufig restrukturiert und in andere Formate konvertiert werden. Das kann zum Beispiel bedeuten, dass Informationen aus verschiedenen Quellen integriert werden. Es kann aber auch heißen das Informationen neu aufgeteilt werden. Zum Beispiel könnte man den Inhalt von Blogposts in verschiedene Felder aufteilen: Titel, Inhalt, und Kommentare. Bei textuellen Daten kann es auch herausfordernd sein, diese in ein einheitliches Textformat zu konvertieren, da es viele verschiedene Kodierungen für Textdaten gibt, zum Beispiel ASCII, ISO-8859, UTF-8 und UTF-16, um nur einige gängige zu nennen. Ähnliche Probleme kann es bei Datumformaten geben. Ob 04/05/21 sich auf den vierten Mai oder den fünften April bezieht, hängt davon ab ob es sich um die amerikanische oder britische Konvention handelt. Ob es sich um das Jahr 2021 oder 1921 hängt vom Zeitpunkt ab, an dem dieses Datum geschrieben wurde. Sobald alle Daten transformiert sind, können sie in die Analyseumgebung geladen werden.</p>
<p>Häufig kann man ETL auch Abwandeln, in dem man die Transformation und das Laden der Daten vertauscht, also ELT. In diesem Fall werden die Rohdaten direkt in die Analyseumgebung geladen und benötigte Transformationen werden innerhalb der Analyseumgebung durchgeführt. Ob ETL oder ELT die bessere Wahl ist, hängt vom Anwendungsfall ab. Ein guter Grund, warum man ELT statt ETL nutzen sollte ist, das die Transformationen so komplex sind, dass man sie ohne die Rechenkraft der Analyseumgebung nicht durchführen kann. Ein weiterer Vorteil von ELT ist, dass man verschieden Transformationen ausprobieren und flexibel miteinander vergleiche kann. Zuletzt gibt es auch Anwendungen, die vom Zugriff auf die Rohdaten profitieren können, da diese zum Beispiel dann auch von Algorithmen als Merkmal verwendet werden können. Auf der andern Seite ist ETL zu favorisieren, wenn die Transformationen sehr Zeitaufwendig sind und nicht mehrfach bei jedem Laden der Daten durchgeführt werden sollen, oder wenn man die Transformationen direkt bei einem Datenbankzugriff durchführen kann.</p>
<p>Ein weiterer wesentlicher Aspekt der Datenvorbereitung ist das detailierte Verständnis der Daten. Dies bedeutet das Studium der Dokumentation der Daten. Sofern diese nicht vorhanden oder nicht ausreichend ist, müssen die Daten mit Hilfe von Domänenwissen interpretiert werden. Im Idealfall kennt der Data Scientists zum Abschluss dieser Phase alle Details der Daten, zum Beispiel die Bedeutung jeder Spalte in einer relationalen Datenbank oder welche Arten von Dokumenten es gibt und wie diese Strukturiert sind. Diese Arbeit kann man auch als Lernen der <em>Metadaten</em> bezeichnen, also der Daten über die Daten. Zusätzlich zu den Metadaten, sollten auch die Daten selbst <em>erkundet</em> werden - eine Aktivität die häufig eng mit den Transformation von ETL zusammenhängt. Hierzu betrachtet man zum Beispiel Statistiken und Visualusierungen (siehe <a class="reference internal" href="kapitel_03.html"><span class="doc std std-doc">Kapitel 3</span></a>). Hierdurch kann man zum Beispiel Erkenntnisse über die Wahrscheinlichkeitsverteilungen der Daten gewinnen, ungültige Daten identifizieren, oder Skaleneffekte entdecken und entfernen um die Daten besser zu vereinheitlichen.</p>
<p>Diese detailierte Betrachtung der Daten erlaubt es dem Data Scientist auch zu erkennen, welche Daten wirklich Wertvoll für das Projekt sind und welche Daten man eventuell doch nicht benötigt werden. Hierbei muss man ein gesundes Mittelmaß finden: Auf der einen Seite geht man ein Risiko ein, wenn man Daten frühzeitig entfernt, da man etwas übersehen haben könnte und die Daten eventuell doch nützlich wären. Auf der andern Seite reduziert man die Komplexität des Projekts, wenn weniger Daten vorhanden sind. Insbesondere wenn große Datenmengen entfernt werden können, kann sich der Analyseaufwand stark reduzieren.</p>
<p>Am Ende der Daten Vorbereitung sind alle Daten für die Analyse verfügbar und alle benötigte Transformationen sind definiert und durchgeführt.</p>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Die Verkaufsdaten sind in einer relationalen Datenbank gespeichert, welche 352.152 Transaktionen beinhaltet. Im Mittel wurden 2,3 Gegenstände in einer Transaktion gekauft. Für jede Transaktion ist ein Zeitstempel im ISO 8601 Format und pseudonymizierter Identifier des Benutzers, der die Gegenstände gekauft hat, verfürbar. In einer zweiten Tabelle werden die Informationen über die Gegenstände selbst gespeichert, unter anderem: Der Preis, die Kategorien denen ein Gegenstand zugeordnet ist (Männerkleidung, Frauenkleidung, Hose, Pullover, Socken, Herstellermarke). Es gibt noch weitere Daten, wie zum Beispiel die Bezahlart. Diese weiteren Daten werden jedoch als für dieses Projekt nicht relevant eingestuft und nicht für die Analyse verwendet.</p>
<p>Das Datenvolumen ist ca. ein Gigabyte. Daher entscheiden wir uns für einen ELT Prozess, da das Laden der Daten aus der Datenbank nur etwa eine Minute dauert und man die Daten flexibel in der Analyseumgebung erkunden kann und während dessen die benötigten Transformationen definiert.</p>
<p>Während dem Erkunden der Daten entdecken wir 2.132 Transaktionen ohne Gegenstände. Diese entfernen wir, da es sich um ungültige Datenpunkte handelt. Außerdem stellen wir fest, dass die Kleidung einiger Marken nur selten gekauft werden. Daher fassen wir diese Marken in einer neuen Kategorie “Sonstige Marken” zusammen.</p>
<p>Wir bestimmen außerdem vier Repräsentationen der Transaktionen, die für die Cross-Sell Analyse nützlich sein könnten:</p>
<ul class="simple">
<li><p>Identisch zur Datenbank, das heißt direkt durch die verkauften Gegenstände.</p></li>
<li><p>Die Gegenstände werden durch die Art der Kleidung ersetzt.</p></li>
<li><p>Die Gegenstände werden durch die Marke ersetzt.</p></li>
<li><p>Die Gegenstände werden durch die Kombination von der Art der Kleidung und der Marke ersetzt.</p></li>
</ul>
<p>Die verschiedenen Repräsentation ermöglichen es Ihnen, den Cross-Sell auf bestimmte Kleidungstypen oder Marken zu fokusieren.</p>
</div></blockquote>
</div>
<div class="section" id="modellplanung">
<h3><span class="section-number">2.2.3. </span>Modellplanung<a class="headerlink" href="#modellplanung" title="Permalink to this headline">¶</a></h3>
<p>Das Ziel der Modellplanung ist das Design des Analysemodells. Hierzu muss man aus den verschiedenen Möglichkeiten, wie die Datenanalyse gestaltet werden kann ein geeignetes Modell auswählen, welches sowohl zu den Daten, als auch zum Analyseziel passt. Es gibt verschiedene Aspekte, die bei der Modellauswahl berücksichtigt werden müssen. Das Projektziel gibt üblicherwise die grundsätzliche Art des Modells vor:</p>
<ul class="simple">
<li><p><em>Assoziationsregeln</em> können benutzt werden um Regeln zu finden, welche relevante Beziehungen innerhalb von Transaktionen beschreiben (<a class="reference internal" href="kapitel_05.html"><span class="doc std std-doc">Kapitel 5</span></a>).</p></li>
<li><p><em>Clusteranalyse</em> ist dazu geeignet Gruppierungen innerhalb von Daten zu finden (<a class="reference internal" href="kapitel_06.html"><span class="doc std std-doc">Kapitel 6</span></a>).</p></li>
<li><p><em>Klassifikation</em> wird benutzt um Vorherzusagen, zu welcher Kategorie Daten gehören (<a class="reference internal" href="kapitel_07.html"><span class="doc std std-doc">Kapitel 7</span></a>).</p></li>
<li><p><em>Regressionsmodelle</em> beschreiben den Zusammenhang zwischen Merkmalen der Daten und können benutzt werden um kontinuierliche Werte vorherzusagen (<a class="reference internal" href="kapitel_08.html"><span class="doc std std-doc">Kapitel 8</span></a>).</p></li>
<li><p><em>Zeitreihenanalyse</em> berücksichtigt zeitliche Abhängigkeiten zwischen Datenpunkten um zukünftige Entwicklungen abzuschätzen (<a class="reference internal" href="kapitel_09.html"><span class="doc std std-doc">Kapitel 9</span></a>).</p></li>
</ul>
<p>Bei der Modellplanung muss man aus den vielen Möglichkeiten, wie man zum Beispiel ein Klassifikationsmodell erstellen kann, eine für das Projekt passende auswählen. Es gibt viele Aspekte die diese Auswahl beeinflussen. Hier sind einige wichtige Fragen, deren Beantwortung bei der Modellauswahl hilft.</p>
<ul class="simple">
<li><p>Ist es wichtig, dass man die Modelldetails als Mensch nachvollziehen kann oder reicht es, wenn das Modell entsprechend definierter Metriken gut ist? Ein <em>White-Box</em> Modell kann vom Data Scientist und eventuell sogar von Domänenexperten benutzt werden, um die Logik, die das Modell verwendet, im Detail nachzuvollziehen. Dies kann zum Beispiel wichtig sein, damit die Nutzer des Modells in der Lage sind, Verantwortung für die Entscheidungen, die mit Hilfe der Modelle getroffen werden, zu übernehmen. Wenn dies für den Anwendungsfall als weniger wichtig betrachtet wird, kann man auch <em>Black-Box</em> Modelle verwenden. Hier versteht man dann zwar in der Regel nicht, wie die Entscheidungen zu stande gekommen sind, dafür sind solche Modelle häufig Präziser. Der Grund für die höhere Präsizion ist, das die interne Komplexität nicht von der Interpretierbarkeit beschränkt ist. Ein Beispiel für ein White-Box Modell sind Entscheidungsbäume, in denen anhand einfacher Regeln (größer, kleiner, gleich) Merkmale in einer festgelegten Reihenfolge bewertet werden, um zu einer Entscheidung zu gelangen. Ein Beispiel für Black-Box Modelle sind tiefe neuronale Netze (engl. Deep Neural Networks, DNN), in es Millionen von Parameter gibt, die eine für Menschen nicht nachvollziehbare mathematische Funktion beschreiben. Eventuell ist die Nachvollziehbarkeit der Entscheidungen sogar ein zwingendes Kriterium, wie es derzeit (für einige Anwendungen ) im Rahmen einer neuen EU Richtlinie für künstliche Intelligenz angedacht wird.</p></li>
<li><p>Wie hoch ist das Datenvolumen? Obwohl in der Regel alle Modelle besser werden, wenn mehr Daten zur Verfügung stehen, gibt es starke Unterschiede bezüglich zwischen den Modellen. Auf der einen Seite ist die Modellperformance, die sich mit der Datengröße für unterschiedliche Modelle mit anderen Trends entwickelt (<a class="reference internal" href="#fig-ceiling"><span class="std std-numref">Fig. 2.3</span></a>). Auf der anderen Seite ist die Laufzeit zum Training und für die Vorhersagen: Wenn die Berechnungen zu komplex sind, kann es schwierig sein ein Modell mit einem großen Datenvolumen zu lernen.</p></li>
<li><p>Welche Modelle wurden in der Vergangenheit in diesem Kontext erfolgreich verwendet? Hier sollte man auf das gesammelte Wissen aus der Discovery zurückgreifen.</p></li>
<li><p>Mit welchen Modellen hat das Projektteam die meiste Erfahrung? Gerade wenn es mehrere geeignete Kandidaten gibt, ist es Sinnvoll, ein Modell zu wählen was von den Mitarbeiterinnen bereits gut verstanden wird.</p></li>
</ul>
<p>Damit der Data Scientist ein geeignetes Modell auswählen können, ist es deshalb wichtig, dass man nicht nur detailwissen über wenige Modelle habe. Stattdessen sollten Data Scientists eine Vielzahl von Modellen kennen, um für jeden Kontext geeignete Modelle identifzieren zu können.</p>
<div class="figure align-default" id="fig-ceiling">
<a class="reference internal image-reference" href="../_images/ceiling.png"><img alt="../_images/ceiling.png" src="../_images/ceiling.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.3 </span><span class="caption-text">Vergleich der Güte zweier fiktiver Modelle, bezogen auf die Datengröße. Das Orange Modell ist besser, wenn nur wenige Daten zur verfügbar sind. Das Blaue Modell ist besser, wenn viele Daten verfügbar sind. In diesem fiktiven Beispiel könnte das orange Modell für Random Forests und das blaue Modell für tiefe neuronale Netze stehen. Diese Modelle werden in <a class="reference internal" href="kapitel_07.html"><span class="doc std std-doc">Kapitel 7</span></a> eingeführt.</span><a class="headerlink" href="#fig-ceiling" title="Permalink to this image">¶</a></p>
</div>
<p>Auch wenn die Auswahl des Modells ein entscheidener Faktor für den Projekterfolg ist, macht Sie in der Regel nur einen relativ kleinen Teil der Modellplanung aus. Weitere wichtige Aktivitäten sind:</p>
<ul class="simple">
<li><p>Die Auswahl von Merkmalen (engl. feature), welche die Grundlage für die Modelle sind. Hierbei müssen auch die genauen Berechnungsvorschriften für die Merkmale definiert werden und der Umgang mit Daten, die nicht plausibel sind.</p></li>
<li><p>Die Auswertungskriterien für die Modelle müssen durch Gütemaße definiert werden, mit denen die Modellqualität bewertet werden kann. Die Güßtemaße können zum Beispiel in Bezug auf die allgemeine Genauigkeit oder bestimmte Fehlerklassen definiert werden</p></li>
<li><p>Statistische Methoden und Visualisierungen zur Interpretation der Ergebnisse müssen definiert werden.</p></li>
<li><p>Die Daten müssen eventuell in verschieden Teilmengen unterteilt werden, zum Beispiel in Trainingsdaten zum Lernen von Modellen, Validationsdaten zur Auswahl des besten Modells, und Testdaten zur Bewertung der Güte des besten Modells (siehe <a class="reference internal" href="kapitel_04.html"><span class="doc std std-doc">Kapitel 4</span></a>).</p></li>
<li><p>Im Fall von Big Data muss eine Arbeitsumgebung geschaffen werden, in der man mit den Daten, oder einer kleinen Teilmenge der Daten, experimentieren kann.</p></li>
</ul>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Wir entscheiden uns für den Apriori Algorithmus zum bestimmen von Assoziationsregeln, um Produkte, die häufig gemeinsam gekauft wurden, zu identifizieren. Für die Varianten der Daten, in denen wir die Produkte durch Kategorien ersetzt haben, möchten wir ein Random Forest Regressionsmodell nutzen, um vorherzusagen, mit welcher Wahrscheinlichkeit bestimmte Produkte gekauft werden, gegeben dem Inhalt des Warenkorbs. Je nach Qualität der Ergebnisse dieser Modelle, planen wir außerdem noch die Modelle gegebenenfalls in einer zweiten Iteration zu verändern um Verbesserungen zu erreichen. Als Risiko, welches zu einer solchen zweiten Iteration führen kann, sehen wir, das eventuell zu viele günstige Produkte vorhergesagt werden, so dass selbst richtige Vorhersagen nur geringe Auswirkungen auf den Umsatz haben würden. Die Güte der Modellqualität bewerten wir basierend darauf, wie häufig zufällig aus dem Warenkorb entfernte Gegenstände richtig vorhergesagt werden. Um die Analyse der Modellqualität zu unterstützen, planen wir eine Visualisierung zu erstellen, die uns hilft zu verstehen welche Produkte häufig gemeinsam gekauft werden. Außerdem entschließen wir uns unsere Daten anhand der Zeitstempel in drei Teildatensätze zu unterteilen: Die ältesten 50% der Daten sollen zum Trainieren der Modelle benutzt werden, die nächsten 25% als Validierungsdaten, und die neusten 25% als Testdaten.</p>
</div></blockquote>
</div>
<div class="section" id="modellerstellung">
<h3><span class="section-number">2.2.4. </span>Modellerstellung<a class="headerlink" href="#modellerstellung" title="Permalink to this headline">¶</a></h3>
<p>Zur Modellerstellung müssen wir den notwendigen Code schreiben um die geplanten Modelle zu trainieren. Häufig gibt es mehrere Iterationen zwischen der Modellplanung und der Modellerstellung. Innerhalb dieser Iterationen wird das Modell stückweise basierend auf den Erkenntnissen von Zwischenauswertungen verbessert. So können zum Beispiel Probleme, die man spät in den Daten entdeckt, noch behoben werden oder Potentiale für die Verbesserung genutzt werden. Wichtig ist jedoch, das für diese iterative Verbesserungen nur die Trainings- und Validierungsdaten verwendet werden. Die Testdaten darf man nur zu abschließenden Bewertung der Modellqualität benutzen. Wenn die Testdaten mehrfach benutzt werden, degenerieren sie zu Validierungsdaten (siehe <a class="reference internal" href="kapitel_04.html"><span class="doc std std-doc">Kapitel 4</span></a>).</p>
<p>Man könnte die Modellplanung und die Modellerstellung auch als eine Phase betrachten. Der Grund, weshalb wir dies hier als getrennte Phasen betrachten, ist das die Modellerstellung sehr teuer und zeitintensiv sein kann, zum Beispiel wenn riesige Neuronale Netze in einem gemieteten Cluster in der Cloud trainiert werden. Da dies nicht oft wiederholt werden kann, hilft es das Training als separaten Teil des Prozesses hervorzuheben.</p>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Wir entschließen uns das Modell mit der Programmiersprache Python basierend auf den Bibliotheken pandas, scikit-learn, und mlextend zu erstellen. Wir können das Modell auf einer normalen Workstation trainieren und evaluieren.</p>
</div></blockquote>
</div>
<div class="section" id="kommunikation-der-ergebnisse">
<h3><span class="section-number">2.2.5. </span>Kommunikation der Ergebnisse<a class="headerlink" href="#kommunikation-der-ergebnisse" title="Permalink to this headline">¶</a></h3>
<p>Irgendwann kommt der Punkt, an dem der Zyklus von iterativen Modellplanungs- und Modellerstellungsschritten beendet wird und die finalen Projektergebnisse kommuniziert werden. Üblicherweise ist dies entweder der Fall, wenn die Zeit oder die geplanten Ressourcen verbraucht sind, die gewünschte Modellqualität erreicht ist, keine Verbesserungen mehr möglich sind oder es absehbar ist, das kein adequates Modell gefunden wird. Die Projektergebnisse müssen dann den relevanten Stakeholdern mitgeteilt werden. Welche dies sind, hängt vom Projekt ab. In der Industrie können das zum Beispiel die Kunden oder das höhere Management sein. In der Forschung könnte das der Betreuer einer Abschlussarbeit sein oder andere Forscher. Die Form, in der die Ergebnisse kommuniziert werden hängt ebenfalls vom Kontext ab. Häufig gibt es eine Abschlusspräsentation, eventuell gibt es auch Projektberichte oder wissenschaftliche Fachartikel.</p>
<p>Egal in welcher Form die Ergebnisse kommunziert werden, der wichtigste Aspekt ist das klar wird ob die Projektziele erreich wurden oder nicht und, als direkte Folgerung, ob das Projekt erfolgreich war. Zusätzlich sollte man noch die wichtigsten Erkenntnisse des Projekts herausstreichen, zum Beispiel ob das Modell zu Gewinne erhöht oder Kosten reduziert, wie der erwartete Return on Investment (ROI) ist oder welche Auswirkungen auf das operative Geschäft oder weitere Projekte erwartet werden. Domänenexperten sind häufig auch an Details interesiert, zum Beispiel welche Merkmale wichtig sind und wie diese mit den Ergebnissen zusammen hängen. Gerade wenn etwas unerwartetes passiert, kann dies zu spannenden Erkenntnissen führen. Bei Forschungsprojekten kommt hinzu, dass man auch klar kommunizieren muss, inwiefern die Erkenntnisse den Stand der Forschung voran bringen.</p>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Die Assoziationsregeln für Kategorien von Produkten in Kombination mit der Vorhersage von geeigneten Gegenständen durch das Regressionsmodell liefert die besten Ergebnisse. Wir schätzen das sich die Verkäufe um 10% erhöhen lassen und das sich der Umsatz hierdurch um 6% erhöhen wird. Wir finden außerdem heraus, das unsere Modelle nur dann zuverlässig Funktionieren, wenn wir zwischen Frauen- und Männerkleidung unterscheiden. Andernfalls sagt das Regressionsmodell zu häufig Frauenkleidung vorraus.</p>
</div></blockquote>
</div>
<div class="section" id="operationalisierung">
<h3><span class="section-number">2.2.6. </span>Operationalisierung<a class="headerlink" href="#operationalisierung" title="Permalink to this headline">¶</a></h3>
<p>Wenn die Ergebnisse des Modells gut genug sind, wird unter Umständen die Entscheidung getroffen, dass Modell im operationellen Betrieb einzusetzen. Hiermit ist in der Regel ein signifikanter Aufwand verbunden: Oft muss das Modell in der operationellen Umgebung neu Implementiert werden. Hierbei müssen sowohl die Sicherheitsaspekte der Umgebung, als auch die Lauftzeitanforderungen respektiert werden. Wenn das Modell zum Beispiel direkt innerhalb einer Datenbank ausgeführt werden soll, muss man es in der Regel von Grund auf neu programmieren. Aufgrund dieses oft hohen Aufwands, ist die Operationalisierung oft ein separates Projekt.</p>
<p>Wenn man sich dafür entscheidet, ein Modell in den operationellen Betrieb zu übernehmen sollte dies pilotiert werden, also zuerst in einem kleinen Kontext ausprobieren. Diese Pilotstudie ist ein Sicherheitheitnetz und soll sicherstellen, dass die Erwartungen bezüglich der Modelleigenschaften, die man auf den Testdaten überprüft hat, auch im operationellen Betrieb gültig sind. Es kann zum Beispiel Unterschiede geben, weil sich die Benutzer nicht so verhalten, wie dies erwarten, weil sich das Nutzerverhalten im Laufe der Zeit verändert hat</p>
<p>Die Alterung der Daten ist ein Problem für die Operationalisierung, welches man nicht unterschätzen sollte: Nicht nur die Daten altern, sondern auch die Modelle. Als Folge verschlechtert sich häufig die Güte der Modelle mit der Zeit. Damit dies kein Problem wird, sollte als Teil der Operationalisierung definiert werden, wann und wie die Modellqualität regelmäßig überprüft wird und unter welchen Umständen das Modell durch ein auf neuen Daten trainiertes Modell ersetzt wird.</p>
<blockquote>
<div><p><strong>Beispiel:</strong></p>
<p>Unser Kunde ist zufrieden mit den Projektergebnissen und möchte das Modell in seinem Webshop durch eine Pilotstudie testen. Die Pilotstudie zeigt die Vorhersagen bei zufällig ausgewählten Kunden und protokoliert ob weitere Produkte basierend auf den Vorhersagen gekauft wurden. Die Dauer der Pilotstudie ist ein Monat. Unabhängig davon, ob Nutzer die Vorhersagen sehen oder nicht, werden alle Nutzer eingeladen an einer Umfrage zur Zufriedenheit mit dem Webshop teilzunehmen. Diese Umfrage soll zeigen, wie sich die gezielt platzierte Werbung auf die Kundenzufriedenheit auswirkt. Nach Abschluss der Pilotstudie wird die Entscheidung getroffen, ob das Modell in Zukunft immer eingesetzt wird, ob basierend auf der Umfrage Verbesserungen mit Bezug auf die Kundenzufriedenheit notwendig sind, oder ob sich der Einsatz nicht lohnt, da die platzierte Werbung ignoriert wird.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="rollen-in-data-science-projekten">
<h2><span class="section-number">2.3. </span>Rollen in Data Science Projekten<a class="headerlink" href="#rollen-in-data-science-projekten" title="Permalink to this headline">¶</a></h2>
<p>In jedem Prozess gibt es <em>Rollen</em> die ausgefüllt werden müssen. Eine Rolle ist eine Funktion oder Tätigkeit, die einen bestimmen Teil einer Operation oder eines Prozesses durchführt. Mit anderen Worten: Rollen beschreiben die Verantwortlichkeiten und Aufgaben der Personen, die an einem Prozess beteiligt sind. In der Praxis sind Rollen oft mit Jobtiteln verwandt, zum Beispiel “Softwareentwicklerin”, “Projektmanagerin”, oder “Data Scientist”. Im Allgemeinen gibt es jedoch keine eins-zu-eins Beziehung von Rollen und Personen. Eine Person kann mehrere Rollen ausfüllen und mehrere Personen können die die gleiche Rolle haben. Es kann zum Beispiel mehrere “Data Scientists” in einem Projekt geben und diese “Data Scientists” können noch weitere Rollen haben. Eine könnte zum Beispiel noch zusätzlich die “Projektmanagerin” sein, eine anderer die “Datenbankadministratorin”.</p>
<p>Wir betrachten sieben Rollen in Data Science Projekten. Bitte beachten Sie, dass es in der Praxis, insbesondere in großen Projekten oder im Rahmen der Operationalisierung häufig noch weitere Rollen gibt, zum Beispiel Softwareentwicklerinnen, Softwarearchitekteninnen, Cloudarchitekteninnen, Community Managerinnen, und Testerinnen. Außerdem betrachten wir das Rollenmodell aus der Perspektive der Industrie. Es gibt jedoch für jede Rolle auch ein Gegenstück in der akademischen Welt, welches wir auch nennen.</p>
<div class="section" id="anwenderin">
<h3><span class="section-number">2.3.1. </span>Anwenderin<a class="headerlink" href="#anwenderin" title="Permalink to this headline">¶</a></h3>
<p>Die Anwenderinnen sind der Teil der Zielgruppe, die später die Projektergebnisse direkt benutzen sollen. Die Anwenderinnen sind daher wichtige Stakeholder und häufig auch Domänenexperteninnen die man für ein besseres Verständnis konsultieren kann. Die Anwenderinnen können helfen die Daten zu verstehen, die Güte der Ergebnisse einzuordnen und um die Rolle eines Projekts in einem Geschäftsprozess zu verstehen. Auch wenn man die Anwenderinnen immer konsultieren sollte, ist dies spätestens dann zwingend nötig, wenn die Projektergebnisse operationalisiert werden sollen. In akademischen Projekten sind alle, die die Forschungsergebnisse direkt oder indirekt in ihrer täglichen Arbeit nutzen die Anwender. Wenn Forschung sich bereits im Stadium des Industrietransfer befindet, sind die Anwenderinnen die gleichen wie bei industriellen Projekten. In der Grundlagenforschung sind die Anwenderinnen in der Regel andere Forscherinnen.</p>
<p>Auch wenn die Anwenderinnen eine wichtige Rolle in Projekte sind, was sich auch insbesondere in modernen agilen Prozessmodellen wie Scrum widerspiegelt <a class="footnote-reference brackets" href="#scrum" id="id2">2</a>, sind sie häufig nicht Teil des Tagesgeschäfts von Projekten, sondern werden eher zu festen Zeitpunkten konsultiert.</p>
</div>
<div class="section" id="projektsponsorin">
<h3><span class="section-number">2.3.2. </span>Projektsponsorin<a class="headerlink" href="#projektsponsorin" title="Permalink to this headline">¶</a></h3>
<p>Ohne die Sponsorin, gäbe es kein Projekt. Die Projektsponsoreninnen entscheiden welche Projekte gestartet werden und stellen die benötigten Ressourcen bereit. Entsprechend könnten die Projektsponsorinnen die Mitgliederinnen des Managements einer Firma sein oder aber auch Kundeninnen die einen Auftrag erteilen. Die Projektsponsoreninnen sind auch für die Beurteilung ob ein Projekt erfolgreich ist wesentlich. Sie entscheiden ob Sie mit den Ergebnissen zufrieden sind und ob weitere Ressourcen für Folgeprojekte bereit gestellt werden, zum Beispiel um ein Ergebnis zu operationalisieren. In der akademischen Welt sind die Projektsponsorinnen in der Regel die <em>Principal Investigator</em>, also Professorinnen und Postdoktorandeninnen.</p>
<p>Ähnlich wie die Anwenderinnen, sind die Projektsponsorinnen in der Regel nicht teil des Tagesgeschäfts. Stattdessen sind sie nur an Meilensteinen involviert, wenn wesentliche Projektergebnisse vorgestellt werden und wenn wichtige Entscheidungen getroffen werden müssen.</p>
</div>
<div class="section" id="projektmanagerinnen">
<h3><span class="section-number">2.3.3. </span>Projektmanagerinnen<a class="headerlink" href="#projektmanagerinnen" title="Permalink to this headline">¶</a></h3>
<p>Die Projektmanagerin organisiert die Projektarbeit und das Tagesgeschäft. Die Aufgaben umfassen unter anderem die Ressourcenplanung (Finanziell, Mitarbeiterinnen, Rechenzeit), sowie die Überwachung des Projektfortschritts. Die Projektmanagerin ist dafür verantwortlich, dass Meilensteine und Projektziele zu den geplanten Zeitpunkten erreicht und die Qualitätsziele dabei erfüllt werden. Damit dies gewährleistet ist, bewertet die Projektmanagerin regelmäßig die Risiken und leitet sofern notwendig Maßnahmen zur Minimierung des Risikos ein. Im Extremfall kann das bedeuten, dass die Projektmanagerin auch entscheiden muss, das ein Projekt gescheitert ist und nicht fortgeführt wird. Alternativ könnten aber auch zusätzliche Ressourcen beantragt oder die Ziele angepasst werden. Im akademischen Umfeld ist die Projektmanagerin in der Regel auch die Projektsponsorin, oder eine wissenschaftliche Mitarbeiterin der Projektsponsorin.</p>
</div>
<div class="section" id="dateningenieurinnen">
<h3><span class="section-number">2.3.4. </span>Dateningenieurinnen<a class="headerlink" href="#dateningenieurinnen" title="Permalink to this headline">¶</a></h3>
<p>Eine Dateningenieurin beschäftigt sich mit dem ELT/ETL der Daten und ist verantwortlich dafür, die Daten in einer skalierbaren Analyseumgebung zur Verfügung zu stellen. Die Dateningenieurinnen brauchen daher ein gutes Verständnis der Technologien zum sammeln, speichern, und verarbeiten von Daten. Insbesondere bei Big Data Projekten oder wenn größere Mengen an Daten noch gesammelt werden müssen, kann dies sehr herausfordernd sein. In akademischen Projekten gibt es in der Regel keine Trennung zwischen der Dateningenieurin und dem Data Scientists. In einigen Disziplinen ist das Bereitstellen der Daten jedoch so aufwending, dass es auch hier Spezialisteninnen gibt, zum Beispiel in Genetik und der Hochenergiephysik. Das Human Genome Project <a class="footnote-reference brackets" href="#genome" id="id3">3</a> und das CERN <a class="footnote-reference brackets" href="#cern" id="id4">4</a> sind daher nicht nur für die Biologie und Physik hochrelevant, sondern auch Treiber von Technologien zum Umgang mit großen Datenmengen.</p>
</div>
<div class="section" id="datenbankadministratorin">
<h3><span class="section-number">2.3.5. </span>Datenbankadministratorin<a class="headerlink" href="#datenbankadministratorin" title="Permalink to this headline">¶</a></h3>
<p>Die Datenbankadministratorin unterstützt die Dateningenieurinnen und Data Scientists durch die Bereitstellung und Administration von Datenbanken als Teil der Analyseumgebung für das Projekt. Die Aufgaben umfassen die Installation und Konfiguration von (verteilten) Datenbanken und Rechenclustern, inklusive der benötigten Werkzeuge für die Datenanalyse. Ob diese Rolle mit der Dateningenieurin oder dem Data Scientist zusammenfällt hängt Organisation ab: Wenn es zum Beispiel ein extra Rechenzentrum gibt oder es eine Organisationseinheit die für die Nutzung und Bereitstellung von Cloudressourcen gibt, ist dies in der Regel separat. Anderfalls fallen diese Aufgaben häufig direkt mit andern Tätitkeiten im Projekt zusammen.</p>
</div>
<div class="section" id="data-scientist">
<h3><span class="section-number">2.3.6. </span>Data Scientist<a class="headerlink" href="#data-scientist" title="Permalink to this headline">¶</a></h3>
<p>Der Data Scientists ist die Expertin für die Datenanalyse und Modellierung und hierfür verantwortlich. Daher muss man als Data Scientist ein detailiertes Verständnis von Modellierungs- und Analysetechniken mitbringen, damit für ein Projekt geeignete Methoden ausgewählt werden können. Als Data Scientists trägt man wesentliche Verantwortung für den Projekterfolg und das erreichen der Projektziele. Wenn klar wird, das dies nicht möglich ist oder Risiken identifiziert werden, muss man als Data Scientist eng mit dem Projektmanagement zusammenzuarbeiten um geeignete Schritte einzuleiten. Die Rolle des Data Scientists ist eng verwand mit der Rolle von Business Intelligence Analysts, die es bereits in der Industrie gibt. Bei Business Intelligence handelt es sich gewissermaßen um den Vorreiter von Data Science in der Industrie, wobei hier der Fokus auf der häufig eher auf der Analyse von dem Status Quo lag, wobei Data Science Projekte häufig auch in die Zukunft blicken um Vorhersagen und automatisierte Entscheidungen zu ermöglichen.</p>
</div>
</div>
<div class="section" id="deliverables">
<h2><span class="section-number">2.4. </span>Deliverables<a class="headerlink" href="#deliverables" title="Permalink to this headline">¶</a></h2>
<p>Als Deliverable bezeichnet man greifbare und virtuelle Projektergebnisse. Welche Deliverables erstellt werden wird häufig als Teil des Projektplans und von Verträgen definiert und ist eng Verbunden mit den Meilensteinen von Projekten. Die Deliverables müssen die Erwartung der Stakeholder erfüllen und sind ein wesentliches Kriterium für den Projekterfolg. Bei Data Science Projekte gibt es üblicherweise verschiedene Deliverables, von denen wir die wichtigsten hier nennen wollen.</p>
<div class="section" id="sponsorenprasentation">
<h3><span class="section-number">2.4.1. </span>Sponsorenpräsentation<a class="headerlink" href="#sponsorenprasentation" title="Permalink to this headline">¶</a></h3>
<p>Die Sponsorenpräsentation adressiert das große Ganze und hat in der Regel eine nicht-technischne Zielgruppe, wie Anwenderinnen, Projektsponsorinnen, und das Projektmanagent. Der Fokus der Sponsorenpräsentation sollte daher auf klaren Botschaften bezüglich des Anwendungsfalls liegen. Man sollte die Güte eines Modells zum Beispiel nicht durch eines der im maschinellen Lernen verwendete Gütemaße ausdrücken, sondern eher in für den Geschäftsprozess relevanten Kriterien. Man sollte zum Beispiel nicht von “wahr positiven”, sondern von der Prozentzahl von korrekt identifizierten Kundeninnen. Die Präsentation sollte darauf ausgelegt sein, die Entscheidungsfindung zu stützen. Was genau das bedeutet, hängt vom Kontext und den Adressatinnen ab. Für Anwenderinnen könnte man die Präsentation zum Beispiel auf die zu erwartenden Änderungen um Geschäftsprozess zuschneiden.</p>
<p>Einfache Visualisierungen wie Balkendiagramme, Liniendiagramme und eventuell einfache Histogramme können hierbei unterstützend eingesetzt werden. Bei Histogrammen sollten die Bins so gewählt werden, dass sie intuitiv sinn ergeben. Detailierte Visualisierungen über Modelldetails sollten vermieden werden.</p>
</div>
<div class="section" id="analystenprasentation">
<h3><span class="section-number">2.4.2. </span>Analystenpräsentation<a class="headerlink" href="#analystenprasentation" title="Permalink to this headline">¶</a></h3>
<p>Die Analystenpräsentation ist an eine technisch versierte Zielgruppe gerichtet, zum Beispiel andere Data Scientists. Diese Präsentation sollte ebenfalls die große Ganz und die Kernbotschaften abdecken. Es sollte jedoch auch betrachtet werden, wir die Datenanalyse durchgeführt wurde, zum Beispiel welche Algorithmen verwendet wurden. Je nach Zielgruppe und der zur Verfügung stehenden Zeit kann man beliebig tief in die Modelldetails abtauchen, oder muss diese eher oberflächlich behandeln. Im Allgemeinen sollte man in diese Präsentation nichts “offensichtliches” wiederholen: Wenn Standardalgorithmen verwendet wurden, sollte man diese zum Beispiel nicht ausführlich beschreiben. Stattdessen sollte der Fokus eher auf Aspekten wie unerwarteten Problemen die aufgetaucht sind, kreativen Lösungsansätzen und neu entwickelten Verfahren liegen.</p>
<p>Insgesamt kann die Anlaystenpräsentation also deutlich komplexer sein, also die Sponsorepräsentation. Dennoch sollte man immer im Blick behalten, dass die Zielgruppe auch in der zur Verfügung stehenden Zeit die präsentierten Inhalte “verdauen” kann. Man kann also durchaus komplexere Visualisierungen verwenden, wenn diese jedoch mit Details überladen werden, ist dies kontraproduktiv.</p>
</div>
<div class="section" id="quelltext">
<h3><span class="section-number">2.4.3. </span>Quelltext<a class="headerlink" href="#quelltext" title="Permalink to this headline">¶</a></h3>
<p>Der zur Datenanalyse entwickelte Quelltext ist in der Regel auch ein Deliverable. Dies Quelltext ist oft nur ein Prototyp und kein auf Wiederverwendbarkeit auslegtes Werkzeug oder eine Programmbibliothek. Meistens liegt nur ein haufen von Quelltextdateien und ausführbaren Skripten vor. Im schlimmsten Fall beinhalten diese sogar noch maschinenspezifische Informationen, wie lokale Pfade zu den Daten. Auch wenn Clean Code ein sekundärer Aspekt ist, sollte dennoch auch hier bereits auf die Wiederverwendbarkeit geachtet werden. Der Quelltext st nämlich gleichzeitig auch eine exakte Spezifikation der erstellten Modelle. Daher ist der Quelltext, egal wie “Hacky” er ist, eine wichtige Ressource, auch wenn er möglicherweise neu geschrieben, aufgeräumt, oder adaptiert werden muss.</p>
</div>
<div class="section" id="technische-spezifikation">
<h3><span class="section-number">2.4.4. </span>Technische Spezifikation<a class="headerlink" href="#technische-spezifikation" title="Permalink to this headline">¶</a></h3>
<p>Zur Ausführung des Quelltextes muss man die technische Spezifikation kennen. Hier werden die Befehle und die Umgebung zur Ausführung beschrieben, zum Beispiel welche Betriebssysteme unterstützt werden, welche Softwarepakete benötigt werden, welche Abhängigkeiten verfügbar sein müssen und wie der Quelltext übersetzt und ausgeführt werden kann. Häufig wird dies vernachlässigt, so dass die technische Spezifikation fehlt, nicht aktuell oder unvollständig ist. In der Konsequenz ist der Quelltext nicht oder nur mit hohem Aufwand zu nutzbar. Gute Data Scientists sorgen dafür, dass dies nicht der Fall ist, zum Beispiel durch Replication Kits in der Forschung und sauberer Paketierung des Quelltexts in der Industrie.</p>
</div>
<div class="section" id="daten">
<h3><span class="section-number">2.4.5. </span>Daten<a class="headerlink" href="#daten" title="Permalink to this headline">¶</a></h3>
<p>Wenn im Rahmen eines Projekts Daten gesammelt werden, sind diese Daten selbst eventuell auch ein Deliverable. Durch das Teilen oder Archivieren der Daten kann man die Durchführung zukünftiger Projekte ermöglichen oder unterstützen. Insbesondere in der Forschung ist das Teilen von Daten ein üblich und wichtig um es anderen Forschern zu ermöglichen Ergebnisse nachzuvollziehen und darauf aufzubauen. Ob und wie die Daten geteilt werden können hängt von der Sensitivität und dem Volumen der Daten ab. Das teilen großer Datensätze kann kostenintensiv sein, was hier eventuell abschreckt. Wenn die Daten persönliche Informationen beinhalten, so dass die Data Privacy eine Rolle spielt, kann es notwendig sein die Daten zu anonymisieren oder auch komplett verboten sein die Daten zu teilen. Ähnlich kann es auch in einem industriellen Kontext mit internen Daten eines Unternehmens sein. Wenn Daten geteilt werden, sollte dies den FAIR Prinzipien genügen:</p>
<ul class="simple">
<li><p>Findable: Die Daten müssen einfach zu finden sein, zum Beispiel durch eine Digital Object Identifier (DOI).</p></li>
<li><p>Accessible: Der Zugriff auf die Daten muss einfach sein.</p></li>
<li><p>Interoperable: Alle notwendigen Informationen zur Nutzung der Daten sollten bekannt sein.</p></li>
<li><p>Reusable: Die Daten sollte eine Lizenz haben, die die Wiederverwendung erlaubt und entsprechend den in einer Community geltenden Standards geteilt werden.</p></li>
</ul>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="manmonth"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.1145/390016.808439">https://doi.org/10.1145/390016.808439</a></p>
</dd>
<dt class="label" id="scrum"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p><a class="reference external" href="https://www.scrumalliance.org/">https://www.scrumalliance.org/</a></p>
</dd>
<dt class="label" id="genome"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p><a class="reference external" href="https://www.genome.gov/human-genome-project">https://www.genome.gov/human-genome-project</a></p>
</dd>
<dt class="label" id="cern"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p><a class="reference external" href="https://home.cern/">https://home.cern/</a></p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="kapitel_01.html" title="previous page"><span class="section-number">1. </span>Big Data und Data Science</a>
    <a class='right-next' id="next-link" href="kapitel_03.html" title="next page"><span class="section-number">3. </span>Erkunden der Daten</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Steffen Herbold<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>