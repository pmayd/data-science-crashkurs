
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7. Klassifikation &#8212; Einführung in Data Science</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Übung 1" href="../exercises/uebung_01.html" />
    <link rel="prev" title="6. Clusteranalyse" href="kapitel_06.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Einführung in Data Science</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../vorwort.html">
   Vorwort
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Kapitel
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_01.html">
   1. Big Data und Data Science
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_02.html">
   2. Der Prozess von Data Science Projekten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_03.html">
   3. Erkunden der Daten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_04.html">
   4. Allgemeines zur Datenanalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_05.html">
   5. Assoziationsregeln
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_06.html">
   6. Clusteranalyse
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   7. Klassifikation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Übungen
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercises/uebung_01.html">
   Übung 1
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/chapters/kapitel_07.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/sherbold/einfuehrung-in-data-science"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sherbold/einfuehrung-in-data-science/main?urlpath=tree/content/chapters/kapitel_07.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uberblick">
   7.1. Überblick
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#binare-klassifikation-und-grenzwerte">
   7.2. Binäre Klassifikation und Grenzwerte
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gutemasze">
   7.3. Gütemaße
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#die-confusion-matrix">
     7.3.1. Die Confusion Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#die-binare-confusion-matrix">
     7.3.2. Die Binäre Confusion Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binare-gutemasze">
     7.3.3. Binäre Gütemaße
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#die-receiver-operator-characteristic-roc">
     7.3.4. Die Receiver Operator Characteristic (ROC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#area-under-the-curve-auc">
     7.3.5. Area Under the Curve (AUC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#micro-und-macro-averages">
     7.3.6. Micro und Macro Averages
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jenseits-der-confusion-matrix">
     7.3.7. Jenseits der Confusion Matrix
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decision-surfaces">
   7.4. Decision Surfaces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-nearest-neighbor">
   7.5.
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -Nearest Neighbor
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#entscheidungsbaume">
   7.6. Entscheidungsbäume
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-forests">
   7.7. Random Forests
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistische-regression">
   7.8. Logistische Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-bayes">
   7.9. Naive Bayes
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="klassifikation">
<h1><span class="section-number">7. </span>Klassifikation<a class="headerlink" href="#klassifikation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="uberblick">
<h2><span class="section-number">7.1. </span>Überblick<a class="headerlink" href="#uberblick" title="Permalink to this headline">¶</a></h2>
<p>Bei der Klassifikation geht es darum Kategorien zu Objekten zuzuweisen. Betrachten wir das Beispiel in <a class="reference internal" href="#fig-class-example"><span class="std std-numref">Fig. 7.1</span></a>. Das obere Bild zeigt einen Wal vor einem Eisberg. Das unsere Bild zeigt einen Bär in einem Wald. Beides erkennt man als menschlicher Betrachter instinktiv ohne darüber nachzudenken. Das Ziel der Klassifikation ist es diese Kategorisierung von Objekten automatisch durch eine Algorithmus durchführen zu lassen. Eine wichtige Einschränkung gegebenüber unserer menschlichen Kategoriesierung ist, das wir nur mit festen und im vorfeld festgelegten Kategorien arbeiten können Im Beispiel in <a class="reference internal" href="#fig-class-example"><span class="std std-numref">Fig. 7.1</span></a> könnten diese Kategorien zum Beispiel “Wal”, “Bär” und “Sonstiges” sein. Die Klassifikationsaufgabe wäre dann die Zuweisung einer dieser drei Kategorien zu den Bilden. Die Kategorien, in die die Objekte eingeteilt werden, nennt man auch <em>Klassen</em>.</p>
<div class="figure align-default" id="fig-class-example">
<a class="reference internal image-reference" href="../_images/classification_example_german.png"><img alt="../_images/classification_example_german.png" src="../_images/classification_example_german.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 7.1 </span><span class="caption-text">Zuweisung der Kategorien Wal und Bär zu Bildern.</span><a class="headerlink" href="#fig-class-example" title="Permalink to this image">¶</a></p>
</div>
<p>Etwas abstrakter können wir uns die Klassifikation wie in Abbildung <a class="reference internal" href="#fig-class-abstract"><span class="std std-numref">Fig. 7.2</span></a> vorstellen. Wir haben also Objekte für die wir ein <em>Konzept</em> kennen. Wenden wir unser Konzept auf die Objekte an, bekammen wir die Einteilung in Klassen. Wir kennen also ein Konzept das Wale beschreibt, welches wir auf das Bild wenden können um die Klasse zu bestimmen.</p>
<div class="figure align-default" id="fig-class-abstract">
<a class="reference internal image-reference" href="../_images/classification_concept_german.png"><img alt="../_images/classification_concept_german.png" src="../_images/classification_concept_german.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 7.2 </span><span class="caption-text">Abstraktes Konzept der Klassifikation</span><a class="headerlink" href="#fig-class-abstract" title="Permalink to this image">¶</a></p>
</div>
<p>Das Ziel von Klassifikationsalgorithmen ist es eine <em>Hypothese</em> aus den Daten abzuleiten, mit der man die Klasse von Objekten anhand der Merkmale bestimmen kann. Als Beispiel betrachten wir das Bild des Wals durch die Merkmale (<a class="reference internal" href="#fig-hypothesis"><span class="std std-numref">Fig. 7.3</span></a>). Über diese Merkmale könnte man auf folgende Hypothese kommen: <em>Objekte mit Flossen, die eine ovale Form haben, welche oben schwarz und unten weiß sind und die sich vor einem blauen Hintergrund befinden, sind Wale</em>. Diese Hypothese mag zwar nicht in jedem Fall richtig sein, sie ist aber eine relativ gute Beschreibung, mit der man viele Wale (bzw. Orcas) richtig erkennt. Fehler würde man zum Beispiel machen, wenn es ein U-Boot mit einer ähnlichen Farbwal gäbe. Die Art der Hypothese, zum Beispiel ob es sich um einen logischen Ausdruck oder eine beliebige mathematische Funktion handelt, hängt von der Wal des Algorithmus ab. Die Hypothese selbst wird vom Lernalgorithmus automatisch aus den Daten bestimmt.</p>
<div class="figure align-default" id="fig-hypothesis">
<a class="reference internal image-reference" href="../_images/hypothesis_german.png"><img alt="../_images/hypothesis_german.png" src="../_images/hypothesis_german.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 7.3 </span><span class="caption-text">Abstraktes Konzept der Klassifikation</span><a class="headerlink" href="#fig-hypothesis" title="Permalink to this image">¶</a></p>
</div>
<p>Formal haben wir eine Menge von Objekten <span class="math notranslate nohighlight">\(O = \{object_1, object_2, ...\}\)</span> welche möglicherweise unendliche viele Elemente enthält. Außerdem haben wir eine Repräsentation der Objekte als Instanzen im Merkmalsraum <span class="math notranslate nohighlight">\(\mathcal{F} = \{\phi(o): o \in O\}\)</span> und eine endliche Anzahl von Klassen <span class="math notranslate nohighlight">\(C = \{class_1, ..., class_n\}\)</span>. Die Klassifikation wird durch ein <em>Zielkonzept</em> (engl. <em>target concept</em>) beschrieben, welches die Objekte auf ihre Klassen abbildet, also</p>
<div class="math notranslate nohighlight">
\[h^*: O \to C.\]</div>
<p>Das Zielkonzept ist die wahre Klasse der Objekte, also eine perfekte Zuweisung von Objekten zu Klassen. Im normalfall ist keine mathematische Beschreibung des Zielkonzepts bekannt. Es gibt zum Beispiel keine mathematische Beschreibung zur Klassifikation von Bildern in Walbilder und Bärenbilder. Die <em>Hypothese</em> bildet die Merkmale auf die Klassen ab, also</p>
<div class="math notranslate nohighlight">
\[h: \mathcal{F} \to C.\]</div>
<p>Die Hypothese wird vom Klassifikationsalgorithmus so bestimmt, dass sie eine gute Approximation des Zielkonzept ist, also</p>
<div class="math notranslate nohighlight">
\[h^*(o) \approx h(\phi(o)).\]</div>
<p>Eine Variante der Klassifikation ist die Berechnung von <em>Scores</em> für jede Klasse <span class="math notranslate nohighlight">\(c \in C\)</span>. In diesem Fall haben wir Scoring-Funktionen</p>
<div class="math notranslate nohighlight">
\[h_c': \mathcal{F} \to \mathbb{R}\]</div>
<p>für jede Klasse. Die Scores sind ähnlich zum Soft Clustering: Anstatt das wir alle Instanzen genau einer Klasse zuweisen, bestimmen wir eine Wert für jede Klasse, den wir dann für die Entscheidungsfindung nutzen können. Im Normalfall wird dann die Klasse zugewiesen, die den höchsten Score hat. Wir haben also</p>
<div class="math notranslate nohighlight">
\[h(x) = \arg\max_{c \in C} h_c'(x)\]</div>
<p>für <span class="math notranslate nohighlight">\(x \in \mathcal{F}\)</span>. Oft handelt es sich bei den Scores um Wahrscheinlichkeitsverteilungen, so dass der Score für jeder Klasse im Intervall <span class="math notranslate nohighlight">\([0,1]\)</span> liegt und die Summe der Scores aller Klasse 1 ergibt. In diesem Fall geben die Scores die Wahrscheinlichkeit an, dass ein Objekt zu einer bestimmten Klasse gehört.</p>
</div>
<div class="section" id="binare-klassifikation-und-grenzwerte">
<h2><span class="section-number">7.2. </span>Binäre Klassifikation und Grenzwerte<a class="headerlink" href="#binare-klassifikation-und-grenzwerte" title="Permalink to this headline">¶</a></h2>
<p>Ein häufig betrachter Spezialfall der Klassifikation ist die binäre Klassifikation bei der es genau zwei Klassen gibt. Auch wenn es sich hierbei um eine starke Einschränkung handelt, gibt es viele Probleme die man mit Hilfe von binärer Klassifikation lösen kann. Beispiele hierfür sind die Vorhersage ob ein Schuldner einen Kredit abbezahlen kann, ob es sich bei einer Transaktion um Kreditkartenbetrug handelt, oder ob eine Email Spam ist.</p>
<p>Bei der binären Klassifikation nennt man eine Klasse <em>Positiv</em> und die andere <em>Negativ</em>. Da es nur die zwei Klassen <span class="math notranslate nohighlight">\(C = \{positive, negative\}\)</span> gibt, kann man das berechnen der Scores unter der Annahme das es sich bei den Scores um Wahrscheinlichkeiten handelt vereinfachen. Es gilt dann nämlich</p>
<div class="math notranslate nohighlight">
\[h_{negative}'(x) = 1-h_{positive}'(x)\]</div>
<p>da die Summer der Wahrscheinlichkeiten 1 ergibt. Entsprechend reicht es auch nur die Scoring-Funktion für die positive Klasse zu berechnen und wir nutzen die Notation <span class="math notranslate nohighlight">\(h'(x) = h_{positive}'\)</span> für die binäre Klassifikation. Jetzt können wir auch statt einfach die Klasse mit der höchsten Wahrscheinlichkeit auszuwählen einen <em>Grenzwert</em> <span class="math notranslate nohighlight">\(t \in [0,1]\)</span> festlegen, der für die positive Klasse erreicht sein muss. Wenn <span class="math notranslate nohighlight">\(h'(x) \geq t\)</span>, it <span class="math notranslate nohighlight">\(x\)</span> positiv, wenn der Score kleiner als der Grenzwert ist, ist <span class="math notranslate nohighlight">\(x\)</span> negativ. Es gilt also</p>
<div class="math notranslate nohighlight">
\[\begin{split}h_t(x) = \begin{cases}\text{positive} &amp; \text{wenn}~h'(x) \geq t \\ \text{negative} &amp; \text{wenn}~h' &lt; t\end{cases}\end{split}\]</div>
<p>Warum Grenzwerte und Scoring-Funktionen relevant für die Klassifikation sind kann man sich gut an einem Beispiel verdeutlichen. Das Histogram unten zeigt fiktive Daten von Scores einer Spamerkennungs Klassifikation bei der positive Instanzen kein Spam sind.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>


<span class="c1"># generate sample data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                  <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># split the data into 50% training data and 50% test data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># predict scores with a random forest</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_score</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">([</span><span class="n">y_score</span><span class="p">[</span><span class="n">y_test</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_score</span><span class="p">[</span><span class="n">y_test</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;No Spam&#39;</span><span class="p">,</span> <span class="s1">&#39;Spam&#39;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Histogram of predicted probabilities&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted probability&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper center&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_1_0.png" src="../_images/kapitel_07_1_0.png" />
</div>
</div>
<p>Die organen Balken zeigen die Scores von Spam-Emails, die blauen Balken die Scores aller regulären Emails. Wenn man keinen Grenzwert auswählt, würde man einfach die Klasse mit dem höchsten Score vorhersagen. Dies ist Äquivalent zu einem Grenzwert von 0,5, dargestellt durch die graue vertikale Linie. In diesem Beispiel würden wir also die meisten Emails richtig vorhersagen, aber auch eine Emails als Spam markieren, obwohl sie eigentlich kein Spam sind, und auch einige Emails nicht als Spam markieren, obwohl es sich um Spam handelt. Es handelt sich hier um unterschiedliche Arten von Fehlern, die für diesen Anwendungsfall nicht gleichwertig sind. Auch wenn Spam nervig ist, ist das löschen einer Email nicht sehr aufwändig, es sei denn man hat es mit hunderten oder gar tausenden von Spam-Emails zu tun. Wenn jedoch auch nur eine einzige wichtige Email ausversehen als Spam markiert und der Empfänger diese Email dadurch nicht angezeigt bekommt, kann dies große negative Konsequenzen haben. Dieses Problem können wir durch die Auswahl eines geeigneten Grenzwerts lösen. Die schwarze Linie markiert einen Grenzwert von 0,1. Mit diesem Grenzwert würde man nur Spam-Emails als solche Kennzeichnen. Als folge würde man zwar auch mehr Spam-Emails nicht erkennen, aber es würden keine regulären Emails durch den Spamfilter abgefangen. Dies zeigt das die Wahl eines geeigneten Grenzwerts den Unterschied zwischen einer guten Lösung für einen Anwendungsfall und einem ungeeignetem Modell machen kann.</p>
</div>
<div class="section" id="gutemasze">
<h2><span class="section-number">7.3. </span>Gütemaße<a class="headerlink" href="#gutemasze" title="Permalink to this headline">¶</a></h2>
<p>Eine Kernfrage der Klassifikation ist wie gut die Hypothese <span class="math notranslate nohighlight">\(h\)</span> das Zielkonzept <span class="math notranslate nohighlight">\(h^*\)</span> approximiert. In der Regel erreicht man keine perfekte Lösung, es gibt also Instanzen die falsch durch die Hypothese klassifiziert werden. Das obige Spamerkennungsbeispiel zeigt bereits, das es unterschiedliche Arte von Fehlern gibt, die wir mit den Gütemaßen berücksichtigen müssen.</p>
<p>Die Grundlage für die Bewertung der Güte von Klassifikationsalgorithmen sind Testdaten. Hierzu wird die Hypothese auf die Merkmale der Testdaten angewandt. Anschließend kann man die Vorhersageergebnisse mit der wahren Klasse vergleichen. Die folgende Tabelle zeigt dies am Beispiel der Wal- und Bärenbilder.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Form</p></th>
<th class="head"><p>Farbe oben</p></th>
<th class="head"><p>Farbe unten</p></th>
<th class="head"><p>Hintergrundfarbe</p></th>
<th class="head"><p>Flossen</p></th>
<th class="head"><p>Klasse</p></th>
<th class="head"><p>Vorhersage</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Oval</p></td>
<td><p>Schwarz</p></td>
<td><p>Weiß</p></td>
<td><p>Blau</p></td>
<td><p>Ja</p></td>
<td><p>Wal</p></td>
<td><p>Wal</p></td>
</tr>
<tr class="row-odd"><td><p>Rechteck</p></td>
<td><p>Braun</p></td>
<td><p>Braun</p></td>
<td><p>Grün</p></td>
<td><p>Nein</p></td>
<td><p>Bär</p></td>
<td><p>Wal</p></td>
</tr>
<tr class="row-even"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
</tbody>
</table>
<p>Due erste Instanz wird korrekt vorhersagt, bei der zweiten Instanz macht unsere fiktive Hypothese einen Fehler. Wenn es tausende, oder sogar Millionen von Instanzen in den Testdaten gibt, ist es nicht machbar die Ergebnisse anhand einer solchen Tabelle auszuwerten. Stattdessen brauchen wir eine kompaktere Darstellung der Güte der Ergebnisse um die Vorhersagen mit den wahren Klassen zu vergleichen.</p>
<div class="section" id="die-confusion-matrix">
<h3><span class="section-number">7.3.1. </span>Die Confusion Matrix<a class="headerlink" href="#die-confusion-matrix" title="Permalink to this headline">¶</a></h3>
<p>Das wichtigste Werkzeug für die Analyse der Qualität von Hypothesen ist die <em>Confusion Matrix</em>, eine tabellenartige Beschreibung der Häufigkeiten mit denen die Hypothese richtig liegt, bzw. Fehler macht, also verwirrt (engl. <em>confused</em>) ist. Die Confusion Matrix für unser Bildklassifikationsbeispiel könnte zum Beispiel so aussehen.</p>
<table>
    <tr><td></td><td colspan=4><b>Wahre Klasse</b></td></tr>
    <tr><td rowspan=4><br><br><b>Vorhersage</b></td><td><td><b>Wal</b></td><td><b>Bär</b></td><td><b>Sonstiges</b></td></tr>
    <tr><td><b>Wal</b></td><td>29</td><td>1</td><td>3</td></tr>
    <tr><td><b>Bär</b></td><td>2</td><td>22</td><td>13</td></tr>
    <tr><td><b>Sonstiges</b></td><td>4</td><td>11</td><td>51</td></tr>
</table>
<p>Im wesentlichen zählt die Confusion Matrix wir oft Instanzen einer bestimmte Klasse als welche Klasse vorhergesagt werden. Man sieht zum Beispiel wie oft Wale als Wale vorhergesagt werden und wie oft sie stattdessen als Bären oder Sonstiges klassifiziert werden. Die Spalten zeigen die wahren Werte der Instanzen, also das Zielkonzept. Die Zeilen zeigen die Vorhersagen, also die Hypothese. In unserem Beispiel haben wir 35 Bilder von Walen. Dies ist die Summe der Werte in der ersten Spalte. 29 diese Walbilder werden richtig klassifiziert, 2 werden fehlerhaft als Bären klassifiziert und 4 werden fehlerhaft als Sonstiges klassifiziert. Wir bekommen mit Hilfe der Confusion Matrix also ein genaues statistische Informationen darüber, wie die Instanzen einer Klasse klassifiziert werden. Die Werte auf der Diagonalen sind die richtigen Vorhersagen, die anderen Werte sind die Fehler.</p>
</div>
<div class="section" id="die-binare-confusion-matrix">
<h3><span class="section-number">7.3.2. </span>Die Binäre Confusion Matrix<a class="headerlink" href="#die-binare-confusion-matrix" title="Permalink to this headline">¶</a></h3>
<p>Die binäre Confusion Matrix ist der Spezialfall der Confusion Matrix für binäre Klassifikationsprobleme. Im Allgemeinen sieht die binäre Confusion Matrix wie folgt aus:</p>
<table>
    <tr><td></td><td colspan=3><b>Wahre Klasse</b></td></tr>
    <tr><td rowspan=3><br><br><b>Vorhersage</b></td><td><td><b>Positiv / True</b></td><td><b>Negativ / False</b></td></tr>
    <tr><td><b>Positiv / True</b></td><td>Richtig Positiv / True Positive (TP)</td><td> Falsch Positiv / False Positive (FP)</td></tr>
    <tr><td><b>Negativ / False</b></td><td>Falsch Negativ / False Negative (FN)</td><td> Richtig Negativ / True Negative (TN)</td></tr>
</table>
<p>Wir bekommen also abhängig davon ob der wahren Klasse und der Vorhersage die Anzahl der <em>richtig positiven</em> (engl. <em>true positive</em> / TP), <em>richtig negativen</em> (engl. <em>true negative</em> / TN), <em>falsch positiven</em> (engl. <em>false positive</em> / FP) und <em>falsch negativen</em> (<em>false negatives</em> / FN) Ergebnisse. Die binäre Confusion Matrix ist auch jenseits des maschinellen Lernens verbreitet, zum Beispiel in der Medizin zur Bewertung der Qualität von Tests. Aus der Medizin kommen auf die Begriffe des <em>Fehlers 1. Art</em> (engl. <em>type I error</em>) und <em>Fehlers 2. Art</em> (engl. <em>Type II Fehler</em>). Der Fehler 1. Art misst die Anzahl der falsch positiven. Das könnte zum Beispiel bedeuten, dass das Ergebnis eine Antigentests auf eine bestimmte Krankheit fälschlicherweise ein positives Ergebnis liefert, obwohl der Patient nicht erkrankt ist. Der Fehler 2. Art misst die Anzahl der falsch negativen. Dies würde zum Beispiel bedeuten, dass eine Krankheit von einem Antigentest übersehen wird, ob ein Patient erkrankt ist.</p>
</div>
<div class="section" id="binare-gutemasze">
<h3><span class="section-number">7.3.3. </span>Binäre Gütemaße<a class="headerlink" href="#binare-gutemasze" title="Permalink to this headline">¶</a></h3>
<p>Mit Hilfe der binären Confusion Matrix können wir Gütemaße defininerne, welche die Güte einer Hypothese durch eine einzelne Zahl basierend auf einem statistischen Kriterium zusammenfassen. Es gibt viele derartige Gütemaße, die alle unterschiedliche Aspekte der Güte messen. Die folgende Tabelle listet elf derartige Gütemaße.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>True positive rate, recall, sensitivity</p></td>
<td><p>Prozentsatz der positiven Instanzen, die korrekt klassifiziert werden.</p></td>
<td><p><span class="math notranslate nohighlight">\(TPR = \frac{TP}{TP+FN}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>True negative rate, specificity</p></td>
<td><p>Prozentsatz der negativen Instanzen, die korrekt klassifiziert werden.</p></td>
<td><p><span class="math notranslate nohighlight">\(TNR = \frac{TN}{TN+FP}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>False negative rate</p></td>
<td><p>Prozentsatz der positiven Instanzen, die fehlerhaft als negativ klassifiziert werden.</p></td>
<td><p><span class="math notranslate nohighlight">\(FNR = \frac{FN}{FN+TP}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>False positive rate</p></td>
<td><p>Prozentsatz der negativen Instanzen, die fehlerhaft als positiv klassifiziert werden.</p></td>
<td><p><span class="math notranslate nohighlight">\(FPR = \frac{FP}{FP+TN}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Positive predictive value, precision</p></td>
<td><p>Prozentsatz der positiven Vorhersagen, die korrekt sind.</p></td>
<td><p><span class="math notranslate nohighlight">\(PPV = \frac{TP}{TP+FP}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Negative predictive value</p></td>
<td><p>Prozentsatz der negativen Vorhersagen, die korrekt sind.</p></td>
<td><p><span class="math notranslate nohighlight">\(NPV = \frac{TN}{TN+FN}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>False discovery rate</p></td>
<td><p>Prozentsatz der positiven Vorhersagen, die fehlerhaft sind und eigentlich negativ sein sollten.</p></td>
<td><p><span class="math notranslate nohighlight">\(FDR = \frac{FP}{TP+FP}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>False omission rate</p></td>
<td><p>Prozentsatz der negativen Vorhersagen, die fehlerhaft sind und eigentlich positiv sein sollten.</p></td>
<td><p><span class="math notranslate nohighlight">\(FOR = \frac{FN}{FN+TN}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Accuracy</p></td>
<td><p>Prozentsatz der korrekten Vorhersagen.</p></td>
<td><p><span class="math notranslate nohighlight">\(accuracy = \frac{TP+TN}{TP+TN+FP+FN}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>F1-Score</p></td>
<td><p>Harmonisches Mittel von <span class="math notranslate nohighlight">\(recall\)</span> und <span class="math notranslate nohighlight">\(precision\)</span>.</p></td>
<td><p><span class="math notranslate nohighlight">\(F_1 = 2\cdot\frac{precision \cdot recall}{precision+recall}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Matthews correlation coefficient (MCC)</p></td>
<td><p>Korrelation zwischen den Vorhersagen und den wahren Klassen.</p></td>
<td><p><span class="math notranslate nohighlight">\(MCC = \frac{TP\cdot TN - FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\)</span></p></td>
</tr>
</tbody>
</table>
<p>Da es so viele Gütemaße gibt, stellt sich die Frage welche man benutzen sollte. Die Auswertung von allen elf Gütemaßen gleichzeitg ist nicht sinnvoll. Viele Gütemaße sind stark miteinander korreliert und sie werden alle basierend auf den selben vier Werten (TP, FP, TN, FN) berechnet. Stattdessen sollte man die Logik hinter der Definition der Güßemaße nachvollziehen um zu verstehen welche für eine bestimmten Anwendungsfall geeignet sind.</p>
<p>Die ersten vier Gütemaße berechnen den Anteil von richtigen, bzw. falschen Vorhersagen in Relation zu den wahren Werten. Hiermit kann man Fragen der Form “wie viele positive/negative Instanzen sind korrekt klassifiziert” beantworten. Die Kombination aus TPR und TNR ist sehr wichtig, da diese zwei Fragen beantworten, die für viele Anwendungsfälle von elementarer Bedeutung sind: Wie viele positive und wie viele negative Instanzen werden korrekt gefunden? Entsprechend sind Gütemaße gut geeignet um den Fehler 1. Art und den Fehler 2. Art zu schätzen. Die FPR und die FNR sind die Gegenstücke zur TNR und TPR und lassen sich auch direkt aus diesen berechnen als <span class="math notranslate nohighlight">\(FPR=1-TNR\)</span>, bzw. <span class="math notranslate nohighlight">\(FNR=1-TPR\)</span>.</p>
<p>Die nächsten vor Gütemaße berechnen den Anteil von richtigen, bzw. falschen Vorhersagen in Relation zu den Vorhersagen. Hiermit kann man Fragen der Form “wie viele ositive/negative Vorhersagen sind richtig klassifiziert” beantworten. Der unterschied zu den ersten vier Gütemaßen liegt in Bezugsgröße, die hier nicht die wahren Klassen sondern die Vorhersagen sind. Ansonsten sind die Gütemaße ähnlich zu den ersten vier Gütemaßen.</p>
<p>Eine gemeinsame Eigenschaft der ersten acht Gütemaße ist das sie niemals alleine benutzt werden. Mit andern Worten bedeutet dass, das man nie nur eines dieser Gütemaße als Optimierungskriterium wählen sollte. Der Grund ist, dass diese Metriken jeweils nur eine Spalte, bzw. Zeile der Confusion Matrix berücksichtigen. Wenn man zum Beispiel die TPR berechnet, werden hierfür nur die Werte aus der ersten Spalte der Confusion Matrix verwendet, die zweite Spalte wird ignoriert. Als konsequenz sind <em>triviale Hypothesen</em> ausreichend um ein optimales Ergebnis in nur eines des Gütemaße zu erreichen. Man nennt eine Hypothese trivial, wenn sie immer die selbe Klasse vorhersagt, für alle Instanzen. Ein Beispiel für eine triviale Hypothese ist <span class="math notranslate nohighlight">\(h^+(x) = true\)</span> für alle <span class="math notranslate nohighlight">\(x \in \mathcal{F}\)</span>. Mit dieser Hypothese hätte man einen perfekten Wert für die FPR von 1. Die Hypothese ist jedoch nicht hilfreiche, da sie nichts über die Daten selbst aussagt. Um das zu vermeiden, muss man mehrere Kriterien verwenden, so dass mindestens drei Werte aus der Confusion Matrix verwendet werden. Mit <span class="math notranslate nohighlight">\(h+\)</span> wären die TNR zum Beispiel 0, also so schlecht es geht. Mit der Kombination aus TPR und TNR verhindert man also, das man mit einer trivialen Hypothese ein optimales Ergebnis bekommt.</p>
<p>Es gibt auch Gütemaße die nicht nur einzelne Aspekte der Güte betrachten, sondern die Güte als ganzes und daher als einziges Kriterium verwendet werden können. Die letzten drei Gütemaße sind Beispiele hierfür. Diese Metriken nutzen die komplette Confusion Matrix um die Güte einer Hypothese zu berechnen. Die Accuracy müsst den Anteil der Vorhersagen, die richtig sind. Dies ist ähnlich zu den ersten vier Gütemaßen, mit dem Unterschied das beide Klassen gleichzeitig betrachtet werden. Der Nachteil der Accuracy ist, dass sie im Fall von <em>Class Level Imbalance</em> zu irreführend sein kann. Man spricht von Class Level Imbalance, wenn es deutlich mehr Instanzen aus einer Klasse, als aus der anderen gibt. Wenn zum Beispiel 95% der Instanzen positiv sind, erreicht würde unsere triviale Hypothese <span class="math notranslate nohighlight">\(h^+\)</span> bereits eine Accuracy von 95% erreichen. Dieser sehr gute Wert ist aber irreführend, da er nicht abbildet das alle negativen Instanzen falsch klassifiziert werden. Daher sollte man die Accuracy nur mit bedacht einsetzen und sicher stellen dass es etwa gleich viele Instanzen für alle Klassen gibt.</p>
<p>Der F1-Score ist das <em>harmonische Mittel</em> aus dem TPR/Recall und dem PPV/Precision. Der F1-Score berücksichtigt also den Anteil der positiven Instanzen, die korrekt Klassifiziert sind und den Anteil der positiven Vorhersagen, die korrekt sind. Das harmonische Mittel ist eine alternative zum arithmetischen Mittel, mit dem man Verhältnisse gut Mitteln kann. Beim harmonischen Mittel wird der niedrigere Wert höher gewichtet. Der F1-Score basiert auf der Idee das es einen Trade-Off zwischen Recall und Precision gibt. Um den Recall zu erhöhen, müssen wir mehr Instanzen als positiv klassifizieren. Hierdurch bekommen wir in der Regel auf mehr falsch positive Instanzen, da es Fehler gibt. Durch den höheren Anteil an falsch positiven Ergebnissen reduziert sich die Precision. Da durch das harmonische Mittel der kleinere Wert überproportional in der Berechnung berücksichtig wird, strebt eine Optimierung des F1-Scores ähnlliche Werte für Recall und Precision an.</p>
<p>Die letzte Metrik in der obigen Tabelle ist MCC, welcher die direkte Korrelation zwischen den wahren Werten und den Vorhersagen berechnet. Im wesentlichen misst MCC wie der Anteil der richtig positiven und richtig negativen Vorhersagen mit dem erwarteten Ergebnis korreliert ist. Der MCC ist robust gegen die Class Level Imbalance und liefert im Allgemeinen eine gute Schätzung der Güte. Der Nachteil des MCC ist, dass es keine einfache Interpretation gibt, die auch Laien zugänglich ist. Alle anderen Gütemaße kann man in wenigen Sätzen natürlicher Sprache erklären, für MCC gibt es keine ähnlich zugängliche Erklärung. Hierdurch ist es auch schwerer die Werte von MCC einzuordnen. Hinzu kommt, dass die Werte von MCC im Intervall nicht im Intervall <span class="math notranslate nohighlight">\([0,1]\)</span> liegen, sondern in <span class="math notranslate nohighlight">\([-1, 1]\)</span> da es sich im ein Korrelationsmaß handelt. Ein hoher negativer Wert bedeutet, dass die Hypothese das Gegenteil von erwarteten Ergebnissen vorhersagt. Je nach Kontext, können hohe negative Werte also auch gut sein, da man theorisch einfach alle Vorhersagen invertieren kann. Zusammenfassend können wir also sagen, dass MCC ein sehr robustes Gütemaß ist, die interpretation der Ergebnisse erfordert aber etwas Übung und Expertenwissen.</p>
</div>
<div class="section" id="die-receiver-operator-characteristic-roc">
<h3><span class="section-number">7.3.4. </span>Die Receiver Operator Characteristic (ROC)<a class="headerlink" href="#die-receiver-operator-characteristic-roc" title="Permalink to this headline">¶</a></h3>
<p>Die oben diskutierten Gütemaße basieren alle aus der Confusion Matrix. Ein Nachteil der Confusion Matrix ist, dass diese die Scores nicht berücksichtigt. Man kann die Confusion Matrix für eine Scoring-Funktion <span class="math notranslate nohighlight">\(h'\)</span> nur für einen festen Grenzwert <span class="math notranslate nohighlight">\(t\)</span> berechnen. Wie sich die Confusion Matrix für verschiedene Werte von <span class="math notranslate nohighlight">\(t\)</span> verändert kann man nicht ablesen. Hierzu kann man <em>Receiver Operator Characteristic</em> (ROC) Kurven einsetzen. Eine ROC Kurve representiert alle möglichen Werte für TPR und FPR, die für eine Scoring-Funktion <span class="math notranslate nohighlight">\(h'\)</span> mit beliebigen Grenzwerten <span class="math notranslate nohighlight">\(t \in [0,1]\)</span> erreichen kann. Die ROC Kurve von unserem Spamerkennungsbeispiel sieht wie folgt aus.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>

<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>

<span class="c1"># Plot ROC Curve</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span>
         <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ROC curve (AUC = </span><span class="si">%0.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">roc_auc</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate (FPR)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate (TPR)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Example of a ROC curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_3_0.png" src="../_images/kapitel_07_3_0.png" />
</div>
</div>
<p>Die ROC Kurve zeigt die FPR auf der x-Achse und die TPR auf der y-Achse an. Man sieht alle möglichen Verhältnisse von FPR und TPR die man mit verschiedenen Grenzwerten erreichen kann. Da eine Hypothese eine hohe TPR und eine niedrige FPR erreichen sollte, liegt die optimale Güte in der oberen linken Ecke der ROC Kurve, wo die FPR 0 und die TPR 1 ist. Dies wäre ein perfektes Ergebnis ohne Fehlklassifikationen. Die ROC Kurve ist ein gutes Werkzeug um eine gute Kombination aus TPR und FPR für einen Anwendungsfall auszuwählen. Wenn wir zum Beispiel eine TPR von mindestens 0,8 erreichen wollen, können wir sehen das wir hierfür eine FPR von 0,05 in kauf nehmen müssen. Dies ist in der folgenden Grafik mit einem Kreis markiert.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">tpr</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">&lt;</span><span class="mf">0.8</span><span class="p">:</span>
    <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Plot ROC Curve</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span>
         <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ROC curve (AUC = </span><span class="si">%0.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">roc_auc</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fpr</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate (FPR)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate (TPR)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Optimal threshold: t=</span><span class="si">%.2f</span><span class="s1"> (TPR=</span><span class="si">%.2f</span><span class="s1">, FPR=</span><span class="si">%.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">fpr</span><span class="p">[</span><span class="n">index</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_5_0.png" src="../_images/kapitel_07_5_0.png" />
</div>
</div>
</div>
<div class="section" id="area-under-the-curve-auc">
<h3><span class="section-number">7.3.5. </span>Area Under the Curve (AUC)<a class="headerlink" href="#area-under-the-curve-auc" title="Permalink to this headline">¶</a></h3>
<p>Wir können dir ROC Curve auch benutzen um ein Gütemaß zu definieren, in dem wir die <em>Fläche unter der Curve</em> (engl. <em>Area Under the Curve</em>, AUC) messen. Die idee ist einfach: Wenn der optimale Wert der ROC Curve in der oberen linken Ecke ist, ist die Fläche in diesem Fall 1. Je kleiner die Fläche unter der Kurve, desto niedriger die Werte von möglichen Kombinationen der TPR und FPR, was bedeutet das wir schlechtere Ergebnisse haben, unabhängig von einem konkreten Grenzwert <span class="math notranslate nohighlight">\(t\)</span>. Wenn wir also das Integral der ROC Kurve berechnen, können wir damit schätzen wie gut eine Hypothese ist. Daher auch der Name dieses Gütemaßes, als die Fläche unter der Kurve.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot ROC Curve with AUC</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span>
         <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;AUC = </span><span class="si">%0.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">roc_auc</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Receiver operating characteristic example&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_7_0.png" src="../_images/kapitel_07_7_0.png" />
</div>
</div>
<p>Je näher der Wer von AUC an 1 ist, desto besser ist die Güte der Hypothese. Was die Interpretation angeht, hat AUC aber ein ähnliches Problem wie MCC: Während ein Wert von 1 optimal ist, ist der schlechteste Werte nicht etwa die 0, sondern die 0,5. Bei der 0 hätte man eine FPR von 1 und eine TPR von 0. Wenn man jetzt das Ergebnis invertiert, hat man eine perfekte Klassifikation. Eine Fläche von 0,5 wiederum bekommt man durch die Diagonale auf der <span class="math notranslate nohighlight">\(TPR=FPR\)</span> gilt. Wenn TPR und FPR gleich groß sind, heißt das aber nichts anderes, als das man so gut ist wie der Zufall. Daher sind Werte von AUC besser, je weiter sie von 0,5 entfernt sind. Falls die Werte jedoch unter 0,5 liegen, sollte man rausfinden warum die Hypothese das Gegenteil der wahren Werte vorhersagt.</p>
</div>
<div class="section" id="micro-und-macro-averages">
<h3><span class="section-number">7.3.6. </span>Micro und Macro Averages<a class="headerlink" href="#micro-und-macro-averages" title="Permalink to this headline">¶</a></h3>
<p>Die bisherigen Gütemaße haben wir alle über die binäre Confusion Matrix definiert. Die Definition der Accuracy können wir ohne Probleme auf mehr als zwei Klassen verallgemeiners, da es sich um den Anteil der korrekt klassifizierten Instanzen handelt. Für MCC gibt es auch eine Erweiterung für mehr als zwei Klassen, auf die wir hier jedoch nicht näher eingehen. Für die anderen Gütemaße können wir einen Trick anwenden: Statt einer nicht-binären Confusion Matrix betrachten wir mehrere binäre Confusion Matrizen. Wie dieser Trick funktioniert, betrachten wir direkt an einem Beispiel. Hier ist noch einmal die Confusion Matrix für die drei Klassen Wal, Bär, und Sonstiges, die wir oben bereits betrachtet haben.</p>
<table>
    <tr><td></td><td colspan=4><b>Actual class</b></td></tr>
    <tr><td rowspan=4><br><br><b>Predicted class</b></td><td><td><b>whale</b></td><td><b>bear</b></td><td><b>other</b></td></tr>
    <tr><td><b>whale</b></td><td>29</td><td>1</td><td>3</td></tr>
    <tr><td><b>bear</b></td><td>2</td><td>22</td><td>13</td></tr>
    <tr><td><b>other</b></td><td>4</td><td>11</td><td>51</td></tr>
</table>
<p>Wir können aus diese Matrix nun Gütemaße für die einzelnen Klassen berechnen, zum Beispiel die TPR für die Klasse Wal als</p>
<div class="math notranslate nohighlight">
\[TPR_{whale} = \frac{TP_{whale}}{TP_{whale}+FN_{whale}}.\]</div>
<p>Wir betrachten also die Klasse Wal als die positive Klasse einer binären Confusion Matrix und die anderen Klassen fassen wir in einer einzigen negativen Klasse zusammen. Basierend auf diesem Konzept können wir jetzt eine Erweiterung unserer Gütemaße für eine beliebige Anzahl von Klassen definieren durch das <em>Macro Averaging</em> und das <em>Micro Averaging</em>. Ein Macro Average ist das arithmetische Mittel von eines Gütemaßes, wenn es individuell auf alle Klassen anwendet wird. Für die TPR ist das Macro Average defieniert als</p>
<div class="math notranslate nohighlight">
\[TPR_{macro} = \frac{1}{|C|}\sum_{c \in C}\frac{TP_{c}}{TP_{c}+FN_{c}}.\]</div>
<p>Mit dem Micro Averaging berechnen wir die Gütemaße direkt, in dem wir die Formeln anpassen um die Summe der Werte aller Klassen zu berechnen. Für die TPR ist das Micro Average definiert als</p>
<div class="math notranslate nohighlight">
\[TPR_{micro} = \frac{\sum_{c \in C} TP_C}{\sum_{c \in C} TP_C + \sum_{c \in C} FN_C}.\]</div>
<p>Ob es sinnvoller ist ein Macro oder ein Micro Average zu berechnen, hängt vom Anwendungsfall und den Daten ab. Wenn es ähnlich viele Instanzen für jede Klasse gibt, sind die Ergebnisse des Macro und Micro Averages nahezu identisch. Wenn es Class Level Imbalance in den Daten gibt, also wenn es für mindestens eine Klasse erheblich mehr oder weniger Instanzen in den Daten gibt, ist die Auswahl der Mittelungsmethode relevant. Beim Macro Average werden alle Klasse gleich gewichtet, unabhängig davon, wie viele Instanzen es in den Daten gibt. Dies liegt daran, dass die Gütemaße individuell für jede Klasse berechnet werden und anschließend ungewichtet gemittelt werden. Im Gegensatz hierzu ist der Einfluss der Klassen auf das Micro Average proportional zur Anzahl der Instanzen die es für eine Klasse in den Daten gibt: Je mehr Daten für eine Klasse vorhanden sind, desto höher der Einfluss. Dies liegt daran, dass die Formeln so angepasst werden, dass die Summen direkt über die Instanzen der Klassen gebildet werden.</p>
<p>Wenn man also eine hohe Class Level Imbalance hat, sollte man das Makro Average wählen, wenn alle Klassen fair und gleichmäßig vom Gütemaß repräsentiert werden sollen. Die hat den potentiellen Nachteil, das Klassen mit wenigen Instanzen einen sehr großen Einfluss auf das Ergebnis haben könnten. Andersrum sollte man Micro Average verwenden, wenn es in Ordnung ist, dass die Ergebnisse durch die Anzahl der Instanzen pro Klasse gewichtet werden.</p>
</div>
<div class="section" id="jenseits-der-confusion-matrix">
<h3><span class="section-number">7.3.7. </span>Jenseits der Confusion Matrix<a class="headerlink" href="#jenseits-der-confusion-matrix" title="Permalink to this headline">¶</a></h3>
<p>Alle Gütemaße die wir betrachtet haben basieren auf der Confusion Matrix. Dies ist der übliche Ansatz um die Güte von Klassifikationsalgorithmen zu bestimmen. Die Confusion Matrix basierten Gütemaße basieren jedoch alle auf einer Annahme, die in der Regel unrealistisch ist: Alle Fehler sind gleich wichtig. In der Praxis sind einige Fehler jedoch schlimmer als andere. Das Risiko ist bei einer großen Kreditsumme zum Beispiel höher als bei einer kleineren. Wenn ein Schuldner eine große Summe nicht zurückzahlen kann, ist eine falsch positive Kreditwürdigkeit schlimmer, als bei einem kleinen Kreditsumme. Daher sollte man sich neben der Confusion Matrix auch immer Gedanken über Kosten, Nutzen, und Risiken die mit richtig positiven, falsch positiven, richtig negativen, und falsch negativen Ergebnissen verbunden sind. Man könnte zum Beispiel eine Kostenmatrix definieren, die die Gewinne und Verluste genauer aufschlüsselt. Hierdurch kann man eine bessere Kostenfunktion für den Anwendungsfall bekommen, was üblicherweise zu besseren Ergebnisse führt <a class="footnote-reference brackets" href="#kdnuggets" id="id1">1</a>.</p>
</div>
</div>
<div class="section" id="decision-surfaces">
<h2><span class="section-number">7.4. </span>Decision Surfaces<a class="headerlink" href="#decision-surfaces" title="Permalink to this headline">¶</a></h2>
<p>Im Folgenden verwenden wir eine visuelle Unterstützung um zu zeigen, wie verschieden Klassifikationsalgorithmen Hypothesen nutzen, um Instanzen zu klassifizieren: das <em>Decision Surface</em>. Bei einem Decision Surface handelt es sich um eine geometrische interpretation von Klassifikationsergebnissen: Der Raum der Instanzen wird in verschiedene Regionen aufgeteilt, so das jede Region die Instanzen gleich klassifiziert. Bei 2-Dimensionalen Daten heißt das, dass man farbige Flächen zeichnen kann, wobei die Farben die Klassen repräsentieren. In höherdimensionalen Räumen kann man die Decision Surfaces leider nicht gut darstellen. Wir nutzen die Kelchblattlänge und Kelchblattbreite der Irisdaten (TODO REF) als Beispieldatensatz um zu zeigen wie die Klassifikationsalgorithmen arbeiten.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>

<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span><span class="o">.</span><span class="n">target_names</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># use only first two columns from iris data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sepal length (cm)&#39;</span><span class="p">:</span> <span class="s1">&#39;Kelchblattlänge&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;sepal width (cm)&#39;</span><span class="p">:</span> <span class="s1">&#39;Kelchblattbreite&#39;</span><span class="p">})</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Iris Daten&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">scatter</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">class_names</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_9_0.png" src="../_images/kapitel_07_9_0.png" />
</div>
</div>
<p>Man erkennt das die Setosas klar von den anderen Arten der Iris getrennt sind, während sich die Versicolor und Virginica überlappen. Wir nutzen jetzt die Hintergrundfarbe um zu zeigen wie ein Decision Surface aussieht: Lila für Setosa, Türkis für Versicolor und Gelb für Virginica. Als Beispiel definieren wir selbst Regeln zu Klassifikation:</p>
<ul class="simple">
<li><p>Alle Instanzen, deren Kelchblattlänge kleiner als 5,5 ist, werden als Setosa klassifiziert.</p></li>
<li><p>Alle Instanzen, deren Kelchblattlänge zwischen 5,5 und 6 liegt, werden als Versicolor klassifiziert.</p></li>
<li><p>Alle Instanzen, deren Kelchblattlänge größer als 6 ist, werden als Virginica klassifiziert.</p></li>
</ul>
<p>Hierdurch bekommen wir folgendes Decision Surface.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">DummyModel</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="c1"># init everything as Versicolor</span>
        <span class="n">result</span><span class="p">[</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">&lt;</span><span class="mf">5.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># rule for Setosa</span>
        <span class="n">result</span><span class="p">[</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">6</span><span class="p">]</span>   <span class="o">=</span> <span class="mi">2</span> <span class="c1"># rule for Virginica</span>
        <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">plot_decision_surface</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="o">.</span><span class="mi">01</span> <span class="c1"># step size in the mesh</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">shading</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">title</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    
<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">DummyModel</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Decision Surface of the Example&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_11_0.png" src="../_images/kapitel_07_11_0.png" />
</div>
</div>
<p>Die Linie, wo sich die Farben ändert, nennt man <em>Decision Boundary</em>. Die Decision Boundaries bestimmen die Struktur der Ergebnisse und sind eine wesentliche Eigenschaft von Klassifikationsalgorithmen, die sowohl praktische Auswirkungen auf die Ergebnisse, als auch ein wichtiges Hilfsmittel für die mathematische Beschreibung und Analyse ist.</p>
</div>
<div class="section" id="k-nearest-neighbor">
<h2><span class="section-number">7.5. </span><span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbor<a class="headerlink" href="#k-nearest-neighbor" title="Permalink to this headline">¶</a></h2>
<p>Der erste Klassifikationsalgorithmus den wir betrachten ist der <span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbor Algorithmus. Dieser Algorithmus basiert auf der Idee, dass Instanzen der selben Klasse nah beieinander liegen. Diese Idee kennen wir bereits von den Clusteralgorithmen, die die Distanz als Maß für die Ähnlichkeit von Instanzen verwendet haben. Der einfachste Ansatz ist, das man einfach jede Instanz so klassifiziert, wie ihren nächsten Nachbarn. Für die Irisdaten würden mit dieser Strategie das folgende Decision Surface bekommen.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces von 1-Nearest Neighbor&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_13_0.png" src="../_images/kapitel_07_13_0.png" />
</div>
</div>
<p>Wir können diese Konzept auf <span class="math notranslate nohighlight">\(k\)</span> Nachbarn erweitern um den vollständigen <span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbor Algorithmus zu erhalten. Hierfür weisen wir die Klasse als das Mehrheitsvotum der <span class="math notranslate nohighlight">\(k\)</span> nächsten Nachbarn zu. Hierdurch ändern sich auch die Decision Surfaces für verschiedene Werte von <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span>
                      <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces von 1-Nearest Neighbor&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span>
                      <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces von 3-Nearest Neighbor&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span>
                      <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces von 5-Nearest Neighbor&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">20</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span>
                      <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces von 20-Nearest Neighbor&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">top</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_15_0.png" src="../_images/kapitel_07_15_0.png" />
</div>
</div>
<p>Man erkennt keine klare Struktur für der Decision Boundaries zwischen den Decision Surfaces bei den vom <span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbor berechneten Hypothesen. Die Decision Boundaries haben viele scharfe Kanten und keine regelmäßige Struktur wie man es erwarten würde, wenn diese zum Beispiel das Ergebnis einer differenzierbaren Funktion wären. Genau dies fehlt beim <span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbor Algorithmus auch: eine mathematische Beschreibung der Hypothese, welche eine Generalisierung der Daten ist. Stattdessen haben wir einen <em>Instanzbasierentes</em> verfahren, welches die Decision Boundaries durch den direkten Vergleich von Instanzen miteinander definiert. Wenn wir betrachten wie sich das Ergebnis für größere Werte von <span class="math notranslate nohighlight">\(k\)</span> verändert, sehen wir dass der Einfluss einzelner Datenpunkte reduziert wird. Bei <span class="math notranslate nohighlight">\(k=1\)</span> sieht man zum Beispiel noch eine einzelne gelbe Instanz an der linken Seiten der Grafik. Hierbei handelt es sich vermutlich um einen Ausreißer der gelben Klasse (Virginica). Die Konsequenz dieses Ausreißers ist, dass es eine relativ große gelbe Region gibt, obwohl dieser Bereich eher lila oder türkis sein sollte. Bei größeren Nachbarschaftsgrößen verschwindet dieser Effekt. Andererseits bedeutet eine große Nachbarschaft auch, das Instanzen die weiter weg liegen, die Klassifikation beeinflussen. Mit <span class="math notranslate nohighlight">\(k=20\)</span> bekommt man hierdurch eine relativ scharfe Trennung von Türkis und Gelb. Dies liegt aber nicht an den Istanzen, die direkt im Bereich der Trennung liegen, sondern an den Istnazen die weiter im Hintergrund liegen: sobald es mehr gelbe Punkte im Hintergrund gibt, bleibt die Farbe stabil gelb. Durch diese größeren Abstände kann es jedoch auch komische Effekte geben, wie man es zum Beispiel an der “türkisen Insel” in einem Ansonsten stabil gelben Bereich sieht. Das besondere hier ist, dass es in dieser Insel nicht einmal eine türkise Instanz gibt, sie entsteht also nur aufgrund von Datenpunkten die relativ weit weg liegen.</p>
<p>Um den effekt der Nachbarschaftsgröße weiter zu verdeutlichen betrachen wir den Punkt (6, 3,5) im Detail.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>

<span class="k">def</span> <span class="nf">plot_neighborhood</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">pnt</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">nbrs</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;ball_tree&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">pnt_neighbors</span> <span class="o">=</span> <span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">pnt</span><span class="p">)</span>
    <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pnt</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
        <span class="n">pnt2</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">pnt_neighbors</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">pnt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">pnt2</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">pnt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">pnt2</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">color</span><span class="o">=</span><span class="n">sc</span><span class="o">.</span><span class="n">to_rgba</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">pnt_neighbors</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]]))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pnt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">pnt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">sc</span><span class="o">.</span><span class="n">to_rgba</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%i</span><span class="s1"> Neighbors&#39;</span> <span class="o">%</span> <span class="n">k</span><span class="p">)</span>
    
<span class="n">pnt</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">6</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">]]</span>
<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plot_neighborhood</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pnt</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_neighborhood</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">pnt</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plot_neighborhood</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pnt</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_neighborhood</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">pnt</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">top</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_17_0.png" src="../_images/kapitel_07_17_0.png" />
</div>
</div>
<p>Bei <span class="math notranslate nohighlight">\(k=1\)</span> und <span class="math notranslate nohighlight">\(k=3\)</span> wird dieser Punkt türkis Markiert, da die es zwei relativ nahe türkise Datenpunkte gibt, jedoch nur einen gelben. Bei <span class="math notranslate nohighlight">\(k=5\)</span> wechselt der Punkt zur gelben Klasse, da es jetzt zwei weitere gelbe Punkte in der Nachbarschaft gibt. Bei <span class="math notranslate nohighlight">\(k=20\)</span> sieht man wie der Punkt zwar gelb bleibt, aber sehr viele Punkte mittlerweile mitbestimmen, obwohl die meisten davon sehr weit weg von unserem Datenpunkt liegen.</p>
</div>
<div class="section" id="entscheidungsbaume">
<h2><span class="section-number">7.6. </span>Entscheidungsbäume<a class="headerlink" href="#entscheidungsbaume" title="Permalink to this headline">¶</a></h2>
<p>Stellen wir uns vor das wir uns ein Auto kaufen wollen. Als Käufer hat man in der Regel ein paar Kriterien, die man sich überlegt, bevor man zum Händler geht: Es sollte zum ein 5-Türer mit einer gewissen Leistung und einer gewissen Kofferraumgräße sein. Manche dieser Kriterien sind wichtiger als andere, sie werden als zuerst Anwendet um Autos auszuschließen. Während die Anzahl der Türen fest steht, könnte es zum Beispiel bei der Leistung einen gewissen Spielraum geben. Dies ist die Intuition von <em>Entscheidungsbäumen</em> (engl. <em>decision tree</em>): Es werden Schrittweise logische Regeln auf die Merkmale angewendet um Entscheidungen zu treffen. Die Entscheidungen sind als Baum organisiert.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">plot_tree</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_19_0.png" src="../_images/kapitel_07_19_0.png" />
</div>
</div>
<p>In diesem Entscheidungsbaum wird als ersten eine Entscheidung mit Hilfe der Kelchblattlänge getroffen. Ist diese kleiner oder gleich 5,55 gehen wir nach links, andernfalls nach rechts. Außerdem sieht man noch weitere Informatione. Das Feld<code class="docutils literal notranslate"><span class="pre">entropy</span></code> ignorieren wir fürs erste. Der Wert von <code class="docutils literal notranslate"><span class="pre">samples</span></code> gibt an, wie viele Instanzen in den Trainingsdaten zur Verfügung standen, um diese Entscheidung zu lernen. Der Wert von <code class="docutils literal notranslate"><span class="pre">value</span></code> gibt an, wie viele Instanzen von jeder Klasse in <code class="docutils literal notranslate"><span class="pre">sample</span></code> enthalten sind. Bei <code class="docutils literal notranslate"><span class="pre">class</span></code> sieht man, welche Klassifikation man hätte, wenn man keine weiteren Entscheidungen trifft. Basierend auf der Entscheidung werden die Daten partioniert: 59 Instanzen haben eine Kelchblattlänge die kleiner oder gleich 5,55 ist, bei 91 Instanzen ist die Kelchblattlänge größer. Dies können wir an den <code class="docutils literal notranslate"><span class="pre">samples</span></code> in den Knoten auf der linken, bzw. rechten Seite ablesen. Bei den 59 Instanzen auf der rechten Seite sind es 47 Instanzen aus der ersten Klasse (Setosa), 11 Instanzen aus der zweiten Klasse (Versicolor), und 1 Instanz aus der dritten Klasse (Virginica). Auf der untersten Ebene (den Blättern des Baums) werden keine weiteren Entscheidungen getroffen.</p>
<p>Für die Interpretation eines Entscheidungsbaums braucht man weder Expertenwissen, noch ein hohes Verständnis über die Daten selbst. Das ist auch der große Vorteil von Entscheidungsbäumen: Die Klassifikation ist nachvollziehbar und Domänenexperten können durch manuelle Analyse die plausibilität der Regeln bewerten.</p>
<p>Das Decision Surface des obigen Baums sieht wie folgt aus.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surface des obigen Baums&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_21_0.png" src="../_images/kapitel_07_21_0.png" />
</div>
</div>
<p>Wie man sieht trennen die Decision Boundires die Decision Surfaces durch Achsenparallele geraden. Dies ist eine Eigenschaft von Entscheidungsbäume. Dadurch das wir logische Entscheidungen der form <span class="math notranslate nohighlight">\(\leq\)</span> und <span class="math notranslate nohighlight">\(\geq\)</span> (manchmal auch <span class="math notranslate nohighlight">\(=\)</span>) haben, werden die Daten durch jede Entscheidung in zwei Partionen zerteilt, so dass die Entscheidung orthogonal zu einer Achse und parallel zu allen anderen Achsen ist.</p>
<p>Das Training von Entscheidungsbäumen basiert auf einem relativ einfachen rekursiven Algorithmus.</p>
<ol class="simple">
<li><p>Beende den Algorithmus wenn die Daten ausreichen “rein” oder “zu klein” sind, oder die maximale Tiefe des Entscheidungsbaums erreicht ist.</p></li>
<li><p>Bestimme das “Merkmal mit dem höchsten Informationsgehalt” <span class="math notranslate nohighlight">\(X'\)</span>.</p></li>
<li><p>Partitioniere die Daten durch eine Regel für das Merkmal <span class="math notranslate nohighlight">\(X'\)</span>.</p></li>
<li><p>Wende den Algorithmus anfangend mit Schritt 1 rekursiv auf die Partionen an um den linken und rechten Teilbaum zu erstellen.</p></li>
</ol>
<p>Wir müssen also nur das “Merkmal mit dem höchsten Informationsgehalt” finden, die Daten partitionieren, und das wiederholen bis die Daten “rein” oder “zu klein” sind oder der Baum zu Tief wird. Das Konzept von “zu klein” ist immer gleich: Die Anzahl der Instanzen in einer Partition unterschreitet einen vorher festgelegten Grenzwert. Durch dieses Kriterium können wir verhindern das Entscheidungen auf zu wenig Daten getroffen werden und somit erzwingen dass es für jede getroffene Entscheidung eine solide Datengrundlage gibt. Die maximale Tiefe beschränkt die Komplexität der Regeln. Die tiefe des Baums ist definiert als die Anzahl der Entscheidungen die getroffen werden, bevor die Klasse bestimmt wird. Der obige Baum hat zum Beispiel eine Tiefe von 2. Zur Bestimmung der Reinheit, des Informationsgehalts und der Partionen gibt es verschiedene Ansätze. Von diesen Ansätzen hängt auch ab um was für eine Art von Entscheidungsbaum es sich handelt, zum Beispiel CART, ID3 oder C4.5.</p>
<p>Als Beispiel betrachten wir wie man diese Konzepte durch die <em>Informationstheory</em> definieren kann. Die Idee der Reinheit lässt sich Informationstheoretisch aus die Unsicherheit der Daten fassen und der Informationsgehalt die die <em>Gemeinesame Information</em> (engl. <em>mutual information</em>) durch die man den Informationsgewinn (engl. <em>information gain</em>) messen kann. Da sich die Informationstheory mit der Zufallsvariablen befasst, interpretieren wir hier alles als Zufallsvariablen: Die Klassifikation von Instanzen ist eine Zufallsvariable <span class="math notranslate nohighlight">\(C\)</span> und unsere Merkmale sind Zufallsvariablen <span class="math notranslate nohighlight">\(X_1, ..., X_m\)</span>. Das Kernkonzept der Informationstheory ist die <em>Entropie</em>. Je höher die Entropy einer Zufallsvariablen ist, desto unsicherer und zufälliger der Ausgangs eines Zufallsexperiments mit dieser Variablen. Die Entropie einer Zufallsvariable die einen fairen Münzwurf beschreibt (50% Kopf, 50% Zahl) ist zum Beispiel 1, der höchst mögliche Wert der Entropy. Die Entropie einer Zufallsvariable für eine manipulierte Münze die immer auf Kopf landet, hätte eine Entropy von 0, es gäbe als keine Unsicherheit über das Ergebnis. Wenn wir also Entscheidungen mit einer hohen Sicherheit treffen wollen, müssen wir Partionen finden, so dass die Entropy der Klassifikation <span class="math notranslate nohighlight">\(C\)</span> minimiert wird. Die Entropy von <span class="math notranslate nohighlight">\(C\)</span> ist definiert als</p>
<div class="math notranslate nohighlight">
\[H(C) = -\sum_{c \in C} p(c) \log p(c)\]</div>
<p>wobei <span class="math notranslate nohighlight">\(p(c)\)</span> die Wahrscheinlichkeit das eine Instanz einer Partition zu Klasse <span class="math notranslate nohighlight">\(c\)</span> gehört ist. Sobald die Entropy <span class="math notranslate nohighlight">\(H(C)\)</span> unter einen Grenzwert fällt, ist die Entscheidung sicher genug und die Partion “rein”.</p>
<p>Die <em>bedingte Entropie</em> können wir nutzen um abzuschätzen, wie viel wissen über die Klasse wir durch ein Merkmal bekommen. Hierzu gibt die bedingte Entropy die Unsicherheit der Klasse <span class="math notranslate nohighlight">\(C\)</span> an, wenn ein Merkmal <span class="math notranslate nohighlight">\(X'\)</span> vollständig bekannt ist. Die Entropie der Klassifikation <span class="math notranslate nohighlight">\(C\)</span> bedingt auf das Merkmal <span class="math notranslate nohighlight">\(X'\)</span> ist definiert als</p>
<div class="math notranslate nohighlight">
\[H(C|X') = -\sum_{x \in X'} p(x) \sum_{c \in C} p(c|x) \log p(c|x)\]</div>
<p>wobei <span class="math notranslate nohighlight">\(p(c|x)\)</span> die bedingte Wahrscheinlichkeit der Klasse <span class="math notranslate nohighlight">\(c\)</span> gegebem dem Wert <span class="math notranslate nohighlight">\(x\)</span> des Merkmals <span class="math notranslate nohighlight">\(X' \in \{X_1, ..., X_m\}\)</span> ist. Die bedingte Entropy ist also ein Maß dafür, wie der <em>Informationsgewinn</em> über <span class="math notranslate nohighlight">\(C\)</span> mit <span class="math notranslate nohighlight">\(X'\)</span> ist. Je niedriger die bedingte Entropy, desto höher der Informationsgewinn. Daher nennt man ein Merkmal <em>informativ</em>, wenn es die bedingte Entropie der Klassifikation reduziert. Wenn man die Entropie der Klassifikation mit der bedingte Entropie kombiniert, bekommt man den <em>Informationsgewinn</em> durch die Reduktion der Entropy als</p>
<div class="math notranslate nohighlight">
\[I(C; X') = H(C)-H(C|X').\]</div>
<p>Das informativste Merkmal ist also das Merkmal, was den Informationsgewinn maximiert. Sobald man dieses Merkmal gefunden hat, bestimmt eine Regel, so dass die mittlere Entropy der Klassikation der Partitionen minimiert wird.</p>
<p>Oben haben wir bereits ein Decision Surface für einen Entscheidungsbaum mit einer niedrigen Tiefe von 2 betrachtet. Dies hat uns zwar geholfen die Struktur von Entscheidungsbäumen und der Entscheidungen zu verstehen, reicht jedoch in der Regel nicht aus um ein gutes Ergebnis aus einem Datensatz zu erzielen. Hierfür benötigt man mehr Entscheidungen, also einen tieferen Entscheidungsbaum. Wenn wir die Tiefe nicht beschränken und keinen Grenzwert für zu wenige Daten um eine Entscheidung zu treffen angeben, bekommen wir folgendes Decision Surface.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surface von einem Entscheidungsbaum ohne Einschränkungen&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_23_0.png" src="../_images/kapitel_07_23_0.png" />
</div>
</div>
<p>Man sieht das sehr viel mehr Entscheidungen getroffen werden, für jede Achsenparallel Decision Boundary eine Entscheidung getroffen wird. Dies führt aber leider zu <em>Overfitting</em>, das heißt es wurde einzelne Datenpunkte auswendig gelernt. Dies sieht man zum Beispiel bei der kleinen gelbe Flöche die man etwa Kelchblattlänge 7 sieht. Hier liegt nur eine gelbe Instanz mitten in einer ansonsten türkisen Umgebung. Hierfür gibt es noch einige weitere Beispiele, zum Beispiel auch zwische dem lila und dem türkisen Bereich. Um zu verhindert das soetwas passiert, müssen wir die die erlaubten Entscheidungen beschränken. Wenn wir fordern, dass nur Partitionen mit mindestens 5 Instanzen erlaubt sind, verschwinden diese kleinen Bereiche.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surface von einem Entscheidungsbaum mit</span><span class="se">\n</span><span class="s2">min. 5 Instanzen in jeder Partition&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_25_0.png" src="../_images/kapitel_07_25_0.png" />
</div>
</div>
</div>
<div class="section" id="random-forests">
<h2><span class="section-number">7.7. </span>Random Forests<a class="headerlink" href="#random-forests" title="Permalink to this headline">¶</a></h2>
<p>Die Idee von Random Forests ist es viele Entscheidungsbäume zu einem Klassifikationsmodell zu kombinieren. Die einzelnen Entscheidungsbäume nennt man hierbei <em>Random Tree</em>. Man spricht hierbei von <em>Ensemble Learning</em>, der Random Forest ist also ein Ensemble von Random Trees. Wie man aus viel Entscheidungsbäumen die Klassifikation eines Random Forests bekommt sieht man im Folgenden.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># Please note that the parameters we use here are not good and should not be used</span>
<span class="c1"># for any real examples. We use only four random trees so that we can better demonstrate </span>
<span class="c1"># the example. Usually, you should use hundreds of trees and more are better, but require</span>
<span class="c1"># more runtime (both for training and predictions). </span>
<span class="n">randomforest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">randomforest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">randomforest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">randomforest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">randomforest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">randomforest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Randomized Trees eines Random Forest&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_27_0.png" src="../_images/kapitel_07_27_0.png" />
</div>
</div>
<p>Jeder einzelne Random Trees ist für sich genommen kein gutes Klassifikationsmodell. Dies sieht man gut an den Decision Surfaces.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">randomforest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces vom ersten Random Tree&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">randomforest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces vom zweiten Random Tree&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">randomforest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces vom dritten Random Tree&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">randomforest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces vom vierten Random Tree&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">top</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_29_0.png" src="../_images/kapitel_07_29_0.png" />
</div>
</div>
<p>Der erste und der vierte Random Tree sind so schlecht, dass es keine Türkise region gibt. Für sich genommen haben wir also <em>schwache Klassifikationsmodelle</em> (engl. <em>weak classifier</em>). Wenn wir diese vier schwachen Klassifikationsmodelle zu durch Mitteln der Vorhersagen kombinieren, bekommen wir ein besseres Ergebnis.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">randomforest</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces der Kombination von vier Random Trees&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_31_0.png" src="../_images/kapitel_07_31_0.png" />
</div>
</div>
<p>Das man aus vielen schwachen Modellen ein gutes Ergebnis bekommt ähnelt dem Prinzip des Publikumsjokers: Wenn man eine zufällige Person fragt, ist die Wahrscheinlichkeit eine richtige Antwort zu bekommen niedriger, als wenn man eine Umfrage macht. Genauso verhält es sich auch mit schwachen Klassifikationsmodellen. Jeder der Random Trees ist zwar schlecht, aber jeder Random Tree hat auch stärken. Zusammengenommen addieren sich die stärken auf und überdecken dadurch die schwächen. Üblicherweise nutzt man bei einem Random Forest nicht nur vier Bäume, sondern hunderte oder sogar tausende von Bäumen. Hier ist das Ergebnis mit 1000 Random Trees.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces eines Random Forest mit 1000 Random Trees&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_33_0.png" src="../_images/kapitel_07_33_0.png" />
</div>
</div>
<p>Bisher haben wir nur erklärt, wie ein Random Forest aus einem <em>Ensemble</em> zu einem einzelnen Ergebnis kommt. Warum wir überhaupt unterschiedliche Entscheidungsbäume für die gleichen Trainingsdaten bekommen ist noch unklar. Schließlich ist der Algorithmus zum Training von Entscheidungsbäumen deterministisch: Wenn man einen Entscheidungsbaum wie im vorigen Abschnitt zwei mal mit den gleichen Daten trainiert, bekommt man zwei identische Entscheidungsbäume. Was uns also noch fehlt ist das Verständnis der Rolle des Zufalls beim Training von Random Forests.</p>
<p>Wenn man sich die einzelnen Random Trees genau anguckt, sieht man bereits einige Hinweise darauf, was randomisiert ist. Hier ist noch einmal der erste der vier Entscheidungsbäume von oben.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">randomforest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_35_0.png" src="../_images/kapitel_07_35_0.png" />
</div>
</div>
<p>Man sieht das es bei in der Wurzel des Baums (die erste Entscheidung) nur 101 Instanzen, statt den gesamten 150 Instanzen gibt. Außerdem gibt es nicht von jeder Irisart gleich viele Instanzen. Dieser Entscheidungsbaum wurde also offensichtlich nicht mit den original Irisdaten trainiert. Was wir hier sehen ist die erste Randomisierung des Trainings: Als Trainingsdaten bekommt jeder Random Tree ein <em>Boostrap Sample</em> der Daten. Wenn wir 150 Instanzen in den Trainingsdaten haben, heißt dass, das wir 150 Instnazen <em>mit zurücklegen</em> aus den Trainingsdaten ziehen. Da wir mit zurücklegen ziehen, bekommen wir einige Instanzen mehrfach, andere gar nicht. Im Mittel bekommt man etwa 63.2% verschiedene Instanzen, der Rest sind duplikate. Alle Random Trees bekommen also andere Trainingsdaten. Bitte beachten Sie, dass <code class="docutils literal notranslate"><span class="pre">samples</span></code> bei einem Random Tree nicht die Anzahl der Instanzen, sondern die Anzahl der Instanzen ohne Duplikate angibt. Das ziehen von Bootstrap Samplen um mehrere Klassifikationsmodelle zu Trainieren nennt man auch auch <em>Bagging</em>, was kur für <em>Boostrap Aggregating</em> ist.</p>
<p>Es gibt noch eine weitere zufällige Komponenten im Training von Random Forests. Nicht jeder Random Tree bekommt alle Merkmale. Stattdessen bekommt jeder Random Tree nur eine Teilmenge der Merkmale zur Verfügung. Üblicherweise verwendet man die Quadratwurzel der Anzahl der Merkmale pro Baum. Wenn es vier Merkmale gibt, darf bekommt ein Random Tree also nur Zugriff auf <span class="math notranslate nohighlight">\(\sqrt[4}=2\)</span> Merkmale. Der Grund hierfür ist, dass sich die Bäume sonst möglicherweise stark ähneln. Wenn ein Merkmal für die gesamten Daten informativ ist, ist es in der Regel auf einem Bootstrap Sample ähnlich informativ. Daher wäre das Risiko groß, dass die Random Trees alle die gleiche Struktur hätten. Wenn nur eine Teilmenge von Merkmalen zur Verfügung steht, haben auch schwächere Merkmale eine Chance benutzt zu werden und damit ihre Information in den Random Forest einzubringen. So wird sichergestellt, das die einzelnen Random Trees auch wirklich unterschiedliche stärken und schwächen haben.</p>
</div>
<div class="section" id="logistische-regression">
<h2><span class="section-number">7.8. </span>Logistische Regression<a class="headerlink" href="#logistische-regression" title="Permalink to this headline">¶</a></h2>
<p>Die <em>Logistische Regression</em> (engl. <em>logistic regression</em>) berechnete die <em>Chance</em>, bzw. die <em>Odds</em> das eine Instanz zu einer Klasse gehört. Die Odds sind ein Konzept der Statistik, was man zum Beispiel bei Sportwetten wiederfindet. Sei <span class="math notranslate nohighlight">\(P(Y=c)\)</span> die Wahrscheinlichkeit das eine Zufallsvariable <span class="math notranslate nohighlight">\(Y\)</span> den Wert <span class="math notranslate nohighlight">\(c\)</span> annimmt. Die Odds, dass <span class="math notranslate nohighlight">\(Y\)</span> gleich <span class="math notranslate nohighlight">\(c\)</span> ist, sind dann definiert als</p>
<div class="math notranslate nohighlight">
\[odds(c) = \frac{P(Y=c)}{1-P(Y=c)}.\]</div>
<p>Die Bedeutung der Odds kann man sich gut an einem Beispiel verdeutlichen: Unsere Zufallsvariable <span class="math notranslate nohighlight">\(Y\)</span> könnte die Wahrscheinlichkeit eine Prüfung zu bestehen modellieren und für <span class="math notranslate nohighlight">\(c=pass\)</span> mit <span class="math notranslate nohighlight">\(P(Y=pass)=0,75\)</span> ist die 75% Wahrscheinlichkeit, dass die Prüfung bestanden wird. Die Odds das die Prüfung bestanden wird sind damit</p>
<div class="math notranslate nohighlight">
\[odds(pass) = \frac{0.75}{1-0.75} = 3.\]</div>
<p>Mit anderen Worten, die Odds/Chance die Prüfung zu bestehen ist drei zu eins.</p>
<p>Die Odds sind verwand mit der <em>Logit</em>-Funktion, welche als</p>
<div class="math notranslate nohighlight">
\[logit(P(Y=c)) = \ln \frac{P(Y=c)}{1-P(Y=c)},\]</div>
<p>definiert ist. Dies ist nichts anderes als der natürliche logarithmus der Odds von <span class="math notranslate nohighlight">\(c\)</span>. Von dieser Funktion hat die Logistische Regression auch ihren Namen. Wenn wir sagen das unsere Zufallsvariable <span class="math notranslate nohighlight">\(Y\)</span> die Wahrscheinlichkeit das eine Instanz zu einer Klasse gehört modelliert und <span class="math notranslate nohighlight">\(c \in C\)</span> eine Klasse ist, dann ist <span class="math notranslate nohighlight">\(logit(P(Y=c))\)</span> nichts anderes als der logarithmus der Odds das eine Instanz zu einer einer gehört. Die Regression die verwendet wird ist eine einfache lineare Regression (TODO ref 8) der Form</p>
<div class="math notranslate nohighlight">
\[logit(P(Y=c)) = \ln \frac{P(Y=c)}{1-P(Y=c)} = b_0 + b_1x_1 + ... + b_mx_m\]</div>
<p>wobei die Merkmale numerisch sein müssen, also <span class="math notranslate nohighlight">\(\mathcal{F} \subseteq \mathbb{R}\)</span>. Im 2-Dimensionalen Fall beschreibt die Formel der Linearen Regression eine Line, im 3-Dimensionalen Fall eine Ebene, im <span class="math notranslate nohighlight">\(m\)</span>-Dimensionalen Fall eine Hyperebene. Das Decision Surface der Logistischen Regression der Irisdaten sieht wie folgt aus.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces von Logistischer Regression&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_37_0.png" src="../_images/kapitel_07_37_0.png" />
</div>
</div>
<p>Die Entscheidungen werden also durch Linien getroffen, die den Merkmalsraum unterteilen.</p>
<p>Ein großer Vorteil der Logistischen Regression ist, dass man den Einfluss der Merkmale aus die Klassifikation direkt nachvollziehen kann. Dies liegt an der Struktur des Regressionsmodells und der Logit-Funktion. Mit einigen Umformungen bekommen wir folgendes.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
&amp;\ln \frac{P(Y=c)}{1-P(Y=c)} = b_0 + b_1x_1 + ... + b_mx_m \\
\Rightarrow\quad &amp;\frac{P(Y=c)}{1-P(Y=c)} = \exp( b_0 + b_1x_1 + ... + b_mx_m ) \\
\Rightarrow\quad &amp;odds(c) = \exp( b_0 ) \cdot \Pi_{i=1}^m \exp(b_ix_i) \\
\end{split}
\end{split}\]</div>
<p>Die Odds einer Klasse ist also das Produkt der den Exponenten von <span class="math notranslate nohighlight">\(b_ix_i\)</span>. Die Auswirkung des <span class="math notranslate nohighlight">\(i\)</span>-ten Merkmals auf die Odds ist damit <span class="math notranslate nohighlight">\(\exp(b_ix_i)\)</span>. Man bezeichnet <span class="math notranslate nohighlight">\(\exp(b_i}\)</span> auch als die <em>Odds Ratio</em>  des <span class="math notranslate nohighlight">\(i\)</span>-ten Merkmals. Die Odds Ratio definiert wie sehr sich die Odds der Klasse in Abhängigkeit eines Merkmals ändern. Eine Odds Ratio von 2 bedeutet zum Beispiel, dass sich die Odds verdoppeln, wenn sich der Wert des Merkmals um eins erhöht. Im Allgemeinen heißt eine Odds Ratio größer als eins, dass sich die Odds erhöhen wenn sich der Wert des Merkmals erhöht. Ein Wert kleiner 1 heißt, dass sich die Odds reduzieren, wenn sich der Wert des Merkmals reduziert. Da man den logarihmus verwenden muss aus <span class="math notranslate nohighlight">\(\exp(b_i)\)</span> den eigentlichen Koeffizienten zu berechnen, spricht man bei den Koeffizienten auch von den <em>Log Odds Ratios</em>.</p>
<p>Betrachten wir jetzt die Odds Ratios der Logistischen Regression für die Irisarten.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">odds_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">odds_df</span><span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="n">odds_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Kelchblattlänge</th>
      <th>Kelchblattbreite</th>
      <th>Intercept</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>setosa</th>
      <td>0.066610</td>
      <td>10.216701</td>
      <td>2733.180675</td>
    </tr>
    <tr>
      <th>versicolor</th>
      <td>1.845467</td>
      <td>0.207923</td>
      <td>6.328398</td>
    </tr>
    <tr>
      <th>virginica</th>
      <td>8.134952</td>
      <td>0.470746</td>
      <td>0.000058</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Anhand Odds sehen wir, dass sich die Chance das es sich um eine Setosa handelt stark mit der Kelchblattbreite erhöht und mit der Kelchblattlänge reduziert. Außerdem sehen wir das die Chance das es sich um eine Versicolor oder Virginica handelt steigt, wenn die Kelchblattlänge erhöht und die Kelchblattbreite reduziert wird. Der <em>intercept</em> markiert die Odds, wenn der Wert für alle Merkmale gleich 0 ist, es sich als um <span class="math notranslate nohighlight">\(b_0\)</span> aus der Gleichung der Logistischen Regression. Alle Änderungen der Merkmale müssen also in Relation zum Intercept betrachtet werden. Da der Intercept für Versicolor größer ist als für Virginica, gibt es eine Region wo die Odds für Versivolor größer sind, obwohl sich die Chance das es sich um eine Virginica handelt stärker erhöht, sowohl in der Kelchblattlänge, als auch in der Kelchblattbreite.</p>
<blockquote>
<div><p><strong>Bemerkung:</strong></p>
<p>In diesem Fall ist der Intercept etwas irreführent. Während der Intercept für Setosa sehr hoch ist, sind die Odds für die Sepal Length sehr niedrig. Da alle Instanzen eine Sepal Length größer als vier haben, verschwindet dieser hohe Intercept, bis man bei den Daten angekommen ist. Um solche Effekte zu vermeiden, sollte man die Daten immer zentrieren, so dass die Werte aller Merkmale symmetrisch zur 0 sind. Hierfür ist zum Beispiel die <em>Z-score Standardisierung</em> geeignet. Wenn die Interpretation der Koeffizieten das Hauptziel ist, sollte man außerdem nicht scikit-learn verwenden, sondern stattdessen Paket wie statsmodels. Hier ist die Regressionanalyse deutlich detailierter, insbesondere was die Analyse der statistischen Significanz der Ergebnisse angeht.</p>
</div></blockquote>
</div>
<div class="section" id="naive-bayes">
<h2><span class="section-number">7.9. </span>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this headline">¶</a></h2>
<p>Der Satz von Bayes (engl. <em>Bayes Law</em>) ist einer der fundamentalen Gleichungen der Stochastik und definiert als</p>
<div class="math notranslate nohighlight">
\[P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}.\]</div>
<p>Der Satz von Bayes ist ein wichtiges Hilfsmittel um mit bedingten Wahrscheinlichkeiten zu rechnen. Wenn wir unsere Merkmale als Zufallsvariable <span class="math notranslate nohighlight">\(X\)</span> interpretieren und unsere Klassifikation als Zufallsvariable <span class="math notranslate nohighlight">\(Y\)</span>, dann ist <span class="math notranslate nohighlight">\(P(Y|X)\)</span> die bedingte Wahrscheinlichkeit der Klassifikation wenn die Merkmale bekannt sind. In der Theorie ist dies die perfekte Scoring-Funktion für eine Hypothese. Wir haben also</p>
<div class="math notranslate nohighlight">
\[P(c|x_1, ..., x_m) = \frac{P(x_1, ..., x_m|c)P(c)}{P(x_1, ..., x_m)}\]</div>
<p>für eine Klasse <span class="math notranslate nohighlight">\(c \in C\)</span> und eine Instanz <span class="math notranslate nohighlight">\((x_1, ..., x_m) \in \mathcal{F}\)</span>. Aus dem Satz von Bayes folgt also, dass die Wahrscheinlichkeit der Klasse <span class="math notranslate nohighlight">\(c\)</span> für die Instanz <span class="math notranslate nohighlight">\((x_1, ..., x_m)\)</span> gegeben ist als die Wahrscheinlichkeit der Instanz bei Daten der Klasse <span class="math notranslate nohighlight">\(c\)</span> zu beobachten, multipliziert mit der Wahrscheinlichkeit diese Klasse zu beobachten und geteilt durch die Wahrscheinlich die Instanz zu beobachten, unabhängig von der Klasse. Das Problem des Satzes von Bayes ist das man die Wahrscheinlichkeiten <span class="math notranslate nohighlight">\(P(x_1, ..., x_m|c)\)</span> und <span class="math notranslate nohighlight">\(P(x_1, ..., x_m)\)</span> in der Regel nicht berechnen kann, da man entweder ein detailiertes Wissen über die Verteilung der gemeinsamen Wahrscheinlichkeit aller Merkmale braucht oder eine sehr grpße Menge von Daten, so dass es für jede Kombination der Werte von Merkmalen auch Trainingsdaten gibt. Beides ist in der Praxis unrealistisch.</p>
<p>Stattdessen vereinfachen wir den Satz von Bayes zu <em>Naive Bayes</em>. Der Hauptaspekt von Naive Bayes ist die als <em>Naive Assumption</em> bekannte annahme dass die Merkmale bedingt auf die Klasse unabhängig voneinander sind. Für die Irisdaten würde das bedeuten, dass die Kelchblattlänge einer Setosa unabhängig der Kelchblattbreite einer Setosa ist. Die Daten zeigen, dass dies eindeutig nicht der Fall ist. Da diese Annahme fast nie erfüllt ist, ist sie daher ziemlich Naiv. Aus mathemtischer Sicht ist diese Annahme jedoch äußerst hilfreich, da für bedingte Unabhängig gilt, dass</p>
<div class="math notranslate nohighlight">
\[P(x_1, ..., x_m | c) = \Pi_{i=1}^m P(x_i|c),\]</div>
<p>also das wir nicht mehr die gemeinsame Wahrscheinlichkeit der Merkmale berechnen müssen, sondern stattdessen nur noch die Wahrscheinlichkeiten für die einzelnen Merkmale, bedingt auf die Klasse, berechnen müssen. Wenn wir dies in den Satz von Bayes einsetzen, bekommen wir</p>
<div class="math notranslate nohighlight">
\[P(c|x_1, ..., x_m) = \frac{\Pi_{i=1}^m P(x_i|c)P(c)}{P(x_1, ..., x_m)}.\]</div>
<p>Leider gibt es immernoch die gemeinsame Wahrscheinlichkeit <span class="math notranslate nohighlight">\(P(x_1, ..., x_m)\)</span> im Nenner. Diese Wahrscheinlichkeit ist aber unabhängig von der Klasse <span class="math notranslate nohighlight">\(c\)</span>. Wenn wir statt der exakten Wahrscheinlichkeit nur eine Scoring-Funktion brauchen, können wir den Nenner einfach weglassen, ohne dass sich die Klasse, welche den höchsten Score bekommt, ändert. Damit haben wir als Scoring-Funktion für Naive Bayes</p>
<div class="math notranslate nohighlight">
\[score(c|x_1, ..., x_m) = \Pi_{i=1}^m P(x_i|c)P(c).\]</div>
<p>Um die Scoring-Funktion berechnen zu können, müssen wir <span class="math notranslate nohighlight">\(P(x_i|c)\)</span> und <span class="math notranslate nohighlight">\(P(c)\)</span> berechnen. <span class="math notranslate nohighlight">\(P(c)\)</span> können wir einfach als Anteil der Instanzen der Klasse <span class="math notranslate nohighlight">\(c\)</span> von allen Instanzen berechnen.</p>
<p>Die beiden wichtigsten Ansätze zur Berechnung von <span class="math notranslate nohighlight">\(P(x_i|c)\)</span> sind <em>Multinomial Naive Bayes</em> und <em>Gaussian Naive Bayes</em>. Beim Multinomial Naive Bayes berechnet man <span class="math notranslate nohighlight">\(P(x_i|c)\)</span> als Anteil der Instanzen der Klasse <span class="math notranslate nohighlight">\(c\)</span> die den Wert <span class="math notranslate nohighlight">\(x_i\)</span> haben. Dieser Ansatz funktioniert sehr gut für kategorische Merkmale und Anzahlen. Für numerische Merkmale funktioniert Multinomial Naive Bayes in der Regel nicht, da die Anzahl der Instanzen die genau einen bestimmten numerischen Wert annehmen häufig 1 ist, da es unwahrscheinlich ist exakt den gleichen Wert mehrfach zu beobachten, wenn eine kontinuierliche Wahrscheinlichkeitsverteilung zugrunde liegt. Gaussian Naive Bayes ist besser geeignet für solche Merkmale. Beim Gaussian Naive Bayes nimmt man an, dass die Merkmale Normalverteilt sind und schätzt die Wahrscheinlichkeit <span class="math notranslate nohighlight">\(P(x_i|c)\)</span> durch die Dichtefunktion der Normalverteilung für dieses Merkmal.</p>
<p>Unten sieht man die Decision Surfaces von Multinomial Naive Bayes und Gaussian Naive Bayes für die Irisdaten.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span><span class="p">,</span> <span class="n">MultinomialNB</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">MultinomialNB</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces von Multinomial Naive Bayes&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_decision_surface</span><span class="p">(</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Decision Surfaces von Gaussian Naive Bayes&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_07_41_0.png" src="../_images/kapitel_07_41_0.png" />
</div>
</div>
<p>Die Decision Boundaries von Multinomail Naive Bayes sind linear, bei Gaussian Naive Bayes werden die Decision Boundaries durch quadratische Gleichungen beschrieben. Wir sehen das bei diesen Daten Multinomial Naive Bayes nicht gut funktioniert, insbesondere die Trennung von Türkis und Gelb sieht nicht sinnvoll aus. Dies ist aber zu erwarten, da es sich um numerische Daten handelt. Der Gaussian Naive Bayes hat dieses Problem nicht und liefert bessere Ergebnisse.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="kdnuggets"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p><a class="reference external" href="https://www.kdnuggets.com/2016/12/salford-costs-misclassifications.html">https://www.kdnuggets.com/2016/12/salford-costs-misclassifications.html</a></p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="kapitel_06.html" title="previous page"><span class="section-number">6. </span>Clusteranalyse</a>
    <a class='right-next' id="next-link" href="../exercises/uebung_01.html" title="next page">Übung 1</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Steffen Herbold<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>