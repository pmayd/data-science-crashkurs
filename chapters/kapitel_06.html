
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6. Clusteranalyse &#8212; Einführung in Data Science</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Klassifikation" href="kapitel_07.html" />
    <link rel="prev" title="5. Assoziationsregeln" href="kapitel_05.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Einführung in Data Science</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../vorwort.html">
   Vorwort
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Kapitel
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_01.html">
   1. Big Data und Data Science
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_02.html">
   2. Der Prozess von Data Science Projekten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_03.html">
   3. Erkunden der Daten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_04.html">
   4. Allgemeines zur Datenanalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_05.html">
   5. Assoziationsregeln
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Clusteranalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_07.html">
   7. Klassifikation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Übungen
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercises/uebung_01.html">
   Übung 1
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/chapters/kapitel_06.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/sherbold/einfuehrung-in-data-science"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sherbold/einfuehrung-in-data-science/main?urlpath=tree/content/chapters/kapitel_06.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uberblick">
   6.1. Überblick
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ahnlichkeitsmasze">
     6.1.1. Ähnlichkeitsmaße
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stadte-und-hauser">
     6.1.2. Städte und Häuser
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-means-algorithmus">
   6.2.
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -Means Algorithmus
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grundidee">
     6.2.1. Grundidee
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#der-algorithmus">
     6.2.2. Der Algorithmus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bestimmen-von-k">
     6.2.3. Bestimmen von
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probleme-des-k-means-algorithmus">
     6.2.4. Probleme des
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -Means Algorithmus
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#em-clustering">
   6.3. EM Clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     6.3.1. Grundidee
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     6.3.2. Der Algorithmus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     6.3.3. Bestimmen von
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probleme-des-em-clusterings">
     6.3.4. Probleme des EM Clusterings
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dbscan">
   6.4. DBSCAN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   6.5. Grundidee
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     6.5.1. Der Algorithmus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bestimmen-von-epsilon-und-minpts">
     6.5.2. Bestimmen von
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     und
     <span class="math notranslate nohighlight">
      \(minPts\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probleme-von-dbscan">
     6.5.3. Probleme von DBSCAN
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#single-linkage-clustering">
   6.6. Single Linkage Clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#der-slink-algorithmus">
     6.6.1. Der SLINK Algorithmus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dendrogramme">
     6.6.2. Dendrogramme
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probleme-von-slink">
     6.6.3. Probleme von SLINK
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vergleich-der-algorithmen">
   6.7. Vergleich der Algorithmen
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clusterformen">
     6.7.1. Clusterformen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anzahl-der-cluster">
     6.7.2. Anzahl der Cluster
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ausfuhrungszeit">
     6.7.3. Ausführungszeit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretierbarkeit-und-darstellung">
     6.7.4. Interpretierbarkeit und Darstellung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kategorische-merkmale">
     6.7.5. Kategorische Merkmale
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fehlende-merkmale">
     6.7.6. Fehlende Merkmale
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#korrelierte-merkmale">
     6.7.7. Korrelierte Merkmale
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#zusammenfassung-des-vergleichs">
     6.7.8. Zusammenfassung des Vergleichs
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="clusteranalyse">
<h1><span class="section-number">6. </span>Clusteranalyse<a class="headerlink" href="#clusteranalyse" title="Permalink to this headline">¶</a></h1>
<div class="section" id="uberblick">
<h2><span class="section-number">6.1. </span>Überblick<a class="headerlink" href="#uberblick" title="Permalink to this headline">¶</a></h2>
<p>Bei der Clusteranalyse, häufig auch einfach Clustern oder Clustering genannt, sucht man Gruppen von verwandten Objekten in einer Menge von Instanzen. Diese Gruppen nennt man auch Cluster. Betrachten Sie das Beispiel in  <a class="reference internal" href="#fig-clust-example"><span class="std std-numref">Fig. 6.1</span></a>.</p>
<div class="figure align-default" id="fig-clust-example">
<a class="reference internal image-reference" href="../_images/clustering_general.png"><img alt="../_images/clustering_general.png" src="../_images/clustering_general.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.1 </span><span class="caption-text">Gruppieren von Emoticons als Beispiel für das Clustern.</span><a class="headerlink" href="#fig-clust-example" title="Permalink to this image">¶</a></p>
</div>
<p>Auf der linken Seite sieht man verschiedene Emoticons, unsere Objekte. Beim Clustern werden die Objekte jetzt in Gruppen unterteilt, in diesem Fall in zwei Gruppen: Die glücklichen Emoticons and und traurigen Emoticons. Die Gruppen werden basierend auf den Merkmalen der Objekte bestimmt, andere Informationen stehen nicht zur Verfügung. Um die Trennung in glücklich und traurig zu erreichen, müssen diese Emotionen also von den Merkmalen repräsentiert werden. Wenn die Merkmale etwas anderes beschreiben würden, zum Beispiel die Farbe, würde das Ergebnis des Clusterns ein anderes sein: eine Trennung in gelbe und orange Emoticons. Wenn die Merkmale die Menge der Haare im Gesicht der Emoticons beschreiben würden, dann würde man die Emoticons ohne Bart von denen mit einem Schnauzer trennen. Auch wenn dies auf den ersten Blick offensichtlich erscheint, ist dieser Punkt extrem wichtig: Wenn die Merkmale nicht zur gewünschten Gruppierung passen, finden Sie vielleicht trotzdem Gruppen, doch diese sind Möglicherweise nicht sinnvoll für Ihren Anwendungsfall.</p>
<p>Etwas abstrakter können wir Clustern wie in <a class="reference internal" href="#fig-clust-abstract"><span class="std std-numref">Fig. 6.2</span></a> beschreiben.</p>
<div class="figure align-default" id="fig-clust-abstract">
<a class="reference internal image-reference" href="../_images/abstract_clustering_german.png"><img alt="../_images/abstract_clustering_german.png" src="../_images/abstract_clustering_german.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.2 </span><span class="caption-text">Konzept des Clusterings.</span><a class="headerlink" href="#fig-clust-abstract" title="Permalink to this image">¶</a></p>
</div>
<p>Wir haben also nur Objekte, für die wir mit Hilfe von einem Clusteralgorithmus eine sinnvolle Gruppierung suchen. Ohne die gefundenen Gruppen manuell zu analysieren weiß man nicht, was die Gruppen repräsentieren und ob die Beziehung zwischen den Objekten wirklich sinnvoll ist.</p>
<p>Formal haben wir eine Menge von Objekten <span class="math notranslate nohighlight">\(O = \{object_1, object_2, ...\}\)</span> welche Möglicherweise unendlich viele Elemente enthält. Außerdem haben wir eine Repräsentation der Objekte als Instanzen im Merkmalsraum <span class="math notranslate nohighlight">\(\mathcal{F} = \{\phi(o): o \in O\}\)</span>. Beim Clustern betrachtet man in der Regel numerische Merkmale, das heißt jede Instanz ist ein reellwertiger Vektor und es gilt also <span class="math notranslate nohighlight">\(\mathcal{F} \subseteq \mathbb{R}^d\)</span>. Das gruppieren der Objekte wird beschrieben durch eine Abbildung <span class="math notranslate nohighlight">\(c: \mathcal{F} \to G\)</span> wobei <span class="math notranslate nohighlight">\(G =\{1, ..., k\}\)</span> die Cluster sind und <span class="math notranslate nohighlight">\(k \in \mathbb{N}\)</span> die Anzahl der Cluster.</p>
<div class="section" id="ahnlichkeitsmasze">
<h3><span class="section-number">6.1.1. </span>Ähnlichkeitsmaße<a class="headerlink" href="#ahnlichkeitsmasze" title="Permalink to this headline">¶</a></h3>
<p>Die Grundvoraussetzung um ähnliche Objekte gruppieren zu können ist, dass man ihre Ähnlichkeit messen kann. Beim Clustern ist der Ansatz hierfür die Ähnlichkeit über den <em>Abstand</em> im Merkmalsraum zu messen. Je näher die Instanzen von Objekten einander sind, desto ähnlicher sind sie. Je weiter entfernt sie voneinander sind, desto verschiedener. Es gibt verschiede Metriken, mit denen man Distanzen messen kann. In der Regel wird eine der folgenden drei Metriken verwendet.</p>
<p>Die mit großem Abstand üblichste Metrik ist die <em>euklidische Abstand</em>, welche man sich geometrisch als die direkte Verbindung zwischen zwei Punkten vorstellen kann (<a class="reference internal" href="#fig-euclidean"><span class="std std-numref">Fig. 6.3</span></a>). Die euklidische Abstand wird über die euklidische Norm <span class="math notranslate nohighlight">\(||\cdot||_2\)</span> defniert als</p>
<div class="math notranslate nohighlight">
\[d(x,y) = ||y-x||_2 = \sqrt{(y_1-x_1)^2+...+(y_n-x_n)^2}.\]</div>
<div class="figure align-default" id="fig-euclidean">
<a class="reference internal image-reference" href="../_images/euclidean.png"><img alt="../_images/euclidean.png" src="../_images/euclidean.png" style="width: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.3 </span><span class="caption-text">Die Euklidische Distanz als direkte Verbindung zweier Punkte.</span><a class="headerlink" href="#fig-euclidean" title="Permalink to this image">¶</a></p>
</div>
<p>Ein weitere verbreitete Metrik ist die <em>Manhatten-Distanz</em>. Der Name dieser Metrik kommt daher, dass sie vom Prinzip her ähnlich ist zu der Distanz, die man in Manhatten gehen muss, wenn man sich in den Gitterartig organisierten Straßen von Manhatten bewegt. Da diese Straßen großteils Achsenparallel sind, kann man nicht diagonal gehen (<a class="reference internal" href="#fig-manhatten"><span class="std std-numref">Fig. 6.4</span></a>). Die Manhatten-Distanz wird über die Manhattennorm <span class="math notranslate nohighlight">\(||\cdot||_1\)</span> definiert als</p>
<div class="math notranslate nohighlight">
\[d(x,y) = ||y-x||_1 = |y_1-x_1|+...+|y_n-x_n|.\]</div>
<p>Der Einsatz der Manhatten-Distanz ist zum Beispiel bei sehr hochdimensionalen Daten sinnvoll, also Daten mit vielen Merkmalen. Hier gibt es die Tendendz bei der euklidischen Norm, dass alle Abstände ähnlich werden können. Außerdem ist die Manhattennorm robuster gegen Ausreißer bei einzelnen Merkmalen, da die Werte der Merkmale nicht vor dem Aufsummieren quadriert werden.</p>
<div class="figure align-default" id="fig-manhatten">
<a class="reference internal image-reference" href="../_images/manhatten.png"><img alt="../_images/manhatten.png" src="../_images/manhatten.png" style="width: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.4 </span><span class="caption-text">Die Manhatten-Distanz als Verbindung zweier Punkte ohne sich Diagonal bewegen zu können.</span><a class="headerlink" href="#fig-manhatten" title="Permalink to this image">¶</a></p>
</div>
<p>Der <em>Chebyshev-Abstand</em> ist auch als Maximumsmetrik bekannt und misst den maximalen Abstand in eine beliebige Richtung (<a class="reference internal" href="#fig-chebyshev"><span class="std std-numref">Fig. 6.5</span></a>). Der Chebyshev-Abstand wird über die Maximumsnorm <span class="math notranslate nohighlight">\(||\cdot||_\infty\)</span> definiert als</p>
<div class="math notranslate nohighlight">
\[d(x,y) = ||y-x||_\infty = \max_{i=1, ..., n} |y_i-x_i|.\]</div>
<p>Man sollte den Chebyshev-Abstand benutzen, wenn lediglich der maximale Abstand den man bei den Merkmalen beobachtet wichtig ist, nicht jedoch wie viele Merkmale sich unterscheiden.</p>
<div class="figure align-default" id="fig-chebyshev">
<a class="reference internal image-reference" href="../_images/chebyshev.png"><img alt="../_images/chebyshev.png" src="../_images/chebyshev.png" style="width: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.5 </span><span class="caption-text">Der Chebyshev-Abstand als maximale Distanz in eine beliebige Richtung. Dies entspricht der Anzahl der Schritte, die ein König beim Schach bräuchte, um ein Feld zu erreichen.</span><a class="headerlink" href="#fig-chebyshev" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="stadte-und-hauser">
<h3><span class="section-number">6.1.2. </span>Städte und Häuser<a class="headerlink" href="#stadte-und-hauser" title="Permalink to this headline">¶</a></h3>
<p>Wir benutzen eine Analogie um die Konzepte der Clusteralgorithmen zu erklären: Unsere Objekte sind Häuser und unsere Cluster sind Städte. Das schöne an dieser Analagie ist, das sie intutiv sinn macht. Man muss nicht wissen zu welcher Stadt ein Haus gehört, um zu erkennen welche weiteren Häuser zu dieser Stadt gehören. Mann muss auch nicht wissen wie viele Städte es gibt um das zu erkennen. Es reicht, wenn man die Position der Häuser betrachtet. Das ist genau das Problem, welches wir mit Clusteralgorithmen lösen wollen: Welche Objekte gehören zum gleichen Cluster und wie viele Cluster gibt es?</p>
</div>
</div>
<div class="section" id="k-means-algorithmus">
<h2><span class="section-number">6.2. </span><span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus<a class="headerlink" href="#k-means-algorithmus" title="Permalink to this headline">¶</a></h2>
<div class="section" id="grundidee">
<h3><span class="section-number">6.2.1. </span>Grundidee<a class="headerlink" href="#grundidee" title="Permalink to this headline">¶</a></h3>
<p>Eine Variante wie man Städte definieren könnte ist mit Hilfe der Rathäuser. Man könnte sagen das jedes Haus zu der Stadt gehört, deren Rathaus ihr man nächsten liegt. Diese Idee ist die Essenz des <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus. Man beschreibt Cluster durch ihr <em>Zentrum</em>. Dieses Zentrum nennt man auch <em>Centroid</em>, weshalb man es sich bei <span class="math notranslate nohighlight">\(k\)</span>-Means auch um einem Vertreter der <em>Centroidbasierten Clusteralgorithmen</em> handelt.</p>
<p>Wenn man wissen möchte zu welchen Cluster eine Instanz gehört, muss man lediglich rausfinden, welcher Centroid am nächsten ist. Das <span class="math notranslate nohighlight">\(k\)</span> im Algorithmusnamen steht für die Anzahl der Cluster. Formal haben wir also Centroids <span class="math notranslate nohighlight">\(C_1, ..., C_k\)</span>  und eine Metrik <span class="math notranslate nohighlight">\(d\)</span> um den Abstand zu messen. We können für jede Instanz <span class="math notranslate nohighlight">\(x \in \mathcal{F}\)</span> bestimmen zu welchem Cluster sie gehört, in dem wir das Minimum</p>
<div class="math notranslate nohighlight">
\[c(x) = argmin_{i=1,...,k} d(x, C_i)\]</div>
<p>bestimmen.</p>
<p>Das folgende Beispiel zeigt wie wir Daten in <span class="math notranslate nohighlight">\(k=4\)</span> Cluster unterteilt werden. Die Centroids sind als große graue Punkte dargestellt, die anderen Farben zeigen die Clusterzugehörigkeit.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we need matplotlib for plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="c1"># we use sklearn to generate data and for the clustering</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="c1"># generate sample data, the _ means that we ignore the second return value</span>
<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                  <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># we fit a k-means model with four clusters</span>
<span class="c1"># then we predict for each point to which cluster it belong</span>
<span class="c1"># finally, we determine the location of the cluster centers</span>
<span class="c1"># n_init, init and random_state should usually not be set. </span>
<span class="c1"># we only use these parameters to make sure we have results</span>
<span class="c1"># that we can re-use later to demonstrate how the algorithm works</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>

<span class="c1"># now we plot the data and the clustering results</span>
<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Daten (ohne Clustering)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis des $k$-Means Algorithmus, $k=4$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_1_0.png" src="../_images/kapitel_06_1_0.png" />
</div>
</div>
<p>Diese Darstellung der Cluster ist einfach, effektive und intuitiv.</p>
</div>
<div class="section" id="der-algorithmus">
<h3><span class="section-number">6.2.2. </span>Der Algorithmus<a class="headerlink" href="#der-algorithmus" title="Permalink to this headline">¶</a></h3>
<p>Ziel des Clusteralgorithmus ist es die Positionen der Centroids zu bestimmen. Wenn wir uns wieder eine Stadt vorstellen, könnte man Argumentieren, dass der Platz an dem ein Rathaus gebaut werden soll so gewählt werden sollte, dass der mittlere Abstand der Häuser zum Rathaus minimiert wird. Dann haben die Bewohner - im Mittel - den kürzesten Weg zum Rathaus. Jetzt stellen Sie sich vor das ein Neubaugebiet entsteht, außerdem werden an anderer Stelle ein paar alte Häuser abgerissen. Hierdurch ist der Ort des Rathauses nicht mehr optimal und idealerweise wird es so neu gebaut, dass der Abstand wieder minimiert wird. Hier ist die Analogie problematisch: Der Neubau eine Rathauses ist Teuer, einen Centroid zu verschieben jedoch sehr einfach. Diese Idee ist also genau das, was der Algorithmus macht.</p>
<p>Seien <span class="math notranslate nohighlight">\(X \subseteq \mathcal{F}\)</span> unsere Daten. Der <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus bestimmt die Cluster, bzw. die Centroids durch folgenden iterativen Algorithmus.</p>
<ol class="simple">
<li><p>Wähle die Startposition der Centroids <span class="math notranslate nohighlight">\(C_1, ..., C_k \in \mathcal{F}\)</span>.</p></li>
<li><p>Bestimme die Cluster <span class="math notranslate nohighlight">\(X_i = \{x \in X: c(x) = i\}\)</span> for <span class="math notranslate nohighlight">\(i=1,...,k\)</span>.</p></li>
<li><p>Verschiebe die Centroids, so dass das der neue Ort des Centroids dem arithmetischen Mittel der Instanzen des Clusters entspricht: <span class="math notranslate nohighlight">\(C_i = \frac{1}{|X_i|} \sum_{x \in X_i} x\)</span>.
4, Wiederhole die Schritte 2 und 3 bis</p>
<ul class="simple">
<li><p>Das Ergebnis konvergiert, also sich die Cluster <span class="math notranslate nohighlight">\(C_1, ..., C_k\)</span> sich nicht mehr verändern.</p></li>
<li><p>Eine vorher festgelegte höchstanzahl an Iterationen erreicht ist.</p></li>
</ul>
</li>
</ol>
<p>Auch wenn dieser Algorithmus relativ Abstrakt klingt, wird schnell klar wie er funktioniert, wenn wir uns die Schritte veranschaulichen.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># run the algorithm with k=1,...,4 iterations to demonstrate how it converges</span>
<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">max_iter</span><span class="o">=</span><span class="nb">iter</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">iter</span><span class="o">-</span><span class="mi">2</span><span class="p">)),</span> <span class="p">(</span><span class="nb">iter</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="k">2</span>]
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Iteration </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_3_0.png" src="../_images/kapitel_06_3_0.png" />
</div>
</div>
<p>Im Detail passiert das Folgende:</p>
<ul class="simple">
<li><p>Iteration 1: Zuerst sehen wir den Startzustand mit zufällig gewählten Centroids und daher auch einer relativ zufälligen Aufteilung der Punkte in Cluster. Ein Centroid is relativ weit weg von den Datenpunkten selbst und liegt zwischen den Datenpunken oben Rechts und unten Links. Dieses Centroid die gelb markierten Punkte oben rechts, sowie einen einzelnen Punkt in der Mitte zugewiesen. Ein weiteres Centroid liegt mittig in der unteren Hälfte der Daten und hat die blauen Datenpunkte zugewiesen bekommen. Außerdem gibt es noch zwei Centroids die recht nah beieinander in der linken unteren Hälfte der Daten liegen und diese Vertikal teilen: Das obere Centroid hat die grünen Punkte, das untere die lila Punkte. Die grünen Punkte reichen bis in die Mitte der Daten.</p></li>
<li><p>Iteration 2: Die Centroids werden entsprechend der ihnen zugewiesenen Punkte verschoben, so dass Sie basierend auf der Zuweisung aus Iteration 1 mittig liegen. Hierdurch bewegt sich das gelbe Centroid nach oben rechts in die Ecke, wo es auch die weiteren Iterationen stabil bleibt. Das blaue Centroid bewegt sich etwas nach unten, da der großteil der zugewiesenen Punkte am unteren Rand der Daten ist. Das grüne Centroid bewegt sich etwas nach rechts, da es einige Punkte in der mitte Daten hatte. Das lila Centroid bewegt sich zwar kaum, bekommt aber mehr Punkt am linken Rand zugewiesen, da sich das grüne Centroid nach rechts bewegt hat.</p></li>
<li><p>Iteration 3: Man sieht wie das Ergebniss anfängt zu konvergieren. Das blaue Centroid liegt jetzt bereits Central in der unteren mittleren Punktgruppe. Das grüne Centroid ist noch weiter nach rechts gewandert, so dass es bereits alle Datenpunkte in der mittleren Gruppe zugewiesen bekommt. Hierdurch hat das lila Centroid freies Spiel auf der linken Seite und bekommt die Punkte in dieser Gruppe.</p></li>
<li><p>Iteration 4: Nach nur vier Iterationen haben wir bereits ein sehr gutes Ergebnis und eine klare Trennung in vier Cluster.</p></li>
</ul>
</div>
<div class="section" id="bestimmen-von-k">
<h3><span class="section-number">6.2.3. </span>Bestimmen von <span class="math notranslate nohighlight">\(k\)</span><a class="headerlink" href="#bestimmen-von-k" title="Permalink to this headline">¶</a></h3>
<p>Im obigen Beispiel haben wir <span class="math notranslate nohighlight">\(k=4\)</span> benutzt, also nach vier Clustern gesucht. Wie man einen geeigneten Wert für <span class="math notranslate nohighlight">\(k\)</span> bestimmt haben wie noch offen gelassen. Bevor wir uns im Detail damit befassen, betrachten wir erst einmal wie sich das Ergebnis mit verschiedenen Werten für <span class="math notranslate nohighlight">\(k\)</span> ändert.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">k</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="nb">iter</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">iter</span><span class="o">-</span><span class="mi">3</span><span class="p">)),</span> <span class="p">(</span><span class="nb">iter</span><span class="p">)</span><span class="o">%</span><span class="k">2</span>]
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis für $k=</span><span class="si">%i</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_5_0.png" src="../_images/kapitel_06_5_0.png" />
</div>
</div>
<p>Mit <span class="math notranslate nohighlight">\(k=2\)</span> bekommen wir einen sehr großen Cluster und einen kleineren Cluster oben rechts. Mit <span class="math notranslate nohighlight">\(k=3\)</span> zerfällt das große Cluster in zwei Gruppen, so das wir einen Cluster oben rechts, einen Cluster in der Mitte und einen Cluster unten haben. Mit <span class="math notranslate nohighlight">\(k=4\)</span> zerfällt der mittlere Cluster in zwei Gruppen, so wie wir es im Beispiel auch hatten. Mit <span class="math notranslate nohighlight">\(k=5\)</span> zerfällt der Cluster in der Mitte in eine obere und eine untere Hälfte. Die Ergebnisse für <span class="math notranslate nohighlight">\(k=2, 3, 4\)</span> machen alle Sinn. Man kann Argumentieren das <span class="math notranslate nohighlight">\(k=2\)</span> ein gutes Ergebnis ist, da es eine große Lücke zwischen den Instanzen in der oberen Ecke und den anderen Daten gibt. Alle anderen Lücken sind kleiner. Man kann auch sagen, <span class="math notranslate nohighlight">\(k=3\)</span> gut ist, da es sich bei den Daten in der Mitte auch um ein langezogenes Cluster handeln könnte. Bei <span class="math notranslate nohighlight">\(k=4\)</span> kann man Argumentieren dass alle Cluster eine ähnliche Anzahl von Daten und eine ähnliche Form haben, auch wenn einige Cluster recht nah beieinander liegen. Für <span class="math notranslate nohighlight">\(k=5\)</span> findet man jedoch keine Begründung mehr, warum dieses Ergbnis besser sein könnte als die andern: die Trennung des mittleren Clusters in zwei Cluster macht einfach keinen Sinn.</p>
<p>Wichtig ist hier auch die Erkenntnis das verschiedene Clusterergebnisse und insbesondere auch Anzahlen von Clustern zu sinnvollen Ergebnissen führen können und oft nicht klar ist, welches Ergebnis jetzt eigentlich das beste hist. Welches Ergebnis man favorisiert hängt dann vom Anwendungsfall ab: Durch eine manuelle Analyse der Bedeutung der Cluster kann man verstehen, welche Gruppen diese Repräsentieren und damit die Ergebnisse einordnen und verstehen welches Ergebnis am besten passt.</p>
<p>Beim <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus gibt es verschiedene Aspekte, welche die Anzahl die Auswahl von <span class="math notranslate nohighlight">\(k\)</span> beeinflussen.</p>
<ul class="simple">
<li><p>Das Domänenwissen über die Daten und den Anwendungsfall. Mit Hilfe des Domänenwissens kann man bewerten ob die Daten innerhalb eines Clusters sich wirklich ähnlich sind oder auch ob sich verschiedene Cluster eventuell ähnlich sind. Wenn man bemerkt das die Daten innerhalb eines Clusters verschieden sind, ist das ein indikator das man <span class="math notranslate nohighlight">\(k\)</span> erhöhen sollte. Wenn man bemerkt das ähnliche Objekte in verschiedenen Clustern liegen, kann es helfen <span class="math notranslate nohighlight">\(k\)</span> zu reduzieren. Je nach Anwendungsfall kann es auch möglich sein, dass eine bestimmte Anzahl von Clustern gesucht wurd. Wenn man zum Beispiel eine binäre Trennung der Daten in zwei Gruppen erreichen möchte, ist durch den Anwendungsfall ein <span class="math notranslate nohighlight">\(k\)</span> von zwei bereits vorgeben.</p></li>
<li><p>Visualisierung sind ein mächtiges Werkzeug um zu erkennen wie gut die Cluster die Daten gruppieren, wie die Daten innerhalb eines Clusters verteilt sind, und ob es klar erkennbare Lücken zwischen den Clustern gibt. Visualisierung ist zum Beispiel auch das Werkzeug, mit dem wir im obigen Beispiel verschiedene Werte von <span class="math notranslate nohighlight">\(k\)</span> beurteilt haben.</p></li>
<li><p>Es gibt auch einen analytischen Ansatz um <span class="math notranslate nohighlight">\(k\)</span> zu bestimmen basierend auf der <em>Within-Sum-of-Squares</em> (WSS). Um zu verstehen was WSS ist, müssen wir uns noch einmal an die Grundlagen des Algorithmus erinnern. Die Centroids werden in jeder Iteration so aktualisiert, dass sie das arithmetische Mittel der Daten in dem Cluster sind. Das bedeutet automatisch auch, dass die <em>Varianz</em> innerhalb eines Clusters vom Centroid minimiert wird.  Die Varianz ist das Quadrat der Standardabweichung und kann berechnet werden in dem man die Summe der quadratischen Distanzen vom arithmetischen Mittel bildet. Da das Centroid auf das arithmetrische Mittel gesetzt wird, minimiert die Aktualisierung der Position des Centroids also die <em>Summe der Quadrate</em> (Sum-of-Squares) <em>innerhalb</em> (Within) eines clusters. Entsprechend ist WSS nichts anderes als ein Maß dafür, wie gut es gelingt die Varianz innerhalb eines Clusters zu miniminieren und definiert als</p></li>
</ul>
<div class="math notranslate nohighlight">
\[WSS = \sum_{i=1}^k\sum_{x \in X_i} d(x, C_i)^2.\]</div>
<p>Wir haben oben bereits basierend auf der Visualisierung diskutiert, wie gut verschiedene Werte von <span class="math notranslate nohighlight">\(k\)</span> sind. Da es für zufällig generierte Daten kein Domänenwissen gibt, können wir dies auch nicht nutzen um die Cluster zu bewerten. Wir können verschieden Werte von <span class="math notranslate nohighlight">\(k\)</span> jedoch mit Hilfe der WSS bewerten. Hierzu zeichnet man am besten ein einfaches Liniendiagram, was die WSS für verschiedene Werte von <span class="math notranslate nohighlight">\(k\)</span> vergleicht.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># sklearn class the WSS inertia</span>
<span class="n">inertia</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">k</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="nb">iter</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
    <span class="n">inertia</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">inertia_</span> <span class="p">)</span> <span class="c1"># we already store this for later, inertia=WSS</span>

<span class="n">fix</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Entwicklung von WSS&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">inertia</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;WSS&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$k$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_7_0.png" src="../_images/kapitel_06_7_0.png" />
</div>
</div>
<p>Wie man sieht gibt es einen starken Abfall der WSS von 2 nach 3 und von 3 nach 4. Ab vier bleibt die WSS nahezu konstant. Außerdem sieht man das sicht die Steigung bei den Werten 2, 3, und 4 stark ändert und die Kurve langsam abflacht. Diese Änderung der Steigung nennt man auch <em>Ellenbogen</em>, bzw. <em>Elbows</em>, da sie ein wenig dem Ellenbogen eines gebeugten armes ähneln. Wenn wir die WSS nutzen um <span class="math notranslate nohighlight">\(k\)</span> auszuwählen finden wir geeignete Werte an diesen Ellenbogen. Der Grund ist, dass wenn sich die Kurve nicht abflacht sonder weiter gleich steil abfällt, die Verbesserung auch gleich bleibt. Es gibt also keinen guten Grund, einen <span class="math notranslate nohighlight">\(k\)</span> zu wählen, wo es keinen Elbow gibt, da <span class="math notranslate nohighlight">\(k+1\)</span> genauso so viel besser ist als <span class="math notranslate nohighlight">\(k\)</span> wie <span class="math notranslate nohighlight">\(k\)</span> als <span class="math notranslate nohighlight">\(k-1\)</span>.</p>
<p>Die WSS-Kurve fällt monoton ab. Das bedeutet das <span class="math notranslate nohighlight">\(WSS(k+1) \leq WSS(K)\)</span> für alle <span class="math notranslate nohighlight">\(k&gt;1\)</span>. Daher kann man <span class="math notranslate nohighlight">\(k\)</span> auch leider nicht als das Minimum der WSS auswählen, da dies einfach bei <span class="math notranslate nohighlight">\(k=|X|\)</span> erreicht wird. Der Grund dafür ist, dass die Varianz sich mit reduziert, wenn wir mehr Cluster hinzufügen. Das Minimum wird erreicht, wenn <span class="math notranslate nohighlight">\(d(x, C_{c(x)})=0\)</span> für alle <span class="math notranslate nohighlight">\(x \in X\)</span>. Unter der Annahme das es keine Datenpunkte mit identischen Werten gibt, braucht man also |X| Cluster um das Minimum zu erreichen. Wie man in der obigen Grafik aber sieht, wird der Abfall der WSS schon deutlich früher sehr klein, im Beispiel bei <span class="math notranslate nohighlight">\(k=4\)</span>. Entsprechend kann man <span class="math notranslate nohighlight">\(k\)</span> gut durch eine visuelle Analyse der WSS auswählen, nicht jedoch durch einen automatismus der die WSS minimiert.</p>
</div>
<div class="section" id="probleme-des-k-means-algorithmus">
<h3><span class="section-number">6.2.4. </span>Probleme des <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus<a class="headerlink" href="#probleme-des-k-means-algorithmus" title="Permalink to this headline">¶</a></h3>
<p>Obwohl das Konzept des <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus relativ einfach ist, bekommt man häufig gute Ergebnisse. Es gibt jedoch einige Probleme mit <span class="math notranslate nohighlight">\(k\)</span>-Means, die man kennen sollte.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k\)</span>-Means ist sensitiv bezüglich der Auswahl der Startcentroids. Je nachdem wie die Centroids gewählt werden, kann es andere Clusterergebnisse geben. Man sagt dann, dass das Ergebnis nicht stabil ist. Entsprechend sollte man <span class="math notranslate nohighlight">\(k\)</span>-Means mehrfach mit verschiedenen Startcentroids ausführen. Wenn das Ergebnis instabil ist, ist das ein Indikator das die Anzahl der Cluster eventuell anders gewählt werden sollte.</p></li>
<li><p>Ein schlechter Wert von <span class="math notranslate nohighlight">\(k\)</span> liefert möglicherweise schlechte Ergebnisse. Da man <span class="math notranslate nohighlight">\(k\)</span> manuell auswählen muss, folgt das man die nötige Erfahrung mit dem Algorithmus haben sollte, damit man <span class="math notranslate nohighlight">\(k\)</span> gut auswählen kann um schlechte Ergebnisse zu vermeiden.</p></li>
<li><p>All Merkmale sollten einen ähnlichen Wertebereich haben, im Idealfall sogar den selben. Anderfalls können die Unterschiede im Wertebereich zu einer ungewollten Gewichtung der Merkmale führen. Betrachten wir hierfür ein Beispiel mit zwei Merkmalen: Alter in Jahren und das Bruttojahreseinkommen in Euro. Das Alter ist (grob) zwischen 0 und 100, das Einkommen in Euro ist selbst bei Minijobs schon bei 12*400=4.800 Euro, kann aber auch im sechsstelligen Bereich liegen. Das heißt, dass wenn man die Abstände zwischen zwei Instanzen berechnet, das Alter irrevant ist. Alles was zählt ist das Einkommen, da hier die Abstände deutlich größer sind. Der höhere Wertebereich sorgt also dafür, dass das Einkommen stärker gewichtet wird.</p></li>
<li><p>Weil die Cluster basierend aud der Distanz zugewiesen werden, tendieren die Cluster dazu rund zu sein. Wenn die Cluster nicht rund sind, kann man sie häufig auch nicht gut durch Centroids beschreiben. Ein Beispiel dafür sind die zwei Halbmonde, die wir unten sehen. Man kann klar erkennen, dass es sich um zwei Halbkreise handelt, jeder sollte ein Cluster sein. <span class="math notranslate nohighlight">\(k\)</span>-Means kann diese Cluster aufgrund ihrer Form nicht finden.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="c1"># this generates our halfmoon data</span>
<span class="n">X</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis des $k$-Means Algorithmus für die Halbmonde&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_9_0.png" src="../_images/kapitel_06_9_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="em-clustering">
<h2><span class="section-number">6.3. </span>EM Clustering<a class="headerlink" href="#em-clustering" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3><span class="section-number">6.3.1. </span>Grundidee<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Wir könnten uns Städte auch als Zufallsvariablen vorstellen und die Häuser wären dann Instanzen dieser Zufallsvariablen. Die Zuweisung von Häusern zu Städten würde also von Wahrscheinlichkeitsverteilungen abhängen. Dies ist die Idee hinter einer Verallgemeinerung des <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus die wir jetzt betrachten wollen. Unsere Rathäuser sind immernoch das Zentrum der Stadt. Aber wir berücksichtigen, das häuser die sehr nah am Rathaus mit einer höheren Wahrscheinlichkeit zur Stadt gehören, also solche die weiter weg liegen. Wenn wir dies weiter denken, kann man dies zu einer Eigenschaft der Städte werden lassen: Es gibt Städte, die sich nicht sehr stark räumlich ausbreiten und andere Städte, die eine große Fläche einnehmen. Wir wollen also die <em>Verteilung</em> der Häuser berücksichtigen. Daher nennt man diese Art zu Clustern auch <em>Verteilungsbasiertes Clustern</em> (engl. <em>distribution-based clustering</em>).</p>
<p>Formal haben wir eine Anzahl von Clustern <span class="math notranslate nohighlight">\(k\)</span> die durch Zufallsvariablen <span class="math notranslate nohighlight">\(C_1, ..., C_k\)</span> beschrieben werden. Wir können die Wahrscheinlichkeit, dass eine Instanz als <span class="math notranslate nohighlight">\(P(C_i = x), i=1, ..., k\)</span> berechnen. Jede Instanz hat also eine gewisse Wahrscheinlichkeit, zu jedem Cluster zu gehören. Da man aber trotzdem für jede Instanz wissen möchte, zu welchem Cluster sie jetzt gehört, kann man das <em>wahrscheinlichste</em> Cluster berechnen, also das Cluster <span class="math notranslate nohighlight">\(C_i\)</span> welches <span class="math notranslate nohighlight">\(P(C_i = x)\)</span> maximiert. Entsprechend ist die Zuweisung der Instanzen zu Clustern definiert als</p>
<div class="math notranslate nohighlight">
\[c(x) = \max_{i=1,..., k} P(C_i = x).\]</div>
<p>Auch wenn man das Konzept des EM Algorithm mit beliebigen Verteilungen benutzen kann, benutzt man in der Regel Normalverteilte Zufallsvariablen um die Cluster zu beschreiben. Man spricht daher auch vom <em>Gaussian Mixture Model</em>. Bitte beachten Sie, dass diese Normalverteilungen <em>multivariat</em> sind, also nicht nur eine einzelne Dimension beschreiben. Die Normalverteilung hat so viele Dimensionen, wie es Merkmale gibt. Eine univariate Normalverteilung kann man durch das arithmetische Mittel und die Standardabweichung beschreiben. Eine multivariate Normalverteilung wird durch einen Vektor von Mittelwerten und eine <em>Kovarianzmatrix</em> beschrieben. Die Kovarianzmatrix beschreibt die Beziehung zwischen den Varianzen der einzelnen Dimensionen. Vereinfacht gesagt, kann man sich die Kovarianz als die ausdehnung der Glockenform der Normalverteilung in die verschiedenen Richtungen vorstellen. Mathematisch betrachtet handelt es sich bei der Kovarianz um eine Ellipse: Die form der Ellipse bestimmt die erwarte Abweichung der Daten in jede Richtung. Eine wichtige Eigenschaft der Kovarianzmatrix, die wir später noch brauchen, ist dass es sich um quadratische Matrix handelt, bei der das obere rechte und das untere linke Dreieck symmetrisch sind. Es gibt also <span class="math notranslate nohighlight">\(\frac{d\cdot(d+1)}{2}\)</span> freie Parameter in der Kovarianzmatrix. Wenn wir also <span class="math notranslate nohighlight">\(d\)</span> Merkmale haben, gibt es <span class="math notranslate nohighlight">\(d+\frac{d\cdot(d+1)}{2}\)</span> Parameter, die wir brauchen um eine entsprechende multivariate Normalverteilung zu beschreiben: <span class="math notranslate nohighlight">\(d\)</span> Mittelwerte und <span class="math notranslate nohighlight">\(\frac{d\cdot(d+1)}{2}\)</span> Parameter für die <span class="math notranslate nohighlight">\(d \times d\)</span> Kovarianzmatrix. Insgesamt haben wir also <span class="math notranslate nohighlight">\(k \cdot (d+\frac{d\cdot(d+1)}{2})\)</span> Parameter, wenn wir <span class="math notranslate nohighlight">\(k\)</span> Cluster beschreiben wollen.</p>
<p>Wir betrachten jetzt die gleichen Daten wie eben, nur das wir uns das Ergebnis des EM Clustering mit <span class="math notranslate nohighlight">\(k=4\)</span> Normalverteilung angucken. Die Mittelwerte sind als große graue Punkte markiert, die Kovarianzen als Ellipsen in der Farbe der jeweiligen Cluster.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>

<span class="c1"># we silence some annyoing warnings</span>
<span class="c1"># if something is not working properly, remove this part and restart the kernel</span>
<span class="k">def</span> <span class="nf">warn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">pass</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">warn</span> <span class="o">=</span> <span class="n">warn</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;indigo&#39;</span><span class="p">,</span> <span class="s1">&#39;navy&#39;</span><span class="p">,</span> <span class="s1">&#39;mediumseagreen&#39;</span><span class="p">,</span> <span class="s1">&#39;gold&#39;</span><span class="p">]</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
                       <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="n">em</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">em</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_em</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">centers</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">means_</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Daten (ohne Clustering)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis des EM Clustering&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_em</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
<span class="c1"># this code is for plotting the elipses</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colors</span><span class="p">):</span>
    <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">covariances_</span><span class="p">[</span><span class="n">n</span><span class="p">][:</span><span class="mi">2</span><span class="p">,:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">angle</span> <span class="o">=</span> <span class="mi">180</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>  <span class="c1"># convert to degrees</span>
    <span class="n">v</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">ell</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                              <span class="mi">180</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
    <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_11_0.png" src="../_images/kapitel_06_11_0.png" />
</div>
</div>
<p>Auch wenn die Beschreibung der Cluster durch Zufallsvariablen komplexer ist als durch Centroids, sieht man in der obigen Grafik den Vorteil: Die Kovarianzen erlauben es, die Verteilung der Daten in die Clusterbeschreibung mit aufzunehmen. Wir haben also mehr Informationen über die Daten als Teil der Clusterbeschreibung.</p>
</div>
<div class="section" id="id2">
<h3><span class="section-number">6.3.2. </span>Der Algorithmus<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Man nennt diese Art zu Clustern EM Clusterin weil sie auf dem <em>Expectation-Maximization (EM) Algorithmus</em> basiert. Der Algorithmus ist im allgemeinen Ähnlich zu <span class="math notranslate nohighlight">\(k\)</span>-Means: Wir starten mit einer zufälligen Initiailisierung der Zufallsvariablen und verbessern die Beschreibung der Daten durch die Zufallsvariablen iterativ. Beim EM Algorithmus müssen also für jedes Cluster die Mittelwerte und die Kovarianzmatrizen zufällig initialisiert und stückweise verbessert werden. Die Verbesserung basiert auf der Wahrscheinlichkeit (engl. <em>likelihood</em>), das die Daten Instanzen dieser Zufallsvariablen sein könnten. Im folgenden Betrachten wir nicht die vollständige mathematische Beschreibung des EM Algorithmus, sondern nur eine vereinfachte Version, in der wir nur die Mittelwerte aktualisieren und die Kovarianzen ignorieren.</p>
<ol class="simple">
<li><p>Wähle zufällig <span class="math notranslate nohighlight">\(k\)</span> Normalverteilungen <span class="math notranslate nohighlight">\(C_1 \sim (\mu_1, \Sigma_1), ..., C_k \sim (\mu_k, \Sigma_k)\)</span>, wobei <span class="math notranslate nohighlight">\(\mu_i \in \mathcal{F}\)</span> die Mittelwerte und <span class="math notranslate nohighlight">\(\Sigma_i \in \mathcal{F}\times\mathcal{F}\)</span> die Kovarianzmatrizen sind.</p></li>
<li><p>Expectation Schritt: Bestimme Gewichte $<span class="math notranslate nohighlight">\(w_i(x) = \frac{p(x|\mu_i, \Sigma_i)}{\sum_{j=1}^k p(x|\mu_j, \Sigma_j)}\)</span><span class="math notranslate nohighlight">\( für alle Instanzen \)</span>x \in X<span class="math notranslate nohighlight">\( und Cluster \)</span>i=1, …, k$.</p></li>
<li><p>Maximization Schritt: Aktualisiere die Mittelwerte, so dass $<span class="math notranslate nohighlight">\(\mu_i = \frac{1}{|X|}\sum_{x \in X} w_i(x)\cdot x\)</span><span class="math notranslate nohighlight">\( für jedes Cluster \)</span>i=1, …, k$.
4, Wiederhole die Schritte 2 und 3 bis</p>
<ul class="simple">
<li><p>Das Ergebnis konvergiert, also sich die Cluster <span class="math notranslate nohighlight">\(C_1, ..., C_k\)</span> sich nicht mehr verändern.</p></li>
<li><p>Eine vorher festgelegte höchstanzahl an Iterationen erreicht ist.</p></li>
</ul>
</li>
</ol>
<p>Der größte Unterschied zwischen dem EM Algorithmus und dem <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus sind die Gewichte. Diese Gewichte geben die Wahrscheinlichkeit an, dass eine Instanz <span class="math notranslate nohighlight">\(x \in X\)</span> zu einem bestimmten Cluster gehört. Der Wert <span class="math notranslate nohighlight">\(w_i(x)=0,9\)</span> bedeutet also, dass die Instanz <span class="math notranslate nohighlight">\(x\)</span> mit 90% Wahrscheinlichkeit zu Cluster <span class="math notranslate nohighlight">\(i\)</span> gehört. Die Zuweisung der Instanzen zu den Clustern erfolgt nach der oben beschriebenen Regel: Jede Instanz wird dem Cluster zugewiesen, welches die Wahrscheinlich maximiert, also</p>
<div class="math notranslate nohighlight">
\[c(x) = argmax_{i=1, ..., k} w_i(x).\]</div>
<p>Durch die Berechnung der Wahrscheinlich für jedes Cluster kann man beim EM Clustern die Unsicherheit der Ergebnisse gut nachvollziehen, zum Beispiel ob ein Punkt für verschiedene Cluster eine relativ hohe Wahrscheinlichkeit hat. Diese Art zu Clustern nennt man auch <em>Soft Clustering</em>, da die Datenpunkte nicht “hart” genau einem Cluster zugewiesen werden, sondern es stattdessen eine “weiche” Zuweisung über Wahrscheinlichkeiten gibt.</p>
<p>Um ein tieferes Verständnis des EM Algorithmus zu bekommen, betrachten wir jetzt am Beispiel wie die Cluster aktualisiert werden.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">cur_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="o">*</span><span class="mi">20</span><span class="o">+</span><span class="mi">1</span>
    <span class="n">em</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">cur_iter</span><span class="p">,</span> <span class="n">init_params</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">)</span>
    <span class="n">em</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_em</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">means_</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">iter</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="p">(</span><span class="nb">iter</span><span class="p">)</span><span class="o">%</span><span class="k">2</span>]
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Iteration </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">cur_iter</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_em</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colors</span><span class="p">):</span>
        <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">covariances_</span><span class="p">[</span><span class="n">n</span><span class="p">][:</span><span class="mi">2</span><span class="p">,:</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="mi">180</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>  <span class="c1"># convert to degrees</span>
        <span class="n">v</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">ell</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                  <span class="mi">180</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_13_0.png" src="../_images/kapitel_06_13_0.png" />
</div>
</div>
<p>Im Detail passiert das Folgende:</p>
<ul class="simple">
<li><p>Iteration 1: Wir starten mit vier Normalverteilungen die nahezu identisch sind. Der Mittelwert liegt in der Mitte der Daten und die Kovarianzen sind in der diagonalen verlängerte Elippsen, die sich leicht eineinander unterscheiden. Man sieht zum Beispiel dass die grüne Ellipse etwas weiter nach links unten geht und die gelbe etwas weiter nach rechts oben. Dies spiegelt sich auch schon in der Zuweisung der Punkte zu den Clustern wieder: links unten ist grün, rechts oben ist gelb, die andern Farben sind kaum zu erkennen aufgrund der Überschneidung.</p></li>
<li><p>Iteration 21: Nach 21 Iterationen sieht man schon wie sich die Normalverteilungen voneinander Trennen. Die gelbe Verteilung wird nach rechts oben gezogen, hat aber immernoch eine sehr große Kovarianzellipse. Die grüne wurde zwischen die Instanzen links und unten gezogen. Die blaue liegt in der Mitte der Daten, hat jedoch jetzt eine deutlich kleinere Kovarianz. Die lila Verteilung hat sich kaum bewegt und die Kovarianz ist immernoch sehr hoch.</p></li>
<li><p>Iteration 41: Man sieht wie die Verteilungen langsam gegen ihre Ziele Konvergieren. Die gelbe Verteilung liegt jetzt fest rechts oben in der Ecke und hat sich dort an die Daten angepasst. Die Kovarianz der blauen Verteilung ist auch weiter geschrumpft, so dass diese jetzt nur noch die Punkte in der Mitte überdeckt. Hierdurch wurde die bisher noch “unentschlossene” lila Verteilung nach links gedrückt, wo sie dank einer relativ kleinen Kovarianz anfängt, die grüne Verteilung zu dominieren. Als Konsequenz hat die grüne Verteilung angefangen, sich zu den Datenpunkten am unteren Rand zu bewegen.</p></li>
<li><p>Iteration 61: Die Cluster sind konvergiert: Die lila Verteilung ist jetzt fest bei den Datenpunkten links und die grüne ist vollständig nach unten gewandert.</p></li>
</ul>
<p>Im Beispiel erkennt man zwei wichtige Eigenschaften des EM Clustern. Zum Einen konverviert der Algorithmus deutlich langsamer als <span class="math notranslate nohighlight">\(k\)</span>-Means. Dies liegt daran, das mehr Parameter optimiert werden müssen, also nicht nur die Mittelwerte, sondern auch die Kovarianzen. Außerdem bremst das Soft Clustering die Konvergenz: Hierdurch bewegen sich die Mittelwerte nur langsam, da sie auch von anderen Instanzen, die eigentlich schon anderen Clustern zugeordnet werden, noch angezogen werden, wenn auch schwächer. Zum anderen sieht man, dass die Cluster möglicherweise keine zusammenhängende Region beschreiben. In Iteration 41 gibt es einige grüne Instanzen am linken Rand, der rest befindet sich unten. Dazwischen befinden sich Instanzen, die dem lila Cluster zugeordnet sind. Es gibt also zwei getrennte Regionen im grünen Cluster. Diese Trennung wird durch die Form der Kovarianzmatrizen ermöglicht. Da die Kovarianz der grünen Verteilung größer ist, haben Punkte die weit weg vom Mittel liegen noch eine relativ hohe Wahrscheinlichkeit. Die Kovarianz der lila Verteilung ist deutlich kleiner, so dass die Wahrscheinlichkeit mit der Distanz stärker abnimmt. Derart getrennte Regionen sind ungewöhnlich und mit den meisten Clusteralgorithmen gar nicht möglich. Beim <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus ist dies zum Beispiel ausgeschlossen, da sich die Distanz für jeden Centroid in alle Richtungen gleich verändert.</p>
<blockquote>
<div><p><strong>Bemerkung:</strong></p>
<p>Die Gemeinsamkeiten des <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus und des EM Clustering gehen noch über die obige Beschreibung hinaus. Genau genommen ist <span class="math notranslate nohighlight">\(k\)</span>-Means nur ein Spezialfall des EM Clustering in dem Voronoi Zellen <a class="footnote-reference brackets" href="#voronoi" id="id3">1</a> zur Modellierung der Wahrscheinlichkeiten genutzt werden.</p>
</div></blockquote>
</div>
<div class="section" id="id4">
<h3><span class="section-number">6.3.3. </span>Bestimmen von <span class="math notranslate nohighlight">\(k\)</span><a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>Genau vom beim <span class="math notranslate nohighlight">\(k\)</span>-Means Clustern, muss auch beim EM Clustern die Anzahl der Cluster durch den Benutzer vorgegeben werden. Der Ansatz ist ähnlich: Domänenwissen und Visualisierungen sind auch hier gute Mittel um <span class="math notranslate nohighlight">\(k\)</span> zu bestimmen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">k</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
    <span class="n">em</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="nb">iter</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">em</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_em</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">means_</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">iter</span><span class="o">-</span><span class="mi">2</span><span class="p">)),</span> <span class="p">(</span><span class="nb">iter</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="k">2</span>]
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis für $k=</span><span class="si">%i</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
    <span class="c1"># we use a bit of an indirect way to define the colors, because they</span>
    <span class="c1"># otherwise sometimes did not match with the ellipses</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="p">[</span><span class="n">colors</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_em</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">iter</span><span class="p">]):</span>
        <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">covariances_</span><span class="p">[</span><span class="n">n</span><span class="p">][:</span><span class="mi">2</span><span class="p">,:</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="mi">180</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>  <span class="c1"># convert to degrees</span>
        <span class="n">v</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">ell</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                  <span class="mi">180</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_15_0.png" src="../_images/kapitel_06_15_0.png" />
</div>
</div>
<p>Man kann gut erkennen, das eine einzlene Normalverteilung nicht ausreicht, um die Daten zu Beschreiben. Die Ergebnisse für <span class="math notranslate nohighlight">\(k=2, 3, 4\)</span> sind besser. Man erkennt jedoch, dass nur für <span class="math notranslate nohighlight">\(k=4\)</span> die Verteilung der Punkte innerhalb der Cluster wirklich der Form der Kovarianzellipsen entspricht, was dafür spricht, dass dies das beste Ergebnis ist.</p>
<p>Es gibt auch einen analytischen Ansatz für die Bestimmung von <span class="math notranslate nohighlight">\(k\)</span> basierend auf dem <em>Bayesian Information Criterion</em> (BIC). Ähnlich bei WSS, ist BIC auch ein Maß dafür, wie gut der Algorithmus sein Optimierungsziel erreicht. Der EM Algorithmus probiert die Wahrscheinlichkeit des Ergebnisses zu maximieren. Formal macht man das mit der <em>Likelihood-Funktion</em> <span class="math notranslate nohighlight">\(\hat{L}(C_1, ..., C_k; X)\)</span>, die wir bei der Beschreibung des EM Algorithmus ausgespart haben und auch hier nicht im Detail definieren wollen. Der Wert der Likelihood-Funktion wird höher, wenn die Cluster eine bessere Erklärung für die Daten sind. Ähnlich zum monotonen Abfall der WSS, ist auch die Likelihood-Funktion in der Regel monoton: Mit mehr Clustern bekommt man eine höheren Wert. BIC wird jedoch nicht nur über die Likelihood-Funktion berechnet. Zusätzlich wird die Komplexität des berechneten Models für die Cluster berücksichtigt. Zu Beginn des Kapitels haben wir bereits gezeigt, dass man <span class="math notranslate nohighlight">\(k' = k \cdot (d+\frac{d\cdot(d+1)}{2})\)</span> Parameter braucht um <span class="math notranslate nohighlight">\(k\)</span> Cluster für <span class="math notranslate nohighlight">\(d\)</span> Merkmale durch multivariate Normalverteilungen zu beschreiben. Occam’s Razor beagt, dass die einfachste Erklärung meistens die beste ist. Hier bedeutet das, dass wenn man eine ähnlichen Wert der Likelihood-Funktion mit weniger Parametern erreichen kann, sollte man dieses kleinere Modell verwenden. Komplexere Beschreibungen, also mehr Parameter, die aber die Likelihood-Funktion nur noch wenig verbessern, könnten hingegen Overfitting sein. Basierend auf diesen Konzepten ist BIC definiert als</p>
<div class="math notranslate nohighlight">
\[BIC = \log(|X|)\cdot k' - 2\cdot \log(\hat{L}(C_1, ..., C_k; X)).\]</div>
<p>BIC wird also größer mit mehr Parameter mehr Trainingsdaten und fällt hab, wenn sich die Likelihood-Funktion verbessert. Daher sollte man probieren den BIC zu minimieren. Im Vergleich zu WSS ist BIC nicht monoton, sondern es gibt einen Minimum. Dies ist möglich, das BIC die Modelkomplexität bestraft. Ab einem gewissen Punkt ist die Strafe für ein komplexeres Modell größer als die Verbesserung der Likelihood-Funktion und das Minimum ist erreicht. Hieraus folgt, dass es basierend auf BIC sogar eine <em>optimale</em> Anzahl von Clustern gibt, die man automatisch als den Wert von <span class="math notranslate nohighlight">\(k\)</span> bei dem BIC minimiert wird, bestimmen kann. Auch das können wir uns am Beispiel angucken.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>
<span class="n">bic</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ks</span><span class="p">:</span>
    <span class="n">em</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">em</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_em</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">means_</span>
    <span class="n">bic</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">bic</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Entwicklung des BIC&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">bic</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;BIC&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$k$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ks</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_17_0.png" src="../_images/kapitel_06_17_0.png" />
</div>
</div>
<p>Das Minimum des BIC wird für <span class="math notranslate nohighlight">\(k=4\)</span> erreicht. Dies bedeutet das vier Cluster der optimale Trade-Off zwischen der Anzahl der Modellparameter und der Qualtität der Cluster. Bitte beachten Sie, dass man die absoluten Werte des BIC ignorieren sollte (im Bild also die Werte der y-Achse). Ob BIC positiv oder negativ ist oder die Werte in der Größenordnung <span class="math notranslate nohighlight">\(10^2\)</span> oder <span class="math notranslate nohighlight">\(10^5\)</span> liegen, hängt eher von den Daten ab, als von der Qualität der Ergebnisse. Entsprechend kann man sogar die Skala der y-Achse ausblenden, ohne wertvolle Informationen zu verlieren.</p>
<blockquote>
<div><p><strong>Bermerkung:</strong></p>
<p>BIC ist aus dem <em>Akitake Information Criterion</em> (AIC) abgeleitet, welches als</p>
<div class="math notranslate nohighlight">
\[AIC = 2\cdot k' - 2\cdot \log(\hat{L}(C_1, ..., C_k; X)).\]</div>
<p>definiert ist. Das AIC ist eine Folgerung aus der KUllback-Leibler Divergenz, eine Informationstheoretisch Maß für den Unterschied zwischen zwei Zufallsvariablen. Dies ist auch der Grund, warum der Logarithmus in der AIC Definition und auch in der BIC Definition verwendet wird. Der einzige Unterschied zwischen AIC und BIC ist der erste Term: Während das AIC einen festen Faktor von 2 hat, wird beim BIC die Modellkomplexität noch mit dem logarithmus Menge der Trainingsdaten gewichtet.</p>
</div></blockquote>
</div>
<div class="section" id="probleme-des-em-clusterings">
<h3><span class="section-number">6.3.4. </span>Probleme des EM Clusterings<a class="headerlink" href="#probleme-des-em-clusterings" title="Permalink to this headline">¶</a></h3>
<p>EM Clustering löst zwei Problem des <span class="math notranslate nohighlight">\(k\)</span>-Means Clustering: Unterschiedliche Wertebereiche der Merkmale können durch die Kovarianzmatrizen berücksichtigt werden. Hierdurch lassen sich auch ellipsoide Cluster beschreiben, was mächtiger als die eher Runde form beim <span class="math notranslate nohighlight">\(k\)</span>-Means Clustering. Die anderen Probleme des <span class="math notranslate nohighlight">\(k\)</span>-Means Clustering hat man auch weiterhin mit dem EM Clustering.</p>
<ul class="simple">
<li><p>EM Clustering ist sensitiv bezüglich der Startwerte. Eine schlechte Startposition kann dazu führen, dass der Algorithmus nur sehr langsam konvergiert oder sogar ein schlechtes Ergebnis liefert. Ein guter Ansatz das EM Clustering zu initialisieren ist es, die Centroids vom <span class="math notranslate nohighlight">\(k\)</span>-Means Clustering als Startwerte für die Mittelwerte der Cluster zu nutzen.</p></li>
<li><p>Die Wahl von <span class="math notranslate nohighlight">\(k\)</span> kann zu schlechten Ergebnisse führen. Auch wenn das BIC ein guter Ansatz für die Auswahl von <span class="math notranslate nohighlight">\(k\)</span> ist, heißt es nicht zwingend, dass man das man ein gutes Ergebnis findet. Es könnte zum Beispiel möglich sein, dass man sehr viele Cluster bräuchte, um das Optimum des BIC zu erreichen. In diesem Fall sollte man BIC ähnlich wie WSS benutzen, und stattdessen nach Ellenbogen bei einer kleineren Anzahl von Clustern suchen.</p></li>
<li><p>EM Clustering ist nur für Cluster die sich durch Normalverteilungen beschreiben lassen gut geeignet, was eine etwa ellipsoide Form der Cluster vorraussetzt. Die Halbmonde könnten auch vom EM Algorithmus nicht richtig gruppiert werden.</p></li>
</ul>
</div>
</div>
<div class="section" id="dbscan">
<h2><span class="section-number">6.4. </span>DBSCAN<a class="headerlink" href="#dbscan" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="id5">
<h2><span class="section-number">6.5. </span>Grundidee<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>Bisher haben wir immer die Rathäuser als Referenzpunkt für die Definition von Städten benutzt. Wenn man aus dem Fenster guckt, erkennt man jedoch auch ohne zu wissen wo das Rathaus liegt, das die Nachbarhäuser zur gleichen Stadt gehören. Das gleiche gilt für die Nachbarn ihrer Nachbarn. Und so weiter und so fort. Im Endeffekt sind alle ihre direkten oder indirekten Nachbarn also Teil der gleichen Stadt. Dies ist die Grundidee des <em>dichtebasierten Clustern</em> (engl. <em>density-based clustering</em>).</p>
<p>Dieser Ansatz ist Grundverschieden vom <span class="math notranslate nohighlight">\(k\)</span>-Means Clustering und vom EM Clustering, weil es keine Formel gibt, mit der man angeben kann, welche Instanzen zu einem Cluster gehören. Stattedessen beschreiben die Instanzen die Cluster: Wenn man wissen will, welche Instanzen zum gleichen Cluster gehören, muss man sich die Nachbarschaften angucken.</p>
<p>Der DBSCAN Algorithmus ist der verbreitestes dichtebasierte Clusteralgorithmus. Auf unseren Beispieldaten liefern DBSCAN das folgende Ergebnis.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;indigo&#39;</span><span class="p">,</span> <span class="s1">&#39;navy&#39;</span><span class="p">,</span> <span class="s1">&#39;mediumseagreen&#39;</span><span class="p">,</span> <span class="s1">&#39;gold&#39;</span><span class="p">]</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
                       <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Data for clustering&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">);</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Result of DBSCAN Clustering&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_19_0.png" src="../_images/kapitel_06_19_0.png" />
</div>
</div>
<p>Wir können die selben vier Cluster wie auch bisher sehen. Es gibt jedoch auch einige Datenpunkte die mit einem grauen Kreuz markiert sind. Hierbei handelt es sich um <em>Rauschen</em> (engl. <em>noise</em>), also Instanzen die keinem Cluster zugeordnet werden können. Dies kann passieren, wenn es Datenpunkte gibt, die sich nicht in einer dichten Nachbarschaft befinden. In unserer Analogie sind die Häuser, die sich außerhalb der Stadtgrenzen befinden, zum Beispiel an Bauernhof an einer Landstraße.</p>
<div class="section" id="id6">
<h3><span class="section-number">6.5.1. </span>Der Algorithmus<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>Der DBSCAN Algorithmus basiert auf dem Konzept von <em>dichten Nachbarschaften</em>. Eine Nachbarschaft wird über die Distanz <span class="math notranslate nohighlight">\(\epsilon \in \mathbb{R}\)</span> definiert, so dass die Nachbarn einer Instanz <span class="math notranslate nohighlight">\(x \in X\)</span> definiert sind als <span class="math notranslate nohighlight">\(neighbors(x) = \{x' \in X: d(x, x') \leq \epsilon\}\)</span>. Eine Nachbarschaft heißt <em>dicht</em>, wenn sie mehr als <span class="math notranslate nohighlight">\(minPts \in \mathbb{N}\)</span> Instanzen beinhaltet. Alle Punkte, die eine dichte Nachbarschaft haben, nennt man <em>Kernpunkt</em> (engl. <em>core points</em>). Wir nutzen die Notation <span class="math notranslate nohighlight">\(core(C) = \{x \in X: |neighbors(x)| \geq minPts\}\)</span> um alle Kernpunkte innerhalb eines Clusters <span class="math notranslate nohighlight">\(X \subseteq X\)</span> zu beschreiben und <span class="math notranslate nohighlight">\(core(X)\)</span> um alle Kernpunkte zu beschreiben.</p>
<p>Sobald man alle Kernpunkte bestimmt hat, kann man Cluster <em>wachsen</em> lassen. Hierzu wählt man einen beliebigen Kernpunkt aus und definiert das dieser Kernpunkt zum ersten Cluster gehört. Dann fügt man alle Nachbarn dem Cluster hinzu. Für alle Nachbarn, die ebenfalls Kernpunkte sind, wird dies wiederholt bis man irgendwann alle Nachbarn von allen Kernpunkten im Cluster hat. Dann wählt man den nächsten Kernpunkt der noch zu keinem Cluster gehört aus, und lässt das nächste Cluster wachsen.</p>
<ol class="simple">
<li><p>Setze <span class="math notranslate nohighlight">\(i=1\)</span> als aktuelle Nummer des Clusters.</p></li>
<li><p>Wählen eine Kernpunkt aus, der noch keinem Cluster zugeordnet ist um ein neues Cluster zu initialisieren, also <span class="math notranslate nohighlight">\(x \in core(X) \setminus \bigcup_{j_i} C_j\)</span> und setze <span class="math notranslate nohighlight">\(C_i = \{x\}\)</span>.</p></li>
<li><p>Füge alle Nachbarn der Kernpunkte in <span class="math notranslate nohighlight">\(C_i\)</span> dem Cluster hinzu, also <span class="math notranslate nohighlight">\(C_i = \bigcup_{x \in core(C_i)} neighbors(x)\)</span>.</p></li>
<li><p>Wiederhole Schritt 3 bis keine weiteren Instanzen mehr hinzugefügt werden.</p></li>
<li><p>Falls es weitere Kernpunkte gibt, die noch keinem Cluster zugeordnet wurden, setze <span class="math notranslate nohighlight">\(i=i+1\)</span> und gehe zurück zu Schritt 2.</p></li>
</ol>
<p>Die Arbeitsweise des Algorithmus können wir uns am Beispiel verdeutlichen.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">core_samples_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">core_samples_mask</span><span class="p">[</span><span class="n">dbscan</span><span class="o">.</span><span class="n">core_sample_indices_</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">cluster_0_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">==</span><span class="mi">0</span>
<span class="n">cluster_0_core_mask</span> <span class="o">=</span> <span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span>
<span class="n">cluster_0_density_connected_mask</span> <span class="o">=</span> <span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span>

<span class="n">cluster_1_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">==</span><span class="mi">1</span>
<span class="n">cluster_1_core_mask</span> <span class="o">=</span> <span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span>
<span class="n">cluster_1_density_connected_mask</span> <span class="o">=</span> <span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span>

<span class="n">cluster_2_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">==</span><span class="mi">2</span>
<span class="n">cluster_2_core_mask</span> <span class="o">=</span> <span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span>
<span class="n">cluster_2_density_connected_mask</span> <span class="o">=</span> <span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span>

<span class="n">cluster_3_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">==</span><span class="mi">3</span>
<span class="n">cluster_3_core_mask</span> <span class="o">=</span> <span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span>
<span class="n">cluster_3_density_connected_mask</span> <span class="o">=</span> <span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Core Points&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Growing Cluster 1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Growing Cluster 2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Growing Cluster 3&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_2_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_2_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_2_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_2_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Growing Cluster 4&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_2_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_2_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_2_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_2_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_3_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_3_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_3_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_3_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_21_0.png" src="../_images/kapitel_06_21_0.png" />
</div>
</div>
<p>Im Detail passiert das Folgende:</p>
<ul class="simple">
<li><p>Als erstes werden alle Kernpunkte bestimmt. Diese Punkte werden später in jedem Fall zu einem Cluster gehören, da sie sich in einer dichten Nachbarschaft befinden.</p></li>
<li><p>Anschlieißend wird das erste Cluster erstellt. Hierzu wird ein Kernpunkt ausgewählt, in diesem ein Punkt aus dem Bereich rechts oben. Alle Kernpunkte in dieser Region werden zu diesem Cluster hinzugefügt, sowie auch alle Nachbarn der Kernpunkte, die nicht selbst Kernpunkte sind. Die mit einem lila Kreuz markierten Punkte sind selbst keine Kernpunkte, befinden sich aber in der Nachbarschaft eines Kernpunkts und gehört daher dem Cluster an. Dies sind quasi die Häuser an der Stadtgrenze. Es gibt auch eine graue Kreuze in der Region rechts oben. Diese befinden sich nicht in der Nachbarschaft eines Kernpunkts und sind Rauschen.</p></li>
<li><p>Anschließend wird ein neuer Kernpunkt gewählt um das zweite Cluster zu initialisieren. In diesem Fall wurde ein Punkt im unteren Bereich ausgewählt. Es werden wieder alle Nachbarn hinzugefügt, einige Punkte liegen in keiner Nachbarschaft und sind Rauschen.</p></li>
<li><p>Dies wird noch zwei mal wiederholt um die letzten beiden Cluster wachsen zu lassen.</p></li>
<li><p>Da jetzt alle Kernpunkte einem Cluster zugewiesen wurden, terminiert der Algorithmus. Alle Instanzen, die jetzt noch mit einem grauen Kreuz markiert sind, sind Rauschen.</p></li>
</ul>
</div>
<div class="section" id="bestimmen-von-epsilon-und-minpts">
<h3><span class="section-number">6.5.2. </span>Bestimmen von <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span><a class="headerlink" href="#bestimmen-von-epsilon-und-minpts" title="Permalink to this headline">¶</a></h3>
<p>Ein Vorteil von DBSCAN ist das man die Anzahl der Cluster nicht selbst bestimmen muss. Stattdessen werden diese vom Algorithmus basierend auf der Kernpunkten bestimmt. Man muss jedoch definieren, was eine (dichte) Nachbarschaft ist, in dem die Parameter <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span> festgelegt werden. Genauso wie ein schlecht gewähltes <span class="math notranslate nohighlight">\(k\)</span> bei den bisherigen Algorithmen zu schlechten Ergebnissen führen kann, gilt dies auch für diese Parameter. In unserem Beispiel haben wir <span class="math notranslate nohighlight">\(\epsilon=0,03\)</span> und <span class="math notranslate nohighlight">\(minPts=4\)</span> verwendet. Hier die Ergebnisse für einige andere Werte.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Result with $\epsilon=0.01, minPts=4$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Result with $\epsilon=0.05$, minPts=4&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Result with $\epsilon=0.01, minPts=2$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Result with $\epsilon=0.05$, minPts=20&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_23_0.png" src="../_images/kapitel_06_23_0.png" />
</div>
</div>
<p>In der ersten Spalte sehen wir Ergebnisse für sehr kleine Werte von <span class="math notranslate nohighlight">\(\epsilon\)</span> mit 0,01. Wenn man die <span class="math notranslate nohighlight">\(minPts\)</span> bei 4 belässt, findet man keine echten Cluster mehr. Es gibt zwar einige wenige Punkte, die noch als Kernpunkte erkennt werden, die meisten Punkte sind jedoch Rauschen. Wenn man minPts auf zwei reduziert, wird das Ergebniss besser. Es gibt vier größere Cluster, die ähnlich zu unserem Beispielergebnis weiter oben sind. Es gibt jedoch noch viele sehr kleine Cluster, die nur 3 Punkte beinhalten. Das ist nicht wünschenswert: diese kleinen Cluster sollten entweder zu größeren Clustern gehören oder als Rauschen erkannt werden. In der zweiten Spalte sehen wir was passiert wenn wir eine relativ großes <span class="math notranslate nohighlight">\(\epsilon\)</span> von 0,05 wählen. Die Ergebnisse sehen besser aus als mit dem kleinen Epsilon. Wenn wir die <span class="math notranslate nohighlight">\(minPts\)</span> bei belassen, finden wir drei große Cluster, die dem Ergebnisse vom <span class="math notranslate nohighlight">\(k\)</span>-Means und EM Clustering mit drei Clustern entsprechen. Wenn wir die <span class="math notranslate nohighlight">\(minPts\)</span> auf 20 erhöhen, haben wir weniger Kernpunkte die stattdessen große Nachbarschaften haben. Das führt zu einem ähnlichen Ergebnis wie mit unserer ursprünglichen Parameterwahl von <span class="math notranslate nohighlight">\(\epsilon=0,03\)</span> und <span class="math notranslate nohighlight">\(minPts=4\)</span>, jedoch mit weniger Rauschen.</p>
<p>Im Allgemeinen ist es so, dass man mit einem kleinen <span class="math notranslate nohighlight">\(\epsilon\)</span> die Wahrscheinlichkeit erhöhrt, das man viel Rauschen oder sehr kleine Cluster bekammt. Wenn man ein größeres <span class="math notranslate nohighlight">\(\epsilon\)</span> wählt, führt dies dazu das Cluster verschmolzen werden. Wenn man <span class="math notranslate nohighlight">\(minPts\)</span> reduziert, bekommt man mehr Kernpunkt, was dazu führt das Brücken zwischen Clustern entstehen und diese zusammengeführt werden oder auch das sehr kleine Cluster möglich werden. Wenn man <span class="math notranslate nohighlight">\(minPts\)</span> erhöht kann man kleine Cluster vermeiden, es könnte aber auch viel Rauschen geben, wenn <span class="math notranslate nohighlight">\(\epsilon\)</span> zu klein gewählt wird. Das Zusammenspiel der Parameter ist also relativ komplex und es gibt keine einfache Faustregel um gute Werte zu bestimmen.</p>
<p>Es gibt jedoch ein analytisches Werkzeug, mit dessen Hilfe man gute Kombinationen von <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span> bestimmen kann. Hierzu nutzt man den Abstand des <span class="math notranslate nohighlight">\(k\)</span>-st nächsten Nachbarn (engl. <span class="math notranslate nohighlight">\(k\)</span> nearest neighbor) einer Instanz entfernt um zu sehen, wie viele Kernpunkte es bei eine Parameterkombination gibt.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>

<span class="c1"># we use sklearn to find the nearest neighbors</span>
<span class="n">neigh</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">nbrs</span> <span class="o">=</span> <span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">distances</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">distances_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">distances</span><span class="p">[:,</span><span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">distances_k</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;minPts=</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of core points&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_25_0.png" src="../_images/kapitel_06_25_0.png" />
</div>
</div>
<p>Gute Parameter findet man in dem Bereich, in dem die Kurve stark gekrümmt ist. Für die Kurve von <span class="math notranslate nohighlight">\(minPts=4\)</span> ist das zum Beispiel im Bereich zwischen 0,03 und 0,04 <span class="math notranslate nohighlight">\(\epsilon\)</span> der Fall.</p>
</div>
<div class="section" id="probleme-von-dbscan">
<h3><span class="section-number">6.5.3. </span>Probleme von DBSCAN<a class="headerlink" href="#probleme-von-dbscan" title="Permalink to this headline">¶</a></h3>
<p>Bei DBSCAN muss man weder die Anzahl der Cluster auswählen, noch gibt es Einschränkungen bezüglich der Form der Cluster. Trotzdem gibt es auch bei diesem Algorithmus verschiedene Probleme, die man berücksichtigen sollte.</p>
<ul class="simple">
<li><p>Das größte Problem ist die Wahl von <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span>. Insbesondere wenn es Gruppen gibt die sehr nah beieinander liegen oder die Daten insgesamt sehr Dicht sind, kann es schwierig sein sinnvolle Parameter und Cluster zu finden. Der Grund dafür ist das in diesem Fall einige Datenpunkte zu <em>Brückepunkten</em> (engl. <em>bridge points</em>) zwischen zwei Clustern werden können. Man spricht von Brückenpunkten, wenn es Kernpunkte gibt, die dafür sorgen das zwei Cluster verschmelzen. Um dies zu vermeiden, muss man große Werte für minPts wählen, um zu vermeiden das in der Grenzregion eines Clusters gibt. Dies führt aber auch dazu, dass man <span class="math notranslate nohighlight">\(\epsilon\)</span> relativ groß wählen muss, was wieder zu neuen Problemen führen kann.</p></li>
<li><p>Ein weiteres Problem sind Gruppierungen mit unterschiedlichen Dichten. Im folgenden Beispiel sollen zwei Kreisförmige gruppen geclustert werden. Mit 100 Datenpunkten pro Kreis sind die Daten aufgrund des größerem Umfangs nicht sehr Dicht im Verhältnis zum inneren Kreis. Daher gibt es eine Stellen, an denen es keine Kernpunkte gibt, so dass es meherere kleinere Cluster im äußeren Kreis gibt. Mann man die Anzahl der Datenpunkte erhöht, werden beide Kreise korrekt geclustert. Insbesondere wenn jedes Cluster eine andere Dichte hat, ist dies ein großes Problem für DBSCAN.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Two circles with 100 instances&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Two circles with 200 instances&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_27_0.png" src="../_images/kapitel_06_27_0.png" />
</div>
</div>
<ul class="simple">
<li><p>Da die Dichte der Daten wichtig für die Wahl von <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span> ist, folgt auch das man DBSCAN nicht ohne weiteres auf eine zufällige Teilmenge der Daten anwenden kann, ohne die Parameter anzupassen. Der Grund hierfür ist, das eine Teilmenge in der Regel eine geringere Dichte hat.</p></li>
<li><p>Da DBSCAN ähnlich wie <span class="math notranslate nohighlight">\(k\)</span>-Means die Cluster basierend auf Distanzen bestimmt, ist der Algorithmus ebenfalls anfällig für Skaleneffekt, wenn Merkmale verschieden Größenordnungen haben.</p></li>
</ul>
</div>
</div>
<div class="section" id="single-linkage-clustering">
<h2><span class="section-number">6.6. </span>Single Linkage Clustering<a class="headerlink" href="#single-linkage-clustering" title="Permalink to this headline">¶</a></h2>
<p>Ein wichtiger Mechanismus wie Städte wachsen ist der Zusammenschluss mit anderen Städten und Gemeinden. Dies passiert natürlich nicht beliebig, sondern mit Städten die ohnehin sehr nah beieinander liegen. Das ist die Grundidee vom <em>hierarchischen Clustern</em>. Die Idee ist ähnlich wie bei DBSCAN, nur das wir nicht mit dichten Nachbarschaften arbeiten, sondern Stattdessen kleine Cluster zu größeren Clustern zusammenfügen. Als Ergebnis hat man daher sehr viele verschiedene mögliche Clusterergebnisse, da das zusammenfügen durchgeführt wird bis alle Instanzen zu einem einzigen Cluster verschmolzen sind. Anschließend wählt man von diesen vielen Ergebnissen ein passendes aus. Das Endergebnis sieht dann zum Beispiel so aus.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">AgglomerativeClustering</span>
<span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">dendrogram</span>

<span class="k">def</span> <span class="nf">warn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">pass</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">warn</span> <span class="o">=</span> <span class="n">warn</span>

<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.03</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
                       <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">sl</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">distance_threshold</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span><span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">,</span> <span class="n">compute_full_tree</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_sl</span> <span class="o">=</span> <span class="n">sl</span><span class="o">.</span><span class="n">labels_</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Daten (ohne Clustering)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis des Single Linkage Clustering&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_sl</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_29_0.png" src="../_images/kapitel_06_29_0.png" />
</div>
</div>
<p>Das Ergebnis sieht ähnlich aus wie bei DBSCAN, es gibt jedoch einen großen Unterschied: Es gibt kein Rauschen, stattdessen gibt es einige sehr kleine Cluster, die zum Teil sogar nur aus einer Instanz bestehen, zum Beispiel am linken Rand.</p>
<p>Es gibt verschiedene hierarchische Clusteralgorithmen, die sich darin unterscheiden wie die Cluster zusammengefügt werden. Es gibt hierbei auch Algorithmen, genau andersrum vorgehen und ein großes Cluster schrittweise in kleine Cluster zerlegen, zum Beispiel das <em>Complete Linkage Clustering</em>. Im Folgenden betrachten wir das <em>Single Linkage Clustering</em> (SLINK) als Beispiel für einen hierarchischen Clusteralgorithmus.</p>
<div class="section" id="der-slink-algorithmus">
<h3><span class="section-number">6.6.1. </span>Der SLINK Algorithmus<a class="headerlink" href="#der-slink-algorithmus" title="Permalink to this headline">¶</a></h3>
<p>SLINK ist ein iterativer Algorithmus, der in jedem Schritt die zwei Cluster mit dem geringsten Abstand voneinandeer zu einem neuen Cluster zusammenfügt. Zu Beginn ist jede Instanz in einem eigenen Cluster. SLINK berechnet dann die Distanzen für zwei Cluster <span class="math notranslate nohighlight">\(C\)</span> und <span class="math notranslate nohighlight">\(C'\)</span> als die Distanz zwischen den Instanzen der jeweiligen Cluster, die sich am nähesten liegen, also</p>
<div class="math notranslate nohighlight">
\[d(C, C') = \min_{x\in C, x' \in X'} dist(x, x').\]</div>
<p>Für jedes Cluster was beim zusammenfügen erstellt wird, berechnet SLINK ein <em>Level</em>. Das Level gibt an, wie weit die Cluster, die verbunden wurden, voneinander getrennt waren. Man kann sich das also als den Abstrand zwischen zwei Städten, die zu einer Stadt verbunden werden, vorstellen. Ein niedriges Level heißt also, dass zwei Cluster die verbunden wurden nah beieinander lagen, ein hohe Level heißt das es eine größere Lücke zwischen den Cluster gab. Diese Lücke ist dann automatisch auch Teil des größeren Clusters.</p>
<p>Basierend auf dieser Idee, ist der eigentlich Algorithmus relativ einfach.</p>
<ol class="simple">
<li><p>Initialisiere die Basiscluster <span class="math notranslate nohighlight">\(C = \{x\}\)</span> für alle <span class="math notranslate nohighlight">\(x \in X\)</span> mit Level <span class="math notranslate nohighlight">\(L(C) = 0\)</span>.</p></li>
<li><p>Finde die zwei Cluster, deren Abstand minimal ist, also <span class="math notranslate nohighlight">\(C, C' = \arg\min_{C, C'} d(C,C')\)</span>.</p></li>
<li><p>Verbinde <span class="math notranslate nohighlight">\(C\)</span> und <span class="math notranslate nohighlight">\(C'\)</span> zu einem neuen Cluster <span class="math notranslate nohighlight">\(C_{new} = C \cup C'\)</span> mit Level <span class="math notranslate nohighlight">\(L(C_{new}) = d(C, C')\)</span>.</p></li>
<li><p>Wiederhole die Schritte 2. und 3. bis alle Instanzen in einem Cluster sind.</p></li>
</ol>
<p>Das Ergebnis dieses Algorithmus ist also keine eindeutige Clustering, sondern sehr viele mögliche Ergebnisse. Diese kann man sich am besten durch ein <em>Dendrogramm</em> veranschaulichen.</p>
</div>
<div class="section" id="dendrogramme">
<h3><span class="section-number">6.6.2. </span>Dendrogramme<a class="headerlink" href="#dendrogramme" title="Permalink to this headline">¶</a></h3>
<p>Ein Dendrogram ist die visuelle Darstellung einer Baumstruktur als Graph. Für unser Beispiel bekommen wir folgendes Dendrogramm.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">dendrogram</span>

<span class="k">def</span> <span class="nf">plot_dendrogram</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Create linkage matrix and then plot the dendrogram</span>

    <span class="c1"># create the counts of samples under each node</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">children_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">merge</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">children_</span><span class="p">):</span>
        <span class="n">current_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">child_idx</span> <span class="ow">in</span> <span class="n">merge</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">child_idx</span> <span class="o">&lt;</span> <span class="n">n_samples</span><span class="p">:</span>
                <span class="n">current_count</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># leaf node</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current_count</span> <span class="o">+=</span> <span class="n">counts</span><span class="p">[</span><span class="n">child_idx</span> <span class="o">-</span> <span class="n">n_samples</span><span class="p">]</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_count</span>

    <span class="n">linkage_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">children_</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">distances_</span><span class="p">,</span>
                                      <span class="n">counts</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

    <span class="c1"># Plot the corresponding dendrogram</span>
    <span class="n">dendrogram</span><span class="p">(</span><span class="n">linkage_matrix</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Dendrogram vom Single Linkage Clustering&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Level&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Instanzen&quot;</span><span class="p">)</span>
<span class="n">plot_dendrogram</span><span class="p">(</span><span class="n">sl</span><span class="p">,</span><span class="n">color_threshold</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">no_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">above_threshold_color</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Gekürztes Dendrogram mit nur den obesten 15 Leveln&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Level&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Instanzen&quot;</span><span class="p">)</span>
<span class="n">plot_dendrogram</span><span class="p">(</span><span class="n">sl</span><span class="p">,</span><span class="n">color_threshold</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">no_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">above_threshold_color</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">truncate_mode</span><span class="o">=</span><span class="s1">&#39;level&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_31_0.png" src="../_images/kapitel_06_31_0.png" />
</div>
</div>
<p>Am unteren Rand des Dendrogramm sind unsere Instanzen, was gleichbedeutend ist mit den Clustern mit Level 0. Jede horizontale Linie verbindet zwei Cluster zu einem neuen Cluster. Die Lage dieser Linie auf der y-Achse ist das Level des neuen Clusters, was durch die Verbindung entsteht. Die horizontale Linie bei 0,1 verbindet zum Beispiel alle Instanzen in den Clustern des linken Teilbaums mit den Instanzen im rechten Teilbaum zu einem neuen Cluster mit Level 0,1. Aufgrund der Definition des Levels wissen wir, dass alle Instanzen im linken Teilbaum mindestens eine Distanz von 0,1 zu allen Instanzen im rechten Teilbaum haben. Außerdem ist noch eine schwarze horizontale Linie bei 0,03 eingezeichnet. Diese Linie markiert die Clusterauswahl, für die wir uns um Beispiel weiter oben entschieden haben.</p>
<p>Die linke Grafik ist sehr Dicht am unteren Rand, so das man eigentlich nur noch größere gleichfarbige Regionen erkennt. Dies liegt daran, das jede einzelne Instaz eine kleine Linie am unteren Rand darstellt, in unserem Fall gibt es daher 300 solche Linien. Ein einfacher Trick, mit dem man auch den unteren Bereich eines Dendrograms besser darstellen kann, ist es die unteren Ebenen des Baums wegzulassen. Man zeigt also nicht die Cluster aller Level, sondern nur die Cluster der höchsten Level. Die rechte Grafik zeigt zum Beispiel nur die obersten 15 Level. Hierdurch sieht man zwar die Instanzen am unteren Rand nicht mehr, kann aber dafür die Abstände zwischen den kleineren Clustern in diesem Bereich besser erkennen.</p>
<p>Auch wenn Dendrogramme als Visualisierung erstmal etwas gewöhnungsbedürftig sind, handelt es sich um ein hervorragendes Werkzeug um die visuell geeignete Cluster auszuwählen. Im obigen Beispiel haben wir einen festen Grenzwert von 0,03 verwendet um die Cluster anhand der Level auszuwählen. Im Dendrogramm kann man direkt erkennen, was zum Beispiel passieren würde, wenn wir stattdessen einen Grenzwert von 0,05 wählen würden: das grüne und das braune Cluster wären dann zusammen in einem Cluster. Wenn man den Grenzwert zwischen 0,06 und 0,26 wählen würde, hätte man zwei Cluster. Für Werte zwischen 0,06 und 0,1 hätte man eigentlich auch schon nur noch zwei Große Cluster, es gäbe jedoch formal noch einen weiteres Cluster mit einer einzigen Instanz. Insgesamt erkennt man die Auswirkungen des Grenzwerts den man wählt, deutlich besser als beim ähnlichen DBSCAN Algorithmus, wo die Auswirkungen von unterschiedlichen Werte von <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span> nicht einfach vorherzusagen sind.</p>
</div>
<div class="section" id="probleme-von-slink">
<h3><span class="section-number">6.6.3. </span>Probleme von SLINK<a class="headerlink" href="#probleme-von-slink" title="Permalink to this headline">¶</a></h3>
<p>Wie alle Algorithmen, ist auch SLINK nicht ohne Probleme.</p>
<ul class="simple">
<li><p>Das größte Problem von SLINK (und vom hierarchischen Clustern im Allgemeinen) ist die Skalierbarkeit für größere Datensätze. Da die Algorithmen in der Regel erfordern, dass man eine quadratische Distanzmatrix berechnet, in der die Distanzen zwischen allen Instanzen gespeichert werden, ist insbesondere der Speicherbedarf problematisch. Bei 100.000 Instanzen wären dies zum Beispiel bereits etwa 4 Gigabyte, selbst wenn man die symmetrie ausnutzt und nur das obere Dreieck der Distanzmatrix speichert. Daher können hierarchische Clusteralgorithmen in der Regel nur bei kleinen Datensätzen angewandt werden.</p></li>
<li><p>Ebenso wie beim DBSCAN Algorithmus sind Bereiche mit unterschiedlichen Dichten problematisch. Es ist jedoch einfacher einen guten Grenzwert zu bestimmen, mit dem man die Cluster trennen kennen. Hier hilft das Dendrogramm. Im Beispiel mit den Kreisen erkennt man zum Beispiel, dass man für Grenzwerte zwischen 0,2 und 0,4 das richtige Ergebnis findet.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>

<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">eps</span><span class="o">=</span><span class="mf">0.2</span>

<span class="n">sl</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">distance_threshold</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span><span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">,</span> <span class="n">compute_full_tree</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_sl</span> <span class="o">=</span> <span class="n">sl</span><span class="o">.</span><span class="n">labels_</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Result of Single Linkage Clustering&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_sl</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Dendrogram of Single Linkage Clustering&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Level&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Objects&quot;</span><span class="p">)</span>
<span class="n">plot_dendrogram</span><span class="p">(</span><span class="n">sl</span><span class="p">,</span><span class="n">color_threshold</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">no_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">above_threshold_color</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_33_0.png" src="../_images/kapitel_06_33_0.png" />
</div>
</div>
<ul class="simple">
<li><p>Ein weiteres Problem, welches wir bereits von DBSCAN kennen, sind Brückenpunkte, die zwei Cluster verbinden. Der Grund dafür ist, das DBSCAN und SLINK die gleichen Cluster liefern, wenn man <span class="math notranslate nohighlight">\(minPts=1\)</span> sets. Der Grenzwert für die Level ist dann von der Bedeutung identisch zum <span class="math notranslate nohighlight">\(\epsilon\)</span> von DBSCAN. Brückenpunkte sind bei SLINK sogar noch ein größeres Problem. Während man bei DBSCAN durch einen hohen Wert für minPts probieren kann, zu verhindert, dass die Brückenpunkte Kernpunkte werden, ist dies bei SLINK nicht möglich.</p></li>
<li><p>SLINK kann zu vielen sehr kleinen Clustern führen, da jede Instanz initial einem eigenen Cluster zugewiesen wird, und sich dies, je nach Grenzwert, möglicherweise auch im Endergebnis nicht ändert. Ausreißer führen daher dazu, dass es viele Cluster gibt, die irrelevant sind, aber die Analyse erschweren.</p></li>
<li><p>Da SLINK auch distanzbasiert ist, ist der Algorithmus auch anfällig für Skaleneffekt durch unterschiedliche Wertebereiche, wie wir es beim <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus beschrieben hab.</p></li>
</ul>
</div>
</div>
<div class="section" id="vergleich-der-algorithmen">
<h2><span class="section-number">6.7. </span>Vergleich der Algorithmen<a class="headerlink" href="#vergleich-der-algorithmen" title="Permalink to this headline">¶</a></h2>
<p>Während der Beschreibung aber wir immer wieder einzelne Aspekte der Algorithmen miteinander vergleichen, insbesondere wenn es ähnliche Probleme gab, oder ein Algorithmus ein Problem geschickt umgangen hat. Im letzten Teil dieses Kapitels wollen wir verschiedene Eigenschaften von allen Algorithmen direkt miteinander vergleichen.</p>
<div class="section" id="clusterformen">
<h3><span class="section-number">6.7.1. </span>Clusterformen<a class="headerlink" href="#clusterformen" title="Permalink to this headline">¶</a></h3>
<p>Die wohl wichtigste Eigenschaft der Algorithmen ist es, Cluster richtig zu erkennen. Hierbei spielt insbesondere die Form der Cluster eine Rolle, wie wir schon am Beispiel der Kreise und der Halbmonde gesehen haben.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span><span class="p">,</span> <span class="n">make_moons</span><span class="p">,</span> <span class="n">make_circles</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span><span class="p">,</span> <span class="n">KMeans</span><span class="p">,</span> <span class="n">AgglomerativeClustering</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="n">datasets</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mf">0.3</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mf">0.35</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.45</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.65</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.75</span><span class="p">,</span><span class="mi">0</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span><span class="kc">None</span><span class="p">),</span>
            <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>

<span class="c1"># parameter values for the different data sets</span>
<span class="n">k</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="n">eps</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.03</span><span class="p">,</span><span class="mf">0.15</span><span class="p">,</span><span class="mf">0.15</span><span class="p">,</span><span class="mf">0.15</span><span class="p">]</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ds_cnt</span><span class="p">,</span> <span class="n">ds</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">datasets</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ds</span>
    
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;indigo&#39;</span><span class="p">,</span> <span class="s1">&#39;gold&#39;</span><span class="p">,</span> <span class="s1">&#39;mediumseagreen&#39;</span><span class="p">,</span> <span class="s1">&#39;crimson&#39;</span><span class="p">]</span>
    
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;cmap_points&quot;</span><span class="p">)</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;indigo&#39;</span><span class="p">,</span> <span class="s1">&#39;crimson&#39;</span><span class="p">,</span> <span class="s1">&#39;mediumseagreen&#39;</span><span class="p">,</span> <span class="s1">&#39;gold&#39;</span><span class="p">]</span>
    
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">],</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
    
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;$k$-Means&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
    
    <span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">],</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>
    
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;DBSCAN&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    
    <span class="n">em</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">],</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">em</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_em</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">means_</span>
    
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;EM Clustering&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="p">[</span><span class="n">colors</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_em</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colors</span><span class="p">[:</span><span class="n">k</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">]]):</span>
        <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">covariances_</span><span class="p">[</span><span class="n">n</span><span class="p">][:</span><span class="mi">2</span><span class="p">,:</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="mi">180</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>  <span class="c1"># convert to degrees</span>
        <span class="n">v</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">ell</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                  <span class="mi">180</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
        
    <span class="n">sl</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">distance_threshold</span><span class="o">=</span><span class="n">eps</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">],</span><span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">,</span> <span class="n">compute_full_tree</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">sl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_sl</span> <span class="o">=</span> <span class="n">sl</span><span class="o">.</span><span class="n">labels_</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;SLINK&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_sl</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_35_0.png" src="../_images/kapitel_06_35_0.png" />
</div>
</div>
<p>Wie man sieht liefern alle Algorithmen auf unserm Beispieldatensatz gute Ergebnisse. Im Detail sieht man jedoch auch hier unterschiede, insbesondere da, wo die Cluster nah beieinander liegen. An der Grenze zwischen den beiden Clustern im mittleren Bereich sieht man eine scheinbar zufällige Trennung bei <span class="math notranslate nohighlight">\(k\)</span>-Means und EM Clustering, Rauschen bei DBSCAN, und einige kleine Cluster bei SLINK.</p>
<p>In der zweiten Zeile sehen wir noch einmal die Halbmonde. Hier erkennt man gut, was wir oben bereits beschrieben haben: <span class="math notranslate nohighlight">\(k\)</span>-Means und EM Clustering sind für solche geometrische Formen nicht geeignet, da sie von einer eher runden, bzw. ellipsoiden Form ausgehen. Da DBSCAN und SLINK stattdessen die direkten Nachbarschaften zwischen den Instanzen betrachten, sind solche Daten für diese Algorithmen kein Problem. Im Allgemeinen funktionieren diese Algorithmen mit beliebigen geometrischen Formen, wichtig ist nur das es Lücken zwischen den Clustern gibt. Das dies ein Problem sein kann, zeigt die dritte Reihe. Hier haben wir einige Brückenpunkte hinzugefügt, so dass die Algorithmen nicht mehr zwischen den Clustern unterscheiden können.</p>
<p>Die vierte Reihe zeigt das auch runde Clusterformen für <span class="math notranslate nohighlight">\(k\)</span>-Means und EM-Clustering bei <span class="math notranslate nohighlight">\(k\)</span>-Means und EM Clustering möglicherweise zu Probleme führen, nämlich wenn es sich um in einander liegende Kreise handelt.</p>
</div>
<div class="section" id="anzahl-der-cluster">
<h3><span class="section-number">6.7.2. </span>Anzahl der Cluster<a class="headerlink" href="#anzahl-der-cluster" title="Permalink to this headline">¶</a></h3>
<p>Eine weitere Stärke von DBSCAN und SLINK ist, dass man die Anzahl der Cluster nicht vorgeben muss, sondern sich diese automatisch aufgrund der Dichte der Daten und der gewählten Parameter ergibt. Beim EM Clustering gibt es mit dem BIC ein analytisches Kriterium, mit dem man das Optimum bestimmen kann. Bei <span class="math notranslate nohighlight">\(k\)</span>-Means ist eine manuelle Analyse zwingend erforderlich, zum Beispiel der WSS. Hier gibt es kein hartes Kriterium, anhand dessen man einen guten Wert für <span class="math notranslate nohighlight">\(k\)</span> aus dem Linienplot der WSS ablesen kann, stattdessen ist die Erfahrung gefragt um relvante Änderungen in der Steigung zu erkennen.</p>
</div>
<div class="section" id="ausfuhrungszeit">
<h3><span class="section-number">6.7.3. </span>Ausführungszeit<a class="headerlink" href="#ausfuhrungszeit" title="Permalink to this headline">¶</a></h3>
<p>Die Ausführungszeit kann, je nach größe des Datensatzes und Anwendungsfall, ein entscheidenes Kriterium bei der Wahl des Algorithmus sein. Unten sieht man die Ergebnisse der Ausführungszeit für das Clustern von Daten mit 20 Merkmalen für 10.000, 100.000, 1.000.000, 10.000.000 und 100.000.000 Instanzen, also für bis zu 15 Gigabyte Datenn. Bitte beachten Sie, dass wir SLINK nicht mit vergleichen, da dieser Algorithmus bereits bei 100.000 aus den oben beschriebenen Problemen mit dem Platzbedarf nicht mehr korrekt arbeitet. Die Zeiten wurden mit einem normalen Laptop (Intel Core i7-8850 &#64; 2.60GHz, 32 GB RAM) mit scikit-learn 0.24.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span><span class="p">,</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>

<span class="kn">import</span> <span class="nn">timeit</span>

<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;$k$-Means&quot;</span><span class="p">,</span>
         <span class="s2">&quot;EM&quot;</span><span class="p">,</span>
         <span class="s2">&quot;DBSCAN&quot;</span><span class="p">]</span>

<span class="n">clusters</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)]</span>

<span class="n">instances</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e3</span><span class="p">,</span><span class="mf">1e4</span><span class="p">,</span><span class="mf">1e5</span><span class="p">,</span><span class="mf">1e6</span><span class="p">,</span><span class="mf">1e7</span><span class="p">]</span>
<span class="n">runtime</span> <span class="o">=</span> <span class="p">[[],[],[]]</span>
<span class="k">for</span> <span class="n">num_instances</span> <span class="ow">in</span> <span class="n">instances</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_instances</span><span class="p">),</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">cl_cnt</span><span class="p">,</span> <span class="n">cl</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">cl_cnt</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">num_instances</span><span class="o">&gt;</span><span class="mi">100000</span><span class="p">:</span>
            <span class="c1"># we skip DBSCAN here, because this takes requires a long time and the trend is already visible</span>
            <span class="k">continue</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
        <span class="n">cl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">elapsed</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="n">runtime</span><span class="p">[</span><span class="n">cl_cnt</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elapsed</span><span class="p">)</span>
        
<span class="n">fix</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">instances</span><span class="p">,</span> <span class="n">runtime</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">instances</span><span class="p">,</span> <span class="n">runtime</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">instances</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">runtime</span><span class="p">[</span><span class="mi">2</span><span class="p">])],</span> <span class="n">runtime</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Laufzeit in Sekunden&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Anzahl der Instanzen&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_37_0.png" src="../_images/kapitel_06_37_0.png" />
</div>
</div>
<p>Wir man sieht ist die Laufzeit des <span class="math notranslate nohighlight">\(k\)</span>-Means und des EM Algorithmus ähnlich, DBSCAN ist langsamer. Dies liegt daran, dass es bei vielen Instanzen möglicherweise sehr große Nachbarschaften gibt, was sich negativ auf die benötigte Rechenzeit von DBSCAN auswirk. Die Laufzeitmessungen hängen aber auch von den Daten ab. Wenn die Daten besonders dünn (engl. <em>sparse</em>) oder dicht sind oder es eine seltsame Verteilung gibt, kann sich dies auf die Laufzeiten auswirken. Im Allgemeinen ist die Laufzeit aber ein sekundärer Aspekt bei der Auswahl eines geeigneten Clusteralgorithmus, es sei denn man hat extrem viele Datenpunkte.</p>
</div>
<div class="section" id="interpretierbarkeit-und-darstellung">
<h3><span class="section-number">6.7.4. </span>Interpretierbarkeit und Darstellung<a class="headerlink" href="#interpretierbarkeit-und-darstellung" title="Permalink to this headline">¶</a></h3>
<p>Ein großer Vorteil des <span class="math notranslate nohighlight">\(k\)</span>-Means und des EM Clustering ist das sie eine gut interpretierbare und kompakte mathematische Beschreibung der Cluster liefern, in dem die Cluster durch den Centroids, bzw. Normalverteilungen beschrieben werden. Dies liefert uns ein mächtiges Werkzeug um die Bedeutung der Cluster zu verstehen. Außerdem ist es hierdurch möglich, einfach zu bestimmen, zu welchen Cluster eine Instanz gehört, unabhängig von den anderen Daten. Hierdurch lassen sich die Ergebnisse zum Beispiel auch einfach auf anderen Systemen einsetzen, wo die Daten selbst nicht zur Verfügung stehen.</p>
<p>DBSCAN und SLINK habe keine vergleichbare Darstellung. Stattdessen sind hier die Instanzen selbst die Beschreibung der Cluster. Hierdurch kann man die Ergebnisse nur visuell interpretieren, was gerade bei hochdimensionalen Daten äußert schwierig ist. SLINK ist für die Interpretation durch die Dendrogramme etwas besser, da man zumindest relativ einfach beurteilen kann, wie groß die Abstände zwischen den Datenpunkten, bzw. den Clustern, sind.</p>
</div>
<div class="section" id="kategorische-merkmale">
<h3><span class="section-number">6.7.5. </span>Kategorische Merkmale<a class="headerlink" href="#kategorische-merkmale" title="Permalink to this headline">¶</a></h3>
<p>Kategorische Merkmale sind ein Problem für alle Algorithmen, die wir betrachtet haben. <span class="math notranslate nohighlight">\(k\)</span>-Means, DBSCAN und SLINK berechnen alle Distanzen. Da man per Definition keine sinnvollen Distanzen zwischen zwei Kategorien berechnen kann, kann man auch keine sinnvollen Distanzen zwischen Instanzen mit kategorischen Daten berechnen. Im Fall von <span class="math notranslate nohighlight">\(k\)</span>-Means gibt es eine Variante namens <span class="math notranslate nohighlight">\(k\)</span>-Modes Clustering, bei dem die Centroids durch den Modus der Daten im Cluster berechnet werden. Beim EM Clustering ist der Grund ähnlich. Die Normalverteilung geht von einem kontinuierlichen Wertebereich aus, was bei kategorischen Daten nicht der Fall ist. Die Lösung hier ist es keine Normalverteilung für die Beschreibung der Cluster zu verwenden, sondern stattdessen den EM Algorithmus mit einer besser passenden Verteilung zu verwenden, zum Beispiel einer Multinomialverteilung.</p>
</div>
<div class="section" id="fehlende-merkmale">
<h3><span class="section-number">6.7.6. </span>Fehlende Merkmale<a class="headerlink" href="#fehlende-merkmale" title="Permalink to this headline">¶</a></h3>
<p>In der Praxis kommt es häufig vor das es Instanzen gibt, bei denen nicht die Werte für alle Merkmale bekannt sind. Man spricht hier von fehlenden Werten, bzw. fehlenden Merkmalen. Es gibt drei Möglichkeiten, mit fehlenden Merkmalen umzugehen: Entweder kann der Algorithmus von sich aus mit unvollständigen Daten arbeiten. Dies ist, bis auf in wenigen Ausnahmefällen, nicht der Fall. Die zweite Möglichkeit ist es, alle unvollständigen Instanzen zu entfernen. Hier hat man jedoch möglicherweise in der Operationalisierung ein Problem. Die dritte Möglichkeit ist es durch <em>Imputation</em> eine Schätzung für die fehlenden Werte zu berechnen, zum Beispiel durch eine Regression basierend auf den vorhandenen Daten. Details zur Imputation finden sie in der Literatur, zum Beispiel bei Barnard und Menge <a class="footnote-reference brackets" href="#barnard" id="id7">2</a>.</p>
</div>
<div class="section" id="korrelierte-merkmale">
<h3><span class="section-number">6.7.7. </span>Korrelierte Merkmale<a class="headerlink" href="#korrelierte-merkmale" title="Permalink to this headline">¶</a></h3>
<p>Korrelierte Merkmale sind für alle betrachteten Algorithmen problemetisch, insbesondere die Algorithmen <span class="math notranslate nohighlight">\(k\)</span>-Means, DBSCAN und SLINK die Distanzen berechnen haben hierdurch ein Problem mit Korrelationen. Um dies zu verdeutlichen betrachten wir einen extremen Fall: Nehmen wir an es gebe zwei Merkmale, das normalisierte Alter in Jahren und das normalisierte Alter in Tagen. Man sagt das ein Merkmal von normalisiert (bzw. <em>normiert</em>) ist, wenn die Werte auf das Interval <span class="math notranslate nohighlight">\([0,1]\)</span> reskaliert wurden. Die Werte dieser beiden Feature wären nicht identisch, da das Alter in Tagen feingranularer ist, als das Alter in Jahren. Die Bedeutung und der Informationsgehalt wären jedoch nahezu identisch, so dass es faktisch das Merkmal “normalisiertes Alter” zwei mal geben würde. Das heißt auch, dass das Alter doppelt in die Distanzberechnung einfließt und entsprechend einen überproportionalen Einfluss auf die Distanz hätte. Je mehr und je stärker die Korrelationen in den Daten sind, desto ausgeprägter ist der negative Effekt auf die Distanzberechnungen.</p>
<p>Das EM Clustering ist robuster gegenüber Korrelationen, da diese durch die Kovarianzen berücksichtigt werden können. Da man jedoch möglicherweise mehr Merkmale als benötigt hat, wirken sich korrelationen auf die Berechnung des BIC aus, weshalb man eventuell eine zu kleine Anzahl an Clustern auswählt, da die Komplexität des Ergebnisses überschätzt wird.</p>
</div>
<div class="section" id="zusammenfassung-des-vergleichs">
<h3><span class="section-number">6.7.8. </span>Zusammenfassung des Vergleichs<a class="headerlink" href="#zusammenfassung-des-vergleichs" title="Permalink to this headline">¶</a></h3>
<p>Die folgenden zwei Tabellen fassen die Stärken und Schwächen der Clusteralgorithmen noch einmal zusammen.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="xref myst"></span></p></th>
<th class="head"><p>Clusterform</p></th>
<th class="head"><p>Clusteranzahl</p></th>
<th class="head"><p>Laufzeit</p></th>
<th class="head"><p>Interpretierbarkeit</p></th>
<th class="head"><p>Darstellung</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(k\)</span>-Means</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p>o</p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>EM Clustering</p></td>
<td><p>o</p></td>
<td><p>o</p></td>
<td><p>o</p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
</tr>
<tr class="row-even"><td><p>DBSCAN</p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p>o</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>SLINK</p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p>o</p></td>
<td><p>o (*)</p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="xref myst"></span></p></th>
<th class="head"><p>Kategorische Merkmale</p></th>
<th class="head"><p>Fehlende Merkmale</p></th>
<th class="head"><p>Korrelierte Merkmale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(k\)</span>-Means</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>EM Clustering</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p>o</p></td>
</tr>
<tr class="row-even"><td><p>DBSCAN</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>SLINK</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
</tr>
</tbody>
</table>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="voronoi"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p><a class="reference external" href="https://mathworld.wolfram.com/VoronoiDiagram.html">https://mathworld.wolfram.com/VoronoiDiagram.html</a></p>
</dd>
<dt class="label" id="barnard"><span class="brackets"><a class="fn-backref" href="#id7">2</a></span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.1177%2F096228029900800103">https://doi.org/10.1177/096228029900800103</a></p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="kapitel_05.html" title="previous page"><span class="section-number">5. </span>Assoziationsregeln</a>
    <a class='right-next' id="next-link" href="kapitel_07.html" title="next page"><span class="section-number">7. </span>Klassifikation</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Steffen Herbold<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>