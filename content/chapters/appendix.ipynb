{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "native-species",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# Selber Ausführen\n",
    "\n",
    "Das Buch ist so geschrieben, das nicht nur alle Quelltextbeispiele vorhanden sind. Es ist sogar möglich sich das komplette Buch runterzuladen und alles selbst auszuführen, anzupassen, und mit den Beispielen zu experimentieren. Hier ist eine kurze Erklärung, wie das unter Linux (getestet mit Ubuntu 20.04) geht. Windowsnutzer müssen entweder Python unter Windows installieren oder - wie ich selbst - mit dem Windows Subsystem for Linux (WSL) arbeiten [^wsl]. \n",
    "\n",
    "1. Wir beginnen mit dem Installieren der benötigten Linuxpakete. Python sollte bereits installiert sein (mindestens Version 3.6). Wir brauchen noch eine virtuelle Umgebung für Python (`venv` genannt). Außerdem wird noch Graphviz benötigt, jedoch nur für das Zeichnen der Struktur des Neuronalen Netzes in [Kapitel 7](kapitel_07).\n",
    "   ```\n",
    "   sudo apt-get update\n",
    "   sudo apt-get install python3-venv graphviz\n",
    "   ```\n",
    "2. Jetzt können wir Inhalte runterladen, entpacken und in den Ordner wechseln. \n",
    "   ```\n",
    "   cd ~\n",
    "   wget https://github.com/sherbold/einfuehrung-in-data-science/archive/refs/heads/main.zip\n",
    "   unzip main.zip\n",
    "   cd einfuehrung-in-data-science-main\n",
    "   ```\n",
    "3. Jetzt können wir uns in diesem Ordner eine virtuelle Pythonumgebung erstellen, aktivieren und die benötigten Pythonpakete installieren. \n",
    "   ```\n",
    "   python3 -m venv venv\n",
    "   source venv/bin/activate\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "4. Jetzt muss noch noch das Jupyter Lab gestartet werden.\n",
    "   ```\n",
    "   cd content\n",
    "   juypter-lab\n",
    "   ```\n",
    "5. Fertig! Jetzt können wir das Juypter Lab im Browser öffnen (der Link wird ja angezeigt) und sehen die Jupyter Notebooks für alle Kapitel, mit denen wir alles selbst ausprobieren können.\n",
    "\n",
    "Wenn wir fertig sind, können wir die Notebooks einfach schließen und mit `Strg+C` das Jupyter Lab im Terminal beenden. Beim nächsten Starten müssen wir nur in den Ordner wechseln, die `venv` aktivieren, und können dann direkt das Jupyter Lab starten.\n",
    "```\n",
    "cd ~\n",
    "cd einfuehrung-in-data-science-main/content\n",
    "source venv/bin/activate\n",
    "jupyter-lab\n",
    "```\n",
    "\n",
    "[^wsl]: https://ubuntu.com/wsl\n",
    "\n",
    "# Notationen\n",
    "\n",
    "In diesem Buch verwenden wir viele Mathematische Notation. Hier sind noch einmal die wichtigsten zum Nachschlagen also Tabelle. \n",
    "\n",
    "| Notation | Definition |\n",
    "|----------|------------|\n",
    "| $\\mathbb{R}$ | Die reellen Zahlen, also mehr oder weniger beliebige numerische Werte. |\n",
    "| $\\mathbb{N}$ | Die natürlichen Zahlen, also alle ganzen Zahlen die größer null sind.  |\n",
    "| $O$ | Objektraum, also die Menge der Objekte aus der realen Welt. |\n",
    "| $\\phi$ | Feature map, also eine Abbildung die definiert wie Merkmale für Objekte berechnet werden. |\n",
    "| $\\mathcal{F}$ | Merkmalsraum (engl. feature space), also die Dimension und die möglichen Werte der Merkmale. Häufig ist dies der $\\mathbb{R}^d$, also die $d$-dimensionalen reellen Zahlen. In diesem Fall gibt es $d \\in \\mathbb{N}$ Merkmale. |\n",
    "| $X$ (Clustering, Klassifikation, Regression) |  Instanzen von Objekten im Merkmalsraum. Je nach Kontext ist $X$ entweder eine Menge von Instanzen $X = \\{x_1, ..., x_n\\} \\subseteq \\mathcal{F}$ oder eine Zufallsvariable, so dass die Instanzen Realisierungen dieser Zufallsvariablen sind. |\n",
    "| $Y$ (Klassifikation, Regression) | Wert von Interesse, also die Klasse oder die abhängige Variable. Wie $X$ wird $Y$ auch entweder als Menge $Y= \\{y_1, ..., y_n\\}$ oder als Zufallsvariable definiert. |\n",
    "| $I$ | Endliche Menge von Gegenständen  \\{i_1, ..., i_m\\}.\n",
    "| $T$ | Endliche Menge von Transaktionen $T=\\{t_1, ..., t_n\\}$, wobei $t_i \\subseteq I$ für $i=1, ..., n$. |\n",
    "| $X$ (Assoziationsregeln) | Bedingung (engl. antecedent) einer Assoziationsregel. |\n",
    "| $Y$ (Assoziationsregeln) | Folgerung (engl. consequent) einer Assoziationsregel. |\n",
    "| $X \\Rightarrow Y$ | Assoziationsregel, wobei $X$ die Bedingung und $Y$ die Folgerung ist. |\n",
    "| ${n \\choose k}$ | Binomialkoeffizient ${n \\choose k} = \\frac{n!}{(n-k)!k!}$. |\n",
    "| $\\mathcal{P}(I)$ | Die Potenzmenge einer endlichen Menge $I$. |\n",
    "| $\\vert \\cdot \\vert$ | Die Kardinalität einer Menge, zum Beispiel $\\vert X \\vert$ für die Anzahl der Elemente der Menge $X$ |\n",
    "| $d(x,y)$ | Distanz zwischen zwei Vektoren $x$ und $y$, zum Beispiel die Euklidische Distanz, die Manhattan Distanz oder die Chebyshev Distanz. |\n",
    "| $argmin_{i=1,...,k} f(i)$ | Der Wert $i$ für den die Funktion $f$ minimiert wird. |\n",
    "| $argmin_{i \\in \\{1, ..., k\\}} f(i)$ | Andere Schreibweise für $argmin_{i=1,...,k} f(i)$. |\n",
    "| $\\min_{i=1,...,k} f(i)$ | Das Minimum der Funktion $f$ über die verschiedenen Werte von $i$. |\n",
    "| $\\min_{i \\in \\{1, ..., k\\}} f(i)$ | Andere Schreibweise für $\\min_{i=1, ..., k}$. |\n",
    "| $argmax$ | Siehe $argmin$. |\n",
    "| $\\max$ | Siehe $\\min$. |\n",
    "| $\\sim$ | Definiert die Verteilung einer Zufallsvariable. Wir schreiben $X \\sim (\\mu, \\sigma)$ um zu definieren das $X$ normalverteilt mit dem Mittelwert $ \\mu$ und der Standardabweichung $\\sigma$ ist.|\n",
    "| $C$ (Klassifikation) | Menge der Klassen. |\n",
    "| $C$ (Clustering) | Beschreibung der Cluster. |\n",
    "| $h$ | Hypothese, Konzept, Klassifikator, Klassifikationsmodell. |\n",
    "| $h^*$ | Zielkonzept. |\n",
    "| $h'_c$ | Score einer Hypothese für die Klasse $c$. |\n",
    "| $P(X=x)$ | Wahrscheinlichkeit, dass die Zufallsvariable $X$ den Wert $x$ annimmt. |\n",
    "| $p(x)$ | Kurzform für $p(x) = P(X=x)$ bei eine Zufallsvariable $x$. |\n",
    "| $P(X \\vert Y)$ | Bedingte Wahrscheinlichkeit einer Zufallsvariablen $X$ gegeben eine andere Zufallsvariable $Y$. | \n",
    "| $H(X)$ | Entropie einer Zufallsvariablen $X$. |\n",
    "| $H(X \\vert Y)$ | Bedingte Entropie der Zufallsvariablen $X$ gegeben der Zufallsvariable $Y$. |\n",
    "| $I(X; Y)$ | Informationsgewinn (engl. information gain) für $X$, bzw. $Y$, wenn die andere Variable bekannt ist. Auch bekannt als gemeinsame Information (engl. mutual information). |\n",
    "| $e_x$ | Residuen einer Regression. |\n",
    "| $x_t$ | Werte einer Zeitreihe $\\{x_1, ..., x_T\\} = \\{x_t\\}_{t=1}^T$. |\n",
    "| $T_t$ | Trend einer Zeitreihe. |\n",
    "| $S_t$ | Saisonalität einer Zeitreihe. |\n",
    "| $R_t$ | Autoregressiver Teil einer Zeitreihe. |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
