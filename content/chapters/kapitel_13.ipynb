{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "similar-dallas",
   "metadata": {},
   "source": [
    "# Weiterführende Konzepte\n",
    "\n",
    "Zum Abschluss dieses Crashkurses wollen wir noch einige wichtige weiterführende Konzepte benennen, die wir nicht behandelt haben. Dieses Kapitel ist also ein Hilfsmittel für alle, die sich weiter mit dem Thema beschäftigen wollen. \n",
    "\n",
    "Nicht alle Merkmale werden von jedem Algorithmus benötigt. Manchmal sind die Merkmale redundant, da es Korrelationen gibt, manchmal helfen die Merkmale auch einfach nicht bei der Analyse. Einige von den betrachteten Algorithmen haben das bereits berücksichtigt. Die Lasso Regulaisierung (siehe [Kapitel 8](kapitel_08)) eliminiert unwichtige Merkmale, in dem die Koeffizienten auf null gesetzt werden. Entscheidungsbäume wählen nur die informativsten Merkmale aus (siehe [Kapitel 7](kapitel_07), so dass einige Merkmale eventuell nicht berücksichtigt werden. Die meisten Algorithmen nutzen aber alle Merkmale, die zur Verfügung stehen. Da mehr Merkmale führen aber zu komplexeren Modellen, was sich auch negativ auf die Rechenzeit auswirkt. Mit Hilfe von Techniken zu Merkmalsauswahl (engl. *feature selection*), kann man dieses Problem unabhängig vom Algorithmus lösen. Solche Techniken suchen nach einer Teilmenge von Merkmalen, die nicht korreliert sind oder eliminieren Merkmale, die nicht relevant sind. Zwei Verfahren haben wir sogar behandelt: Mit der PCA (siehe [Kapitel 4](kapitel_04)) lassen sich neue Merkmale berechnen, so dass man eine kleinere und unkorrelierte Menge von Merkmalen nutzen könnte. Und der Informationsgewinn, den wir bei den Entscheidungsbäumen kennen gelernt haben, kann auch dazu benutzt werden Merkmale mit einem sehr geringen Informationsgewinn zu entfernen. \n",
    "\n",
    "Wir haben auch oft von Skaleneffekten gesprochen, und davon Merkmale zu normalisieren. Im Detail haben wir dieses Thema jedoch nicht behandelt und verglichen wie die Auswirkungen von verschiedenen Techniken sind. Auch ein vollständigen Überblick haben wir nicht gegeben: Neben der im Buch vorkommenden Normierung, Logarithmierung, und der Z-Score Standardisierung, gibt es zum Beispiel auch kernelbasierte Verfahren und quantilbasierte Verfahren. Je nach Datenlage und Anwendungsfall kann dies einen großen Unterschied machen. \n",
    "\n",
    "Bei der Klassifikation haben wir im Zuge der Gütemaße auch über die Class Level Imbalance gesprochen, nicht jedoch wie man dieses Problem lösen kann. Hierzu gibt es viele Verfahren, zum Beispiel durch das *Resampling* oder durch *Gewichte*. Beim Resampling werden Instanzen der Klasse die seltener vorkommt hinzugefügt (*Oversampling*) oder der Mehrheitsklasse entfernt (*Undersampling*), manchmal wird auch beides kombiniert. Mit Gewichten kann man dafür sorgen, dass bestimmte Instanzen eine höheren, bzw. geringeren Einfluss auf das Ergebnis haben. Durch höhere Gewichte der unterrepräsentierten Klasse können wir also Problemen entgegenwirken. \n",
    "\n",
    "Außerdem haben wir gesehen, dass die Algorithmen Parameter haben, mit denen man sie konfigurieren kann. Beispiele hierfür sind die Tiefe von Entscheidungsbäumen oder die Struktur eines Neuronalen Netzwerks. Wir haben auch erwähnt, dass man die Validationsdaten zur Wahl geeigneter Parameter benutzen kann. Wie das jedoch genau funktioniert, zum Beispiel durch eine *Grid Search*, *Random Search* oder gezielte Suchalgorithmen, haben wir nicht behandelt. \n",
    "\n",
    "Mit der Kreuzvalidierung haben wir ein Konzept gelernt, wie man auch bei einer kleinen Datenmenge mit einer Art Testdaten arbeiten kann. Es gibt noch weitere Verfahren, die je nach Datenmenge und Anwendungsfall relevant sein können, zum Beispiel Leave-One-Out (auch *Jackknife*) genannt, Bootstrap Sampling (kennen wir von den Random Forests), und das *Stratified Sampling*, welches berücksichtigt, dass sich die Verteilung nicht ändert. \n",
    "\n",
    "Außerdem haben wir natürlich nicht alle Algorithmen betrachtet, die es gibt. Insbesondere beim CLustern (z.B. spektrales Clustering, Self-Organizing Maps) und bei der Klassifikation (z.B. Ripper zum lernen von Disjunktionen, AdaBoost und Gradient Boosting Trees als Boostingalgorithmen, gausßsche Prozesse, Bayesian Networks) gibt es noch viele weitere Verfahren. Bei den Neuronalen Netzen haben wir auch nur an der Oberfläche gekratzt, hier gibt es viele weitere Netzwerkstrukturen, die wir nur kurz genannt oder gar nicht betrachtet haben. Mit dem *Reinforcement Learning* haben wir auch eine wichtigen Anwendungsfall nicht betrachtet: Wie können Systeme sich mit der Zeit selbst verbessern, in dem Sie ihre Beobachtungen nutzen um ständig neue Daten zu erzeugen, die direkt zu Modellverbesserung benutzt werden?\n",
    "\n",
    "Bei allem was wir betrachtet haben, blieb außerdem die Theorie im Hintergrund. Wie die meisten Algorithmen ihre Lösungen berechnen, zum Beispiel mit verfahren wie dem *Stochastic Gradient Descent* haben wir nicht behandelt. Auch die Theorie des maschinellen Lernens, also wie man diesen Bereich in der theoretischen Informatik betrachtet und hier unter bestimmten Annahmen zeigen kann, dass es Lösungen einer bestimmten Güte gibt, haben wir ignoriert. \n",
    "\n",
    "Zusammengefasst kann man also sagen, dass dieser Crashkurz zwar ein guter Start in das Thema war, aber es noch viel gibt, was man zusätzlich lernen kann und sollte, bevor man diese Disziplin gemeistert hat. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
