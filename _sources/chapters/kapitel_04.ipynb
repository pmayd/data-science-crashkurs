{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "developing-robinson",
   "metadata": {},
   "source": [
    "# Allgemeines zur Datenanalyse\n",
    "\n",
    "## Das No Free Lunch Theorem\n",
    "\n",
    "Bevor wir uns in die Algorithmen zu Modellierung mit Hilfe von Daten stürzen, benötigen wir noch einige Grundlagen. Die erste ist eine fundamentale Aussage über Optimierungsalgorithmen, das *No Free Lunch Theorem* (NFL). Das Theorem selbst ist sehr theoretisch, wenn Sie sich nur für die (sehr wichtigen!) praktischen Auswirkungen interessieren, springen Sie einfach direkt zur umgangssprachlichen Definition. Die mathematische Formulierung des NFL lautet wie folgt. \n",
    "\n",
    "> **No Free Lunch Theorem:**\n",
    ">\n",
    "> Sei $d_m^y$ eine geordnete Menge der Kardinalität $m$ mit Kostenwerten $y \\in Y$. Sei $f: X \\to Y$ eine Funktion die optimiert werden soll. Sei $P(d_m^y|f, m, a)$ die bedinge Wahrscheinlichkeit die Kosten $d_m^y$ durch $m$ wiederholte Ausführungen des Algorithmus $a$ für die Funktion $f$ zu beobachten.\n",
    ">\n",
    "> Für jedes Paar von Algorithmen $a_1$ und $a_2$ gilt:\n",
    ">\n",
    "> $$\\sum_f P(d_m^y|f, m, a_1) = \\sum_f P(d_m^y|f, m, a_2)$$\n",
    "\n",
    "Einen Beweis für das Theorem findet man in der Literatur [^nfl]. Die Gleichung des NFL besagt das die Summe der Wahrscheinlichkeiten bestimmte Kosten zu erhalten gleich ist, wenn man die Gesamtheit aller Funktionen $f$ betrachtet, unabhängig vom Algorithmus. Einfacher formuliert bedeutet das NFL also Folgendens. \n",
    "\n",
    "> **No Free Lunch Theorem (Umgangssprachlich):**\n",
    ">\n",
    "> Alle Algorithmen sind gleich, wenn man alle möglichen Optimierungsprobleme betrachtet.\n",
    "\n",
    "Diese Aussage entspricht nicht der Intuition, die man durch das praktische Arbeiten mit Daten hat. Für bestimmte Probleme merkt man sehr wohl das einige Algorithmen besser sind als andere. Dies wird durch das NFL aber auch gar nicht ausgeschlossen, im Gegenteil. Ein Algorithmus kann durchaus für einige Funktionen $f$ besser sein als andere Algorithmen. Aus dem NFL folgt aber, dass dieser Algorithmus dann auf allen anderen Problemen unterdurchschnittlich sein muss. Das ist auch die wichtige praktische Konsequenz des NFL: *Es gibt keinen Algorithmus, der für alle Probleme optimal ist!* Stattdessen hängt der optimale Algorithmus zur Lösung eines Problems vom Problem selbst ab. Daher auch der Name des Theorems: Es gibt kein \"Gratisessen\", also einen Algorithmus für alle Probleme. Stattdessen müssen Sie sich als Data Scientist ihr Essen verdienen, in dem sie passende Algorithmen kennen und auswählen. \n",
    "\n",
    "\n",
    "Das heißt natürlich auch, dass es nicht ausreicht sich auf eine bestimmte Art von Algorithmus zu spezialisieren. Dieser Algorithmus kann mathematisch Beweisbar gar nicht in jeder Situation die beste Wahl sein. Die Falle in die man tappen könnte ist, dass man nicht mehr den Algorithmus an das Problem anpasst, sondern das Problem an den Algorithmus. Denn wenn man einen Hammer hat, sieht manchmal alles aus wie ein Nagel. Stattdessen sollten Sie eine ganze Werkzeugbox an Algorithmen kennen. Nur dann können wir basierend auf der Problemstellung, den zur Verfügung stehenden Daten und unserer Erfahrung den am besten geeigneten Algorithmus auswählen. Dies lernt man nicht über Nacht, sondern nur aus der Erfahrung von vielen Projekten in denen man mit verschiedenen Datensätzen und Methoden arbeitet. \n",
    "\n",
    "## Definition von maschinellem Lernen\n",
    "\n",
    "Maschinelles lernen ist derzeit eines der heißesten Themen in der Informatik, da es in den letzten Jahren große Fortschritte bei der Lösung von praxisrelevanten Problemen durch die immer höher werdende  Rechenkraft, die größeren Datenmengen die zur Verfügung stehen, und innovatives Design von Lernalgorithmen, gab. Einige Problem die noch vor wenigen Jahren als extrem schwierig galten, sind jetzt durch maschinelles Lernen gelöst (siehe [Kapitel 1](kapitel_01)). Wenn man sich näher mit dem maschinellen Lernen beschäftigen möchte, muss man zuerst die Bedeutung des Begriffs verstehen. Es gibt viele Definition in der Literatur. Eine sehr mächtige und gleichzeitig intuitive kommt von Tom Mitchel [^mitchel]:\n",
    "\n",
    "> **Definition von maschinellem Lernen:**\n",
    ">\n",
    "> Ein Computerprogramm lernt aus Erfahrung $E$ in Bezug auf eine Klasse von Aufgaben $T$ und ein Gütemaß $P$, wenn die Güte der Lösungen von $T$ gemessen durch $P$ sich mit mehr Erfahrung $E$ verbessert. \n",
    "\n",
    "Auf den ersten Blick wirkt diese sehr abstrakt wirkt und ist schwer zu lesen. Insbesondere die abstrakten Begriffe von *Erfahrung*, einer *Klasse von Aufgaben*, und der *Güte* sind intuitiv in diesem Zusammenhang schwer einzuordnen. Das ist aber der Grund, weshalb die Definition so gut ist: Maschinelles Lernen ist ein sehr vielseitiges Gebiet, was anders kaum zu fassen ist. Außerdem ist die Definition nur auf den ersten Blick komplex. Bei einer genaueren  Betrachtung was mit Erfahrung, Aufgaben, und Güte gemeint ist, erscheint die Definition sehr naheliegend. \n",
    "\n",
    "- Die *Erfahrung* ist (im Normalfall) unser Datensatz. Je mehr Daten wir haben, desto mehr Erfahrung haben wir. Es gibt außerdem auch selbst-lernende System, die keine externen Daten benötigen und die stattdessen ihre eigenen Daten generieren, wie AlphaZero [^alphazero], das durch das Spielen gegen sich selbst lernt. In diesem Fall steigt die Erfahrung mit jedem Spiel. \n",
    "- Die *Aufgaben* sind nichts anderes als die Probleme, die sie mit maschinellem Lernen lösen möchten. Sie wollen Fußgänger erkennen, damit diese von ihrem autonomen Fahrzeug nicht überfahren werden? Sie wollen Spiele spielen und gewinnen? Sie wollen das die Besucher ihrer Website auf die Werbung klicken? Das sind die Aufgaben. Im Allgemeinen ist die Aufgabe eng verwandt mit dem Anwendungsfalls und den Projektzielen. Etwas Abstrakter kann man Klassen von Aufgaben auf Kategorien von Algorithmen abbilden, wie wir am Ende dieses Kapitels sehen werden. \n",
    "- Die *Gütemaße* messen wie gut die Aufgabe erfüllt wird. Das könnte zum Beispiel die Anzahl der korrekt erkannten Fußgänger, der gewonnen Spiele, oder die Zahl der Klicks auf Werbung sein.\n",
    "\n",
    "Mit diesem Wissen erscheint die Definition ganz einfach: Wir betrachten einfach Algorithmen, die besser werden, wenn ihnen mehr Daten zur Verfügung stehen. \n",
    "\n",
    "## Merkmale\n",
    "\n",
    "Im letzten Kapitel haben wir bereits oft von *Merkmalen* (engl. *feature*) gesprochen, ohne jedoch genau zu Erklären was wir damit eigentlich meinen. Die Merkmale sind eine Kernkomponente von maschinellem Lernen. Die Bedeutung von Merkmalen kann man sich gut an einem Beispiel verdeutlichen. Wenn Sie sich das folgende Bild angucken werden Sie sofort erkennen, dass es sich um einen Wal handelt. \n",
    "\n",
    "```{figure} images/whale.png\n",
    "---\n",
    "width: 400px\n",
    "name: fig-whale\n",
    "---\n",
    "Bild von einem Wal.\n",
    "```\n",
    "\n",
    "Wie genau wir als Menschen erkennen, dass es sich um einen Wal handelt ist nicht abschließend geklärt und immer noch der Gegenstand der Forschung. Nichtsdestotrotz nehmen wir hier an, dass wir verschiedene Aspekte vom Bild erkennen und Aufgrund der Aspekte dann zur Erkenntnis gelangen, dass es sich um einen Wal handelt. Man könnte zum Beispiel erkennen, dass sich ein etwa *ovales* Objekt im Vordergrund befindet, welches *oben schwarz und unten weiß* ist, das *Flossen hat* und das der *Hintergrund blau* ist. Das alles sind *Merkmale* des Bilds. \n",
    "\n",
    "Und so funktioniert auch maschinelles Lernen: Schlussfolgerungen über Objekte anhand ihrer Merkmale. Formal haben wir einen *Objektraum* $O$ mit Objekten aus der realen Welt und einen *Merkmalsraum* (engl. *feaature space*) $\\mathcal{F}$ mit Beschreibungen der Objekte durch ihre Merkmale. Eine *Feature Map* $\\phi: 0 \\to \\mathcal{F}$ bildet die Objekte auf ihre Repräsentation im Merkmalsraum ab. Für unser Walbild ist der Objektraum $O$ die Menge der Bilder und der Merkmalsraum hätte fünf Dimensionen: Form, Farbe oben, Farbe unten, Hintergrundfarbe, Flossen. Die Represenation des Bilds im Merkmalsraum wäre daher $\\phi(Walbild) = (Oval, Schwarz, Weiß, Blau, Ja)$. \n",
    "\n",
    "Es gibt verschiedene *Skalen* über die Merkmale definiert sein können. Die am häufigsten verwendete Skalendefinition geht auf Stevens zurück und ist auch als *Steven's Levels of Measurements*, bzw. *NOIR*-Skalen bekannt [^stevens]. NOIR ist ein Akronym für die vier Arten von Skalen: Nominal, Ordinal, Intervall, und Rational. \n",
    "\n",
    "| Skala | Eigenschaft | Erlaubte Operationen | Beispiel |\n",
    "|-------|----------|--------------------|---------|\n",
    "| Nominal | Klassifikation oder Zugehörigkeit | $=, \\neq$ | Farben als Schwarz, Weiß und Blau.  |\n",
    "| Ordinal | Vergleichbare Level | $=, \\neq, >, <$ | Größe in Small, Medium und Large |\n",
    "| Intervall | Unterschiede und Abstände | $=, \\neq, >, <, + ,-$ | Daten, Temperaturen |\n",
    "| Rational | Absolute Größen und Mengen | $=, \\neq, >, <, +, -, *, /$ | Größe in cm, Dauer in Sekunden |\n",
    "\n",
    "Nominale und ordinale Merkmale nennt man auch *kategorische* Merkmale, da sie ihre Werte in Kategorien darstellen. Bei nominalen Merkmalen gibt es keine Ordnung der Kategorien, das heißt, dass man zum Beispiel nicht sagen kann welche Kategorie größer oder kleiner ist. Bei ordinalen Merkmalen gibt es eine wohldefinierte Ordnung, wir können die Kategorien also sortieren. Der Abstand zwischen den Kategorien ist jedoch nicht bekannt und auch nicht notwendigerweise immer der gleiche zwischen benachbarten Kategorien. Entsprechend kann man zum Beispiel durchaus sagen, dass T-Shirts der Größe *Small* kleiner sind als *Medium* und *Medium* wiederum kleiner als *Large*. Aber wie groß der Unterschied zwischen *Small* und *Medium* ist, und ob der Unterschied zwischen *Medium* und *Large* gleich groß sind, ist nicht bekannt. Dieses Wissen hat man erst, wenn man eine Intervallskala verwendet, auf der man die Unterschiede quantifizieren kann. Wir können zum Temperaturdifferenz von 10° C und 5° C berechnen. Was jedoch auf Intervalskalen keinen Sinn ergibt, sind Verhältnisse. Man kann zum Beispiel nicht Sinnvoll sagen, dass dass das Datum 2000-01-01 doppelt so hoch ist wie 1000-01-01. Daher kann man Verhältnisse nur bei Merkmalen mit einer rationalen Skala sinnvoll berechnen. Wenn wir zum Beispiel die Zeit messen, die im Gregorianischen Kalender seit dem Jahr 0 vergangen ist, kann man durchaus sagen das 2000 Jahre unterschied doppelt so viel ist wie 1000 Jahre. \n",
    "\n",
    "Viele Algorithmen gehen davon aus, dass die Merkmale durch Zahlen repräsentiert werden. Während diese auf Intervall- und Rationalskalen kein Problem ist, sind Nominal- und Ordinalskalen nicht numerisch. Ein einfacher Ansatz wäre es die Kategorien einfach durchzunummerieren, zum Beispiel Schwarz als 1, Weiß als 2 und Blau als 3. Diesen Ansatz sollte man jedoch in der Regel vermeiden, da die Gefahr besteht das Algorithmen dann mit diesen Zahlen rechnen und zum Beispiel Differenzen bilden. Dann wäre Blau Minus Weiß auf einmal Schwarz, was natürlich keinen Sinn ergibt. Eine bessere Lösung ist daher das *One-Hot Encoding*. Das Konzept hinter dem One-Hot Encoding ist eine Nominal-, bzw. Ordinalskala durch viele Merkmale mit den Werten 0 und 1 zu ersetzen. Für jede Kategorie der ursprünglichen Skala gibt es ein neues Merkmal. Für einen Datenpunkt ist der Wert dieses neuen Merkmals 1, wenn der Datenpunkt zur entsprechenden Kategorie gehört und ansonsten 0. \n",
    "\n",
    "Als Beispiel betrachten wir die Nominalskala mit den Werten Schwarz, Weiß, und Blau, es gilt also $x \\in \\{Schwarz, Weiß, Blau\\}$. Wir ersetzen dieses Feature durch drei neue Feature $x^{Schwarz}, x^{Weiß}, x^{Blaub} \\in \\{0,1\\}$. Die Werte der neuen Merkmale sind definiert als\n",
    "\n",
    "$$x^{Schwarz} = \\begin{cases}1 & \\text{wenn}~x=\\text{Schwarz} \\\\ 0 & \\text{sonst}\\end{cases}$$\n",
    "\n",
    "$$x^{Weiß} = \\begin{cases}1 & \\text{wenn}~x=\\text{Weiß} \\\\ 0 & \\text{sonst}\\end{cases}$$\n",
    "\n",
    "$$x^{Blau} =  \\begin{cases}1 & \\text{wenn}~x=\\text{Blau}  \\\\ 0 & \\text{sonst}\\end{cases}$$\n",
    "\n",
    "Mit diesem Ansatz transformiert man also $n$ Kategorien in $n$ neue Merkmale. Es ist auch möglich ein One-Hot Encoding mit $n-1$ neuen Merkmalen zu erreichen. Wir könnten im Beispiel einfach $x^{Blau}$ weglassen und wüssten trotzdem noch eindeutig, welche Farbe es ist: Wenn $x^{Schwarz} = x^{Weiß} = 0$ gilt, muss die Farbe stattdessen Blau sein. Diese Eigenschaft sollte man insbesondere dann ausnutzen, wenn man eine Skala mit zwei Kategorien umwandelt. \n",
    "\n",
    "Bitte beachten Sie, dass One-Hot Encoding möglicherweise nicht gut funktioniert, wenn sie sehr viele verschiedene Kategorien haben. Dies führt zu vielen neuen Merkmalen, welches bei der anschließenden Modellierung zum Problem werden kann. Hinzu kommt, dass man bei der Umwandlung von Ordinalskalen die Informationen über die Ordnung verliert. In solchen Fällen sollte man nach Möglichkeit auch Analysemethoden zurückgreifen, die direkt mit Nominal-, bzw. Ordinalskalen arbeiten können. \n",
    "\n",
    "## Trainings- und Testdaten\n",
    "\n",
    "Daten sind das Herz von jedem Data Science Projekt. Die Daten bestehen aus *Instanzen* von *Merkmalen*, wobei eine Instanz die Representation eines Objekts der realen Welt durch seine Merkmale ist. Die folgende Tabelle zeigt uns ein Beispiel von Daten: \n",
    "\n",
    "| Form     | Farbe oben | Farbe unten | Hintergrundfarbe | Flossen | \n",
    "|----------|------------|-------------|------------------|---------|\n",
    "| Oval     | Schwarz    | Weiß        | Blau             | Ja      |\n",
    "| Rechteck | Braun      | Braun       | Grün             | Nein    |\n",
    "| ...      | ...        | ...         | ...              | ...     |\n",
    "\n",
    "Jede Zeile in der Tabelle ist eine Instanz, jede Spalte beinhaltet wie Werte von einem Merkmal. Häufig gibt es noch eine gewissen Eigenschaft die man aus den Merkmalen lernen möchte, sozusagen einen *Wert von Interesse* der mit jeder Instanz verbunden wird. \n",
    "\n",
    "| Form     | Farbe oben | Farbe unten | Hintergrundfarbe | Flossen | Wert von Interesse | \n",
    "|----------|------------|-------------|------------------|---------|--------------------|\n",
    "| Oval     | Schwarz    | Weiß        | Blau             | Ja      | Wal                |\n",
    "| Rechteck | Braun      | Braun       | Grün             | Nein    | Bär                |\n",
    "| ...      | ...        | ...         | ...              | ...     | ...                |\n",
    "\n",
    "Je nachdem ob dieser Wert bekannt ist und für die Erstellung eines Modells verwendet wird oder nicht, spricht man vom *überwachten*, bzw. *unüberwachten Lernen* (engl. *supervised/unsupersized*).\n",
    "\n",
    "Daten sind häufig in Form von Datensätzen organisiert. In diesem Zusammenhang sind die Begriffe *Trainingsdaten* und *Testdaten* für die Datenanalyse besonders wichtig. Die Trainingsdaten werden für die Erstellung von Modellen benutzt, das heißt für alles von der Datenexploration, über die Bestimmung von geeigneten Modellen, als auch für Experimente um das beste Modell auszuwählen. Man kann die Trainingsdaten so oft man will benutzt und beliebig transformieren. \n",
    "\n",
    "Die Testdaten werden jedoch idealerweise nur ein einziges Mal verwendet, und zwar am Ende des Projekts um zu überprüfen ob die Ergebnisse von den Trainingsdaten auch auf andere Daten verallgemeinern und um hierdurch abzuschätzen wie gut ein Modell im operationellen Betrieb funktionieren würde. Hierzu wird das auf den Trainingsdaten ermittelte Modell mit den Testdaten gefüttert um die Güte, basierend auf den in der Discovery definierten Kriterien, zu messen.\n",
    "\n",
    "Die Unterscheidung zwischen Trainings- und Testdaten ist wichtig um sicherzustellen das man *Overfitting* verhindert und stattdessen die *Generalisierung* sicherstellt. Overfitting bedeutet, dass das Modell die Daten auswendig lernt, statt das eigentliche Problem zu lösen. Es ist nämlich relativ einfach ein perfektes Modell für die Trainingsdaten zu erstellen: Man lernt diese einfach auswendig und gibt diese später wieder. Bei beliebigen anderen Daten würde dieses Modell jedoch nicht mehr funktionieren. Die Testdaten verhindern genau das. Wenn das Modell nur auswendig gelernt hat statt Zusammenhänge zu erkennen, wird es auf den Testdaten nicht gut funktionieren. Andersrum gilt aber auch, dass wenn ein Modell auch auf den Testdaten gute Ergebnisse liefert, dass es wahrscheinlich ist das es auch in der realen Anwendung wie gewünscht funktioniert. \n",
    "\n",
    "Leider sind Daten häufig eine nur begrenzt verfügbare Ressource, da das Sammeln von Daten häufig zeitaufwendig und/oder kostenintensiv ist. Auf der einen Seite wissen wir, dass maschinelles Lernen bessere Ergebnisse liefert, wenn mehr Daten zur Verfügung stehen. Auf der anderen Seite ist die Bewertung der Güte auf den Testdaten  genauer, wenn mehr Testdaten zur Verfügung stehen. Wie viele und welche Daten fürs Testen verwendend werden ist also immer eine Abwägung die innerhalb von einem Projekt gemacht werden muss. Im Idealfall stehen genug Daten zur Verfügung, dass man mit *Hold-Out* Daten arbeiten kann. Das bedeutet man hat einen echten Testdatensatz, der auch nur zu diesem Zweck verwendet wird. Wie viele Daten als Hold-Out-Daten genutzt legt man üblicherweise als Prozentsatz der zur Verfügung stehenden Daten fest, zum Beispiel\n",
    "\n",
    "- 50% Trainingsdaten, 50% Testdaten,\n",
    "- 66,6% Trainingsdaten, 33,4% Testdaten, oder auch\n",
    "- 75% Trainingsdaten, 25% Testdaten. \n",
    "\n",
    "Es kann auch Sinn machen, zusätzlich noch *Validationsdaten* zu nutzen. Validationsdaten sind \"wiederverwendbare Testdaten zur Modellauswahl\". Das heißt, dass man die Daten ähnlich wie die Testdaten verwendet, jedoch nicht nur am Ende des Projekts zur abschließenden Bewertung. Stattdessen kann man diese Daten bereits vorher nutzen, zum Beispiel um verschiedene Modelle miteinander zu vergleichen und oder um Modellparameter zu optimieren. Validationsdaten können ebenfalls als Hold-Out Daten erstellt werden, zum Beispiel mit 50% Trainingsdaten, 25% Validationsdaten und 25% Testdaten. \n",
    "\n",
    "Für den Fall das nicht genug Daten zu Verfügung stehen um mit Hold-Out Daten zu arbeiten, kann man *Kreuzvalidierung* (engl. *cross-validation*) nutzen um die Wahrscheinlichkeit von Overfittung zu reduzieren. Bei der Kreuzvalidierung gibt es keine klare Trennung zwischen Trainings- und Testdaten. Stattdessen werden alle Daten sowohl für das Training, als auch für die Bewertung der Güte benutzt. {numref}`fig-cv` beschreibt die Kreuzvalidierung. Bei $k$-Facher Kreuzvalidierung (engl. $k$-fold cross-validation) werden die Daten hierfür in $k$ gleichgroße Partitionen unterteilt. Jede Partition wird einmal für das Testen benutzt und alle anderen Partitionen für das Training. \n",
    "\n",
    "```{figure} images/cv-german.png\n",
    "---\n",
    "width: 800px\n",
    "name: fig-cv\n",
    "---\n",
    "Verwendung von Daten zur Kreuzvalidierung.\n",
    "```\n",
    "\n",
    "Die Güte wird als arithmetisches Mittel der Güte auf der jeweiligen Testpartition berechnet. Bei der Kreuzvalidierung wird somit jede Instanz der Daten genau einmal in den Testdaten und $k-1$ mal zum Training verwendet. Die Nachteil der Kreuzvalidierung ist das es keine echten Testdaten gibt. Stattdessen handelt es sich vielmehr um Validierungsdaten. Dennoch verwenden wir hier auch den Begriff Testdaten, da dies in der Literatur üblich ist. Da es keine echte Testdaten gibt ist das Risiko von Overfitting bei der Kreuzvalidierung höher als bei der Verwendung von Hold-Out Daten. Daher sollte man, sofern möglich, Hold-Out Daten verwenden und Kreuzvalidierung nur als Notlösung betrachten. \n",
    "\n",
    "## Kategorien von Algorithmen\n",
    "\n",
    "Data Science ist ein vielseitiges Gebiet. Je nach Problemstellung gibt es unterschiedliche Algorithmen, die für ein Projekt geeignet sind. \n",
    "\n",
    "- *Assoziationsregeln* befassen sich mit Zusammenhängen zwischen Gegenständen in Transaktionen. Der klassische Anwendungsfall von Assoziationsregel ist die Vorhersage von Gegenständen, die zu einem Warenkorb noch hinzugefügt werden, basierend auf dem aktuellen Inhalt des Warenkorbs. Wir betrachten den Apriori-Algorithmus zum Bestimmen von Assoziationsregeln in [Kapitel 5](kapitel_05). \n",
    "- *Clustern* beschäftigt sich mit der Suche von Gruppen, also Daten die zueinander ähnlich sind und daher zur selben Gruppe gehören. Wir betrachten den $k$-Means Algorithmus, den EM Algorithmus, DBSCAN, und Single Linkage Clustern in [Kapitel 6](kapitel_06). \n",
    "- *Klassifikation* behandelt das zuweisen von Labeln zu Objekten. Wir behandeln den $k$-Nearest Neighbor Algorithmus, Entscheidungsbäume, Random Forests, Logistische Regression, Naive Bayes, Support Vector Machines, und Neuronale Netze in [Kapitel 7](kapitel_07). \n",
    "- *Regression* such nach Zusammenhängen zwischen Merkmalen, eventuell noch mit dem Ziel einen numerischen Wert vorherzusagen. Wir beschäftigen uns mit der linearen Regression durch Ordinary Least Squares (OLS), Ridge, Lasso, und das Elastic Net in [Kapitel 8](kapitel_08). \n",
    "- *Zeitreihenanalyse* berücksichtig die zeitliche Struktur von Daten, zum Beispiel bei täglichen, wöchentlichen, oder monatlichen Daten. Die Zeitreihenanalyse get über die Regression hinaus, da auch Aspekte wie die Saisonalität berücksichtig werden. Wir betrachten ARIMA in [Kapitel 9](kapitel_09). \n",
    "- *Text Mining* befasst sich mit der Analyse von unstrukturierten textuellen Daten. Grundsätzlich können alle Arten von Algorithmen zum Textmining verwendet werden. Die Schwierigkeit liegt darin, eine geeignete Struktur für den Text zu finden und die Daten zu verarbeiten. Wir betrachten ein einfaches Text Mining Verfahren basierend auf einem Bag-of-Words in [Kapitel 10](kapitel_10). \n",
    "\n",
    "Bei der Suche nach Assoziationsregeln und beim Clustern werden unüberwachte Lernverfahren die Anhand der Merkmale nach Mustern in den Daten suchen. Daher werden solche verfahren auch manchmal als Musterkennung (engl. *pattern recognition*) bezeichnet. Klassifikation, Regression, und Zeitreihenanalyse sind Beispiele für überwachtes Lernen. Das heißt, dass man für diese Ansätze den Wert von Interesse kennen muss und die Verfahren den Zusammenhang zu den Merkmalen lernen. \n",
    "\n",
    "\n",
    "[^nfl]: https://doi.org/10.1109/4235.585893\n",
    "[^mitchel]: https://doi.org/10.1007/978-3-662-12405-5\n",
    "[^alphazero]: https://arxiv.org/abs/1712.01815\n",
    "[^stevens]: https://doi.org/10.1126/science.103.2684.677"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
